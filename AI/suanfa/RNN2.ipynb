{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9c6538-d96f-46cd-ba1f-0b033bebd696",
   "metadata": {},
   "source": [
    "# 文本生成案例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5c78c-7e3f-4f58-a84c-4cab3f555de9",
   "metadata": {},
   "source": [
    "### 词嵌入层 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd143701-92ec-45ec-ad84-6c348ed086af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北京', '东奥', '的', '进度条', '已经', '过半', '，', '不少', '外国', '运动员', '在', '完成', '自己', '的', '比赛', '后', '踏上', '归途', '。']\n",
      "Embedding(18, 4)\n",
      "北京 -> tensor([0.3457, 0.3825, 0.5380, 1.7388], grad_fn=<EmbeddingBackward0>)\n",
      "东奥 -> tensor([0.3363, 0.7970, 1.7221, 0.0421], grad_fn=<EmbeddingBackward0>)\n",
      "的 -> tensor([-0.2941, -0.8324,  1.8048, -0.0365], grad_fn=<EmbeddingBackward0>)\n",
      "进度条 -> tensor([ 0.7872, -0.1584,  0.8657, -1.6459], grad_fn=<EmbeddingBackward0>)\n",
      "已经 -> tensor([ 0.7600, -1.7611,  1.2300,  0.3257], grad_fn=<EmbeddingBackward0>)\n",
      "过半 -> tensor([-1.1398,  0.8186, -2.8867,  2.1403], grad_fn=<EmbeddingBackward0>)\n",
      "， -> tensor([ 0.2748, -0.1309, -0.6391,  0.2122], grad_fn=<EmbeddingBackward0>)\n",
      "不少 -> tensor([ 1.1781,  0.7161, -1.8390, -1.4239], grad_fn=<EmbeddingBackward0>)\n",
      "外国 -> tensor([-0.5269,  0.2707,  0.8005, -0.5764], grad_fn=<EmbeddingBackward0>)\n",
      "运动员 -> tensor([ 1.9003, -0.2485, -0.8052,  0.1101], grad_fn=<EmbeddingBackward0>)\n",
      "在 -> tensor([1.5074, 0.5562, 1.0836, 1.3871], grad_fn=<EmbeddingBackward0>)\n",
      "完成 -> tensor([ 0.2636, -1.2331,  0.1731,  0.2015], grad_fn=<EmbeddingBackward0>)\n",
      "自己 -> tensor([ 1.3232, -0.3706, -0.5867, -0.0190], grad_fn=<EmbeddingBackward0>)\n",
      "的 -> tensor([-0.2941, -0.8324,  1.8048, -0.0365], grad_fn=<EmbeddingBackward0>)\n",
      "比赛 -> tensor([ 0.5163, -0.2148, -1.3204,  1.8485], grad_fn=<EmbeddingBackward0>)\n",
      "后 -> tensor([ 1.2490, -1.0645,  0.9838,  0.3320], grad_fn=<EmbeddingBackward0>)\n",
      "踏上 -> tensor([ 0.6138, -0.7303,  0.3944,  0.3472], grad_fn=<EmbeddingBackward0>)\n",
      "归途 -> tensor([-0.9553, -0.6905,  0.5758, -0.0264], grad_fn=<EmbeddingBackward0>)\n",
      "。 -> tensor([-1.6899,  1.0526,  0.5242,  0.1932], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import jieba #分词工具 pip install jieba\n",
    "\n",
    "def test01():\n",
    "    text = '北京东奥的进度条已经过半，不少外国运动员在完成自己的比赛后踏上归途。'\n",
    "    \n",
    "    # 1.分词\n",
    "    words = jieba.lcut(text) \n",
    "    print(words)\n",
    "    \n",
    "    # 2.构建词表\n",
    "    index_to_word = {} #用索引区词\n",
    "    word_to_index = {} #用word取词索引\n",
    "    # 去重\n",
    "    unique_words = list(set(words))\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        index_to_word[idx] = word\n",
    "        word_to_index[word] = idx\n",
    "    \n",
    "    # print(index_to_word)\n",
    "    \n",
    "    ##########\n",
    "    # 3.构建词嵌入层    num_embeddings:词表大小;   embedding_dim:词向量维度\n",
    "    embed = nn.Embedding(num_embeddings=len(index_to_word), embedding_dim=4)\n",
    "    print(embed)\n",
    "    \n",
    "    # 4.句子用词向量表示\n",
    "    for word in words:\n",
    "        # \n",
    "        idx = word_to_index[word]\n",
    "        # 获取这个词的词向量表示\n",
    "        word_vec = embed(torch.tensor(idx))\n",
    "        print(word, '->', word_vec)\n",
    "        \n",
    "    \n",
    "test01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78949956-8b5c-44df-9d01-88f26ddef21c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8623, -0.9483,  0.9983]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.8623, -0.9483,  0.9983]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# rnn输入单个单词\n",
    "def test02(): \n",
    "\n",
    "    # input_size: 单个词向量的维度\n",
    "    # hidden_size: 隐藏层大小，也就是隐藏层中神经元个数，同时也是这个隐藏层输出数据的个数(每个神经元输出一个标量，多个神经元就输出多个标量)\n",
    "    # num_layers: 隐藏层的个数\n",
    "    input_size = 128\n",
    "    hidden_size = 3\n",
    "    num_layers = 1\n",
    "    rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "    \n",
    "    # 模拟要输入的一个词向量\n",
    "    # inputs形状 (seq_len, batch_size, input_size)\n",
    "    # seq_len: 句子长度，句子中词向量个数\n",
    "    seq_len = 1\n",
    "    batch_size = 1\n",
    "    inputs = torch.randn(seq_len,batch_size,input_size)\n",
    "    \n",
    "    # 初始隐藏层\n",
    "    # 隐藏层形状 (num_layers, batch_size, hidden_size)\n",
    "    hn = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "    \n",
    "    # 将这个词向量送到rnn中\n",
    "    # output表示每个输入词向量对应的中间状态，h0表示最后一个词向量的中间状态\n",
    "    # 在Pytorch中定义的RNN，其实是没有y这个的输出。\n",
    "    # Pytorch版本的两个输出，output=[h1, h2, h3, h4], hn = h4\n",
    "    # 如果想要得到输出层y，可以自行加一个全连接层\n",
    "    output, hn = rnn(inputs, hn)\n",
    "    print(output)\n",
    "    print(hn)\n",
    "    \n",
    "test02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b4bb02e-916f-4b50-8501-d73bc53a6452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4931, -0.6627, -0.9259,  0.2352, -0.8524, -0.6563, -0.9168,\n",
      "          -0.9999]],\n",
      "\n",
      "        [[ 0.9473, -0.9958,  0.2207, -0.9973, -0.6312,  0.9970, -0.9940,\n",
      "          -0.7935]],\n",
      "\n",
      "        [[-0.9988,  0.4833,  0.9190, -0.0450,  0.8556, -0.2926,  0.3449,\n",
      "           0.9977]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[-0.9988,  0.4833,  0.9190, -0.0450,  0.8556, -0.2926,  0.3449,\n",
      "           0.9977]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# rnn输入句子\n",
    "def test03(): \n",
    "    # input_size: 单个词向量的维度\n",
    "    # hidden_size: 隐藏层大小，也就是隐藏层中神经元个数，同时也是这个隐藏层输出数据的个数(每个神经元输出一个标量，多个神经元就输出多个标量)\n",
    "    # num_layers: 隐藏层的个数\n",
    "    input_size = 128\n",
    "    hidden_size = 8\n",
    "    num_layers = 1\n",
    "    rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "    \n",
    "    # 模拟要输入的一个词向量\n",
    "    # seq_len: 句子长度，句子中词向量个数\n",
    "    # inputs形状 (seq_len, batch_size, input_size)\n",
    "    seq_len = 3 #3个词向量\n",
    "    batch_size = 1\n",
    "    inputs = torch.randn(seq_len,batch_size,input_size)\n",
    "    \n",
    "    # 初始隐藏层\n",
    "    # 隐藏层形状 (num_layers, batch_size, hidden_size)\n",
    "    hn = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "    \n",
    "    # 将句子送到rnn中\n",
    "    # output表示每个输入词向量对应的中间状态，h0表示最后一个词向量的中间状态\n",
    "    # 在Pytorch中定义的RNN，其实是没有y这个的输出。\n",
    "    # Pytorch版本的两个输出，output=[h1, h2, h3, h4], hn = h4\n",
    "    # 如果想要得到输出层y，可以自行加一个全连接层\n",
    "    output, hn = rnn(inputs, hn)\n",
    "    print(output)\n",
    "    print(hn)\n",
    "    \n",
    "test03()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8af9a-155f-4f34-978a-4237443fb499",
   "metadata": {},
   "source": [
    "# 文本生成---周杰伦歌词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "92fbfa6b-9303-4dca-bd5b-4221ac59f841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import jieba\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# 构建词典\n",
    "def build_vocab():\n",
    "    fname = 'data/jaychou_lyrics.txt'\n",
    "    \n",
    "    # 1.文本数据清晰\n",
    "    clean_sentences = [] #存放的是清理好的句子\n",
    "    for line in open(fname, 'r'):\n",
    "        # 去除某些内容\n",
    "        line = line.replace('〖韩语Rap译文〗','') \n",
    "        # 只保留特定符号 (中文、英文、数字、部分标点符号)\n",
    "        line = re.sub(r'[^\\u4e00-\\u9fa5 a-zA-Z0-9!?,]', '', line) \n",
    "        # 连续空格替换成1个\n",
    "        line = re.sub(r'[ ]{2,}', ' ', line)\n",
    "        # 去除两侧空格、换行\n",
    "        line = line.strip()\n",
    "        # 去除单字的行\n",
    "        if len(line) <= 1:\n",
    "            continue\n",
    "        \n",
    "        #去掉重复行\n",
    "        if line not in clean_sentences:\n",
    "            clean_sentences.append(line)\n",
    "        \n",
    "    # print(clean_sentences)\n",
    "    # 2.分词并构建重要词表映射\n",
    "    index_to_word = [] # 索引到词的映射 ['词1'，'词2'，'词3']\n",
    "    all_sentences = [] # 存放所有句子分词后的形式[[句子1分词],[句子2分词],[]]\n",
    "    for line in clean_sentences:\n",
    "        #分词\n",
    "        words = jieba.lcut(line)\n",
    "        # 分好词的句子存放到all_sentences中\n",
    "        all_sentences.append(words)\n",
    "        \n",
    "        ####构造index_to_word 索引到词的 词表映射\n",
    "        for word in words:\n",
    "            if word not in index_to_word:\n",
    "                index_to_word.append(word)\n",
    "    \n",
    "    # print(index_to_word)\n",
    "    ##### 构建词到索引的词表映射\n",
    "    word_to_index = {word:idx for idx, word in enumerate(index_to_word)}\n",
    "    \n",
    "    ##### 将输入的句子转换为索引表示\n",
    "    # 把all_sentences中的每个词变成索引 ->放到 corpus_index\n",
    "    corpus_index = []\n",
    "    for line in all_sentences:\n",
    "        temp = []\n",
    "        for word in line:\n",
    "            temp.append(word_to_index[word])\n",
    "        \n",
    "        # 每个句子最后加个空格 ' ' ?????\n",
    "        temp.append(word_to_index[' '])\n",
    "        #corpus_index.append(temp)\n",
    "        corpus_index.extend(temp) # 变成一维数组? 其实无所谓，做能做成datasets就行\n",
    "    \n",
    "    # [0, 1, 2, 39,   0, 3, 4, 5, 6, 7, 39,    ]\n",
    "    # print(corpus_index)\n",
    "    # 词的数量\n",
    "    word_count = len(index_to_word)\n",
    "    return index_to_word, word_to_index, word_count,corpus_index,all_sentences\n",
    "   \n",
    "    \n",
    "# test\n",
    "def test00():\n",
    "    index_to_word, word_to_index, word_count,corpus_index, all_sentences = build_vocab()\n",
    "    print('word_count-> ',word_count)\n",
    "    print('index_to_word-> ',index_to_word[:10])\n",
    "    print('word_to_index-> ',dict(list(word_to_index.items())[:7]))\n",
    "    \n",
    "    print('corpus_index-> ',corpus_index[:10]) #显示前10个词\n",
    "    print('all_sentences-> ',all_sentences[:2]) #显示前2个句子\n",
    "\n",
    "# test00()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "3de7e2cd-7c29-4a4f-ba95-6084f31a987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据集对象\n",
    "class LyricsDataset:\n",
    "    # corpus_idx: 所有句子的索引表示\n",
    "    # num_chars: 用它表示句子的长度，也就是一个句子的单词个数，但实际情况句子长度不是固定的，这里就假设固定了\n",
    "    #           \n",
    "    def __init__(self, corpus_index, num_chars):\n",
    "        # 所有句子的索引表示\n",
    "        self.corpus_index = corpus_index\n",
    "        # 句子固定长度\n",
    "        self.num_chars = num_chars\n",
    "        # 词的个数\n",
    "        self.word_count = len(self.corpus_index)\n",
    "        # 整个数据集中有多少个句子\n",
    "        self.number = self.word_count // self.num_chars\n",
    "    \n",
    "    # 整个数据集中有多少个句子\n",
    "    def __len__(self):\n",
    "        return self.number\n",
    "    \n",
    "    # 获取一个样本, idx:表示取第几个样本\n",
    "    def __getitem__(self, idx):\n",
    "        # 修改索引值: [0, self.word_count - 1]\n",
    "        start = min(max(idx,0), self.word_count - self.num_chars - 2)\n",
    "        \n",
    "        # 假设样本      x = [0, 1, 9, 8, ... ]\n",
    "        # 那么目标值就是 y = [1, 9, 8, ... ], 正好跟上面的x错开一位\n",
    "        # start = start * self.num_chars # 要不要加这一句 ?????\n",
    "        x = self.corpus_index[start: start + self.num_chars]\n",
    "        y = self.corpus_index[start+1: start+1  + self.num_chars]\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "    \n",
    "    \n",
    "def test02():\n",
    "        _, _, _, corpus_idx,_ = build_vocab()\n",
    "        # 数据集\n",
    "        lyrics = LyricsDataset(corpus_idx, 5) # 句子长度是5\n",
    "        # 加载数据\n",
    "        dataloader = DataLoader(lyrics, batch_size=2)\n",
    "\n",
    "        \n",
    "        print('corpus_index-> ',corpus_idx[:15])\n",
    "        for x,y in dataloader:\n",
    "            print(x)\n",
    "            print(y)\n",
    "            break\n",
    "        \n",
    "# test02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "a41954f5-f2d1-4b8c-bdfe-3fcb32be4528",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebd.weight --->  Parameter containing:\n",
      "tensor([[-0.9871, -1.8750, -0.1660,  ...,  1.1023,  1.7314, -2.4037],\n",
      "        [-0.7514,  1.0828, -1.1543,  ...,  2.1350, -1.4456,  1.2311],\n",
      "        [-0.6336, -0.0432,  1.0558,  ..., -0.8379,  0.3617,  0.5216],\n",
      "        ...,\n",
      "        [ 1.0259, -0.3053, -0.4325,  ...,  1.2294, -0.7571,  2.8377],\n",
      "        [-1.7234,  0.0603,  0.5656,  ...,  0.0094,  2.3275, -1.6497],\n",
      "        [-0.5478, -0.0125, -1.4502,  ...,  0.3223,  0.9581, -0.0769]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_ih_l0 --->  Parameter containing:\n",
      "tensor([[-0.0648,  0.0108,  0.0286,  ...,  0.0432,  0.0474, -0.0714],\n",
      "        [ 0.0350, -0.0263, -0.0877,  ..., -0.0767, -0.0076, -0.0200],\n",
      "        [ 0.0315,  0.0481, -0.0115,  ..., -0.0470, -0.0108,  0.0408],\n",
      "        ...,\n",
      "        [ 0.0578,  0.0585, -0.0114,  ...,  0.0716, -0.0770,  0.0497],\n",
      "        [ 0.0247,  0.0069, -0.0774,  ..., -0.0147,  0.0163,  0.0871],\n",
      "        [ 0.0861, -0.0839, -0.0691,  ...,  0.0472, -0.0582,  0.0558]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_hh_l0 --->  Parameter containing:\n",
      "tensor([[ 0.0608,  0.0361, -0.0652,  ...,  0.0011, -0.0659, -0.0170],\n",
      "        [ 0.0234, -0.0276, -0.0804,  ..., -0.0436,  0.0235, -0.0521],\n",
      "        [ 0.0710,  0.0491, -0.0258,  ..., -0.0073,  0.0741, -0.0154],\n",
      "        ...,\n",
      "        [ 0.0714, -0.0744, -0.0658,  ...,  0.0294,  0.0106,  0.0686],\n",
      "        [ 0.0253, -0.0711,  0.0242,  ...,  0.0272, -0.0469, -0.0543],\n",
      "        [-0.0381,  0.0498, -0.0746,  ...,  0.0671, -0.0121, -0.0780]],\n",
      "       requires_grad=True)\n",
      "rnn.bias_ih_l0 --->  Parameter containing:\n",
      "tensor([-0.0447, -0.0501, -0.0508, -0.0047, -0.0759, -0.0477, -0.0365, -0.0121,\n",
      "         0.0086,  0.0451,  0.0208, -0.0679,  0.0873,  0.0481, -0.0720, -0.0581,\n",
      "         0.0233, -0.0180,  0.0360, -0.0758,  0.0824, -0.0546,  0.0483, -0.0185,\n",
      "         0.0012, -0.0211,  0.0672,  0.0435,  0.0413,  0.0069,  0.0342, -0.0164,\n",
      "        -0.0178, -0.0096, -0.0310, -0.0035,  0.0152,  0.0428, -0.0009,  0.0739,\n",
      "        -0.0058,  0.0534,  0.0261, -0.0400, -0.0770,  0.0038,  0.0530,  0.0064,\n",
      "         0.0175, -0.0219,  0.0181, -0.0570,  0.0701,  0.0636, -0.0174, -0.0433,\n",
      "        -0.0320, -0.0507,  0.0142,  0.0849, -0.0349, -0.0185,  0.0015, -0.0158,\n",
      "        -0.0515,  0.0488, -0.0586, -0.0773,  0.0843, -0.0268,  0.0882, -0.0506,\n",
      "         0.0694,  0.0721,  0.0715,  0.0684,  0.0228,  0.0209,  0.0657,  0.0250,\n",
      "        -0.0711,  0.0684,  0.0148,  0.0707, -0.0685,  0.0814, -0.0868,  0.0774,\n",
      "         0.0096, -0.0701, -0.0044,  0.0355, -0.0855, -0.0558, -0.0425,  0.0458,\n",
      "         0.0650, -0.0496, -0.0110,  0.0387,  0.0333, -0.0795,  0.0664, -0.0866,\n",
      "        -0.0438,  0.0660, -0.0012, -0.0375, -0.0164,  0.0425,  0.0826,  0.0422,\n",
      "        -0.0819, -0.0593,  0.0455, -0.0653,  0.0205, -0.0728, -0.0503,  0.0023,\n",
      "         0.0545, -0.0014, -0.0215,  0.0728, -0.0558, -0.0056,  0.0625,  0.0845],\n",
      "       requires_grad=True)\n",
      "rnn.bias_hh_l0 --->  Parameter containing:\n",
      "tensor([ 0.0196, -0.0796,  0.0789,  0.0606,  0.0009, -0.0538,  0.0847,  0.0466,\n",
      "        -0.0548, -0.0728, -0.0536,  0.0392,  0.0525,  0.0821,  0.0448,  0.0628,\n",
      "         0.0411,  0.0859, -0.0508, -0.0254, -0.0821, -0.0835,  0.0546, -0.0077,\n",
      "         0.0103,  0.0661, -0.0506,  0.0573, -0.0025, -0.0671, -0.0615,  0.0692,\n",
      "        -0.0107,  0.0646,  0.0259,  0.0443,  0.0168, -0.0456, -0.0405,  0.0373,\n",
      "        -0.0326, -0.0377, -0.0322, -0.0015, -0.0387,  0.0508, -0.0634, -0.0032,\n",
      "        -0.0412, -0.0348, -0.0246,  0.0130,  0.0623, -0.0012,  0.0130,  0.0476,\n",
      "        -0.0464,  0.0779, -0.0016, -0.0083,  0.0519, -0.0609, -0.0034, -0.0571,\n",
      "         0.0702,  0.0360, -0.0388,  0.0842,  0.0282,  0.0081, -0.0882, -0.0169,\n",
      "         0.0255,  0.0150, -0.0217,  0.0467,  0.0133,  0.0877,  0.0461,  0.0346,\n",
      "         0.0316,  0.0653, -0.0351,  0.0274,  0.0381, -0.0230, -0.0432,  0.0840,\n",
      "         0.0768,  0.0555,  0.0475, -0.0252,  0.0356, -0.0278,  0.0004,  0.0532,\n",
      "        -0.0198, -0.0119,  0.0675, -0.0819,  0.0830, -0.0372, -0.0314,  0.0121,\n",
      "         0.0770,  0.0856,  0.0413,  0.0063, -0.0135, -0.0441, -0.0188,  0.0848,\n",
      "        -0.0023, -0.0170,  0.0354,  0.0565,  0.0203, -0.0779,  0.0100,  0.0146,\n",
      "        -0.0774,  0.0661, -0.0169, -0.0744, -0.0749, -0.0602, -0.0269,  0.0459],\n",
      "       requires_grad=True)\n",
      "out.weight --->  Parameter containing:\n",
      "tensor([[ 0.0330, -0.0234, -0.0421,  ...,  0.0228,  0.0785,  0.0828],\n",
      "        [-0.0387, -0.0833, -0.0716,  ..., -0.0569,  0.0729, -0.0211],\n",
      "        [-0.0063, -0.0044,  0.0256,  ...,  0.0079,  0.0565,  0.0551],\n",
      "        ...,\n",
      "        [-0.0520,  0.0057, -0.0141,  ...,  0.0437, -0.0102,  0.0240],\n",
      "        [-0.0559, -0.0659,  0.0834,  ...,  0.0607,  0.0697,  0.0363],\n",
      "        [ 0.0215, -0.0712, -0.0599,  ...,  0.0883, -0.0149, -0.0023]],\n",
      "       requires_grad=True)\n",
      "out.bias --->  Parameter containing:\n",
      "tensor([-0.0426, -0.0658,  0.0088,  ..., -0.0582, -0.0556,  0.0804],\n",
      "       requires_grad=True)\n",
      "epoch   1 loss: 6.58697 time 3.20\n",
      "ebd.weight --->  Parameter containing:\n",
      "tensor([[-1.0180, -1.8558, -0.1748,  ...,  0.8215,  1.4606, -2.6742],\n",
      "        [-0.4939,  1.1077, -1.1486,  ...,  2.0821, -1.4532,  1.2601],\n",
      "        [-0.7652, -0.1749,  0.9713,  ..., -0.6842,  0.3335,  0.6535],\n",
      "        ...,\n",
      "        [ 1.0259, -0.3053, -0.4325,  ...,  1.2294, -0.7571,  2.8377],\n",
      "        [-1.7234,  0.0603,  0.5656,  ...,  0.0094,  2.3275, -1.6497],\n",
      "        [-0.5478, -0.0125, -1.4502,  ...,  0.3223,  0.9581, -0.0769]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_ih_l0 --->  Parameter containing:\n",
      "tensor([[ 0.1458,  0.0134,  0.1276,  ..., -0.1723,  0.1572,  0.1467],\n",
      "        [ 0.2721, -0.0375, -0.1258,  ..., -0.3635, -0.2922, -0.0520],\n",
      "        [ 0.2504,  0.0520, -0.0165,  ..., -0.1956, -0.0295, -0.1623],\n",
      "        ...,\n",
      "        [ 0.3305,  0.1255,  0.0256,  ...,  0.0972, -0.1003, -0.2213],\n",
      "        [ 0.0573, -0.2609, -0.3424,  ...,  0.1294,  0.1565, -0.1953],\n",
      "        [-0.1298, -0.1661,  0.1987,  ...,  0.2367,  0.2188,  0.3406]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_hh_l0 --->  Parameter containing:\n",
      "tensor([[ 0.0842, -0.1095,  0.1600,  ...,  0.0093, -0.0434, -0.0245],\n",
      "        [ 0.2605,  0.0066,  0.1531,  ...,  0.1953,  0.2559, -0.0374],\n",
      "        [ 0.0964,  0.0082,  0.2082,  ...,  0.2277,  0.1007,  0.0023],\n",
      "        ...,\n",
      "        [ 0.3090,  0.1826,  0.1790,  ...,  0.2485,  0.0106,  0.0337],\n",
      "        [ 0.0120,  0.1319,  0.0043,  ..., -0.1948, -0.2793, -0.2888],\n",
      "        [-0.0350, -0.1742, -0.2969,  ..., -0.1446, -0.2442, -0.3049]],\n",
      "       requires_grad=True)\n",
      "rnn.bias_ih_l0 --->  Parameter containing:\n",
      "tensor([-0.2802, -0.3023, -0.3039,  0.2387, -0.0847, -0.3206, -0.0285, -0.0133,\n",
      "        -0.0026,  0.0113,  0.2816, -0.1077,  0.0929,  0.0436, -0.3328, -0.3305,\n",
      "        -0.2151,  0.2096, -0.0148, -0.3381,  0.0854, -0.3193, -0.0245, -0.2515,\n",
      "        -0.2748, -0.0071,  0.2904, -0.2111,  0.3016,  0.0246, -0.2260, -0.0498,\n",
      "        -0.0305, -0.0196, -0.1038, -0.2304,  0.2698,  0.0319, -0.0234,  0.3046,\n",
      "        -0.2349,  0.0789,  0.0610,  0.1653, -0.0997,  0.0321,  0.3132, -0.0110,\n",
      "         0.0395,  0.0365,  0.2524, -0.2751,  0.2898,  0.3123, -0.0185,  0.1804,\n",
      "        -0.0349, -0.0494,  0.0416,  0.0829, -0.0087, -0.0300, -0.2531,  0.2303,\n",
      "        -0.2835,  0.2722, -0.3059, -0.3402,  0.0633,  0.0097, -0.0783,  0.2252,\n",
      "         0.3502,  0.1009,  0.3268,  0.3204,  0.2627,  0.0307,  0.0370,  0.0214,\n",
      "        -0.1258,  0.0644, -0.2704,  0.0062, -0.0643, -0.1509, -0.1577,  0.0905,\n",
      "         0.2388,  0.0177, -0.0036, -0.1718,  0.1326, -0.1798,  0.2097,  0.0541,\n",
      "         0.0620, -0.3014,  0.2300, -0.2164,  0.2609,  0.1539,  0.0568, -0.3284,\n",
      "         0.0444,  0.3213, -0.0160, -0.0566,  0.2149,  0.2770,  0.0991,  0.0695,\n",
      "        -0.0619, -0.3101,  0.0452, -0.0847, -0.2259, -0.3251, -0.0683,  0.0288,\n",
      "         0.0576,  0.2395, -0.2336,  0.3198, -0.3060, -0.2622,  0.0757,  0.0972],\n",
      "       requires_grad=True)\n",
      "rnn.bias_hh_l0 --->  Parameter containing:\n",
      "tensor([-0.2159, -0.3318, -0.1742,  0.3041, -0.0080, -0.3267,  0.0927,  0.0454,\n",
      "        -0.0661, -0.1066,  0.2071, -0.0006,  0.0581,  0.0776, -0.2160, -0.2097,\n",
      "        -0.1973,  0.3135, -0.1016, -0.2877, -0.0791, -0.3482, -0.0181, -0.2407,\n",
      "        -0.2656,  0.0802,  0.1726, -0.1974,  0.2579, -0.0495, -0.3217,  0.0358,\n",
      "        -0.0233,  0.0546, -0.0469, -0.1826,  0.2713, -0.0565, -0.0630,  0.2681,\n",
      "        -0.2616, -0.0123,  0.0026,  0.2037, -0.0613,  0.0792,  0.1968, -0.0206,\n",
      "        -0.0191,  0.0237,  0.2096, -0.2051,  0.2820,  0.2475,  0.0119,  0.2713,\n",
      "        -0.0493,  0.0792,  0.0258, -0.0104,  0.0780, -0.0724, -0.2581,  0.1890,\n",
      "        -0.1618,  0.2594, -0.2861, -0.1786,  0.0071,  0.0446, -0.2548,  0.2589,\n",
      "         0.3063,  0.0439,  0.2336,  0.2987,  0.2532,  0.0975,  0.0173,  0.0311,\n",
      "        -0.0231,  0.0613, -0.3204, -0.0370,  0.0422, -0.2553, -0.1141,  0.0971,\n",
      "         0.3060,  0.1432,  0.0483, -0.2325,  0.2537, -0.1517,  0.2525,  0.0615,\n",
      "        -0.0228, -0.2637,  0.3086, -0.3370,  0.3107,  0.1963, -0.0409, -0.2297,\n",
      "         0.1652,  0.3409,  0.0265, -0.0128,  0.2178,  0.1904, -0.0023,  0.1122,\n",
      "         0.0177, -0.2677,  0.0351,  0.0370, -0.2262, -0.3302, -0.0080,  0.0411,\n",
      "        -0.0743,  0.3070, -0.2290,  0.1726, -0.3251, -0.3169, -0.0137,  0.0585],\n",
      "       requires_grad=True)\n",
      "out.weight --->  Parameter containing:\n",
      "tensor([[-0.0606, -0.3066, -0.3150,  ..., -0.2403,  0.0974,  0.1276],\n",
      "        [-0.2997,  0.1717, -0.3062,  ..., -0.2900, -0.1704, -0.0245],\n",
      "        [ 0.2593,  0.1278,  0.1672,  ..., -0.0571, -0.2000,  0.0722],\n",
      "        ...,\n",
      "        [ 0.2069,  0.1059,  0.2534,  ...,  0.2929, -0.0218, -0.0337],\n",
      "        [ 0.2170,  0.1088,  0.3517,  ...,  0.0665,  0.0567, -0.1000],\n",
      "        [ 0.0716,  0.1706,  0.1900,  ...,  0.1153,  0.0055, -0.2408]],\n",
      "       requires_grad=True)\n",
      "out.bias --->  Parameter containing:\n",
      "tensor([ 0.2449,  0.2023, -0.1426,  ..., -0.3322, -0.3263, -0.1973],\n",
      "       requires_grad=True)\n",
      "epoch   2 loss: 5.00442 time 3.48\n",
      "ebd.weight --->  Parameter containing:\n",
      "tensor([[-1.1316, -1.9521, -0.3958,  ...,  0.7911,  1.2341, -2.6677],\n",
      "        [-0.2683,  1.2081, -1.3142,  ...,  2.1690, -1.7001,  1.3493],\n",
      "        [-0.9579, -0.2959,  1.0132,  ..., -0.4750,  0.4556,  0.8458],\n",
      "        ...,\n",
      "        [ 1.0259, -0.3053, -0.4325,  ...,  1.2294, -0.7571,  2.8377],\n",
      "        [-1.7234,  0.0603,  0.5656,  ...,  0.0094,  2.3275, -1.6497],\n",
      "        [-0.5478, -0.0125, -1.4502,  ...,  0.3223,  0.9581, -0.0769]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_ih_l0 --->  Parameter containing:\n",
      "tensor([[ 0.2139,  0.0786,  0.1716,  ..., -0.2812,  0.1816,  0.2943],\n",
      "        [ 0.3685,  0.0376, -0.1566,  ..., -0.5735, -0.3100, -0.0905],\n",
      "        [ 0.4450, -0.1376, -0.1193,  ..., -0.1708, -0.1370, -0.2215],\n",
      "        ...,\n",
      "        [ 0.5511,  0.0996,  0.0694,  ...,  0.0833, -0.2260, -0.3503],\n",
      "        [-0.0535, -0.4426, -0.5136,  ...,  0.1182,  0.0636, -0.3073],\n",
      "        [-0.2430, -0.1995,  0.3508,  ...,  0.2938,  0.3923,  0.5329]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_hh_l0 --->  Parameter containing:\n",
      "tensor([[ 0.1695, -0.2804,  0.2350,  ...,  0.0734,  0.0642,  0.0470],\n",
      "        [ 0.4062,  0.0435,  0.2842,  ...,  0.3322,  0.3890,  0.0437],\n",
      "        [ 0.1918, -0.0933,  0.3091,  ...,  0.3508,  0.2297,  0.1129],\n",
      "        ...,\n",
      "        [ 0.4340,  0.3663,  0.2962,  ...,  0.3570,  0.0952,  0.0670],\n",
      "        [-0.0493,  0.2581, -0.0737,  ..., -0.2649, -0.3804, -0.3905],\n",
      "        [-0.1886, -0.4057, -0.4500,  ..., -0.3205, -0.2825, -0.4057]],\n",
      "       requires_grad=True)\n",
      "rnn.bias_ih_l0 --->  Parameter containing:\n",
      "tensor([-0.4002, -0.4643, -0.4550,  0.3740, -0.0391, -0.5070, -0.1015, -0.0957,\n",
      "         0.0665, -0.0697,  0.4446, -0.1013,  0.1664,  0.1726, -0.3231, -0.4920,\n",
      "        -0.2910,  0.3241, -0.0294, -0.5119,  0.1174, -0.4996, -0.0773, -0.4102,\n",
      "        -0.4634,  0.0227,  0.4078, -0.3466,  0.4599,  0.0543, -0.3583, -0.1116,\n",
      "         0.0722, -0.0076, -0.1378, -0.2793,  0.4198,  0.1006, -0.1102,  0.4325,\n",
      "        -0.3566,  0.1598, -0.0439,  0.2228, -0.1817,  0.0292,  0.4443,  0.0978,\n",
      "         0.0016, -0.1389,  0.3902, -0.4375,  0.3999,  0.4663, -0.1222,  0.2905,\n",
      "        -0.1428, -0.1422,  0.0613,  0.1463, -0.0117, -0.1127, -0.4241,  0.3533,\n",
      "        -0.3936,  0.3678, -0.4444, -0.5037, -0.0243,  0.0118, -0.1594,  0.3929,\n",
      "         0.5837,  0.2892,  0.4892,  0.4897,  0.4039,  0.0418,  0.0968,  0.0748,\n",
      "        -0.0897,  0.1513, -0.3131,  0.0240, -0.1699, -0.2802, -0.1861,  0.1906,\n",
      "         0.3693,  0.0282,  0.1129, -0.2663,  0.2267, -0.1479,  0.3271,  0.1352,\n",
      "         0.1139, -0.4583,  0.3638, -0.3639,  0.3777,  0.1919,  0.1046, -0.4559,\n",
      "         0.0301,  0.4841,  0.0581, -0.1517,  0.3603,  0.3085,  0.2194,  0.2061,\n",
      "        -0.1577, -0.4448,  0.1203, -0.1406, -0.3283, -0.4829, -0.0555,  0.1266,\n",
      "         0.0965,  0.3692, -0.3307,  0.4668, -0.4607, -0.4260,  0.1623,  0.2351],\n",
      "       requires_grad=True)\n",
      "rnn.bias_hh_l0 --->  Parameter containing:\n",
      "tensor([-0.3360, -0.4938, -0.3253,  0.4394,  0.0376, -0.5132,  0.0197, -0.0371,\n",
      "         0.0031, -0.1875,  0.3701,  0.0057,  0.1316,  0.2066, -0.2063, -0.3711,\n",
      "        -0.2731,  0.4280, -0.1162, -0.4615, -0.0472, -0.5285, -0.0709, -0.3994,\n",
      "        -0.4542,  0.1099,  0.2900, -0.3328,  0.4162, -0.0198, -0.4540, -0.0260,\n",
      "         0.0794,  0.0666, -0.0809, -0.2314,  0.4213,  0.0121, -0.1498,  0.3959,\n",
      "        -0.3833,  0.0686, -0.1023,  0.2613, -0.1434,  0.0763,  0.3279,  0.0882,\n",
      "        -0.0570, -0.1517,  0.3474, -0.3675,  0.3921,  0.4014, -0.0918,  0.3814,\n",
      "        -0.1572, -0.0136,  0.0455,  0.0530,  0.0751, -0.1551, -0.4291,  0.3120,\n",
      "        -0.2719,  0.3550, -0.4246, -0.3421, -0.0805,  0.0467, -0.3358,  0.4266,\n",
      "         0.5399,  0.2321,  0.3959,  0.4680,  0.3944,  0.1086,  0.0772,  0.0844,\n",
      "         0.0130,  0.1482, -0.3631, -0.0192, -0.0633, -0.3845, -0.1424,  0.1972,\n",
      "         0.4365,  0.1537,  0.1648, -0.3271,  0.3479, -0.1199,  0.3699,  0.1426,\n",
      "         0.0291, -0.4207,  0.4423, -0.4845,  0.4275,  0.2342,  0.0068, -0.3572,\n",
      "         0.1509,  0.5037,  0.1006, -0.1078,  0.3632,  0.2219,  0.1180,  0.2488,\n",
      "        -0.0781, -0.4025,  0.1102, -0.0189, -0.3286, -0.4879,  0.0048,  0.1390,\n",
      "        -0.0354,  0.4367, -0.3261,  0.3196, -0.4799, -0.4807,  0.0729,  0.1964],\n",
      "       requires_grad=True)\n",
      "out.weight --->  Parameter containing:\n",
      "tensor([[-0.2024, -0.3371, -0.4281,  ..., -0.4082, -0.0651,  0.0265],\n",
      "        [-0.2929,  0.3183, -0.2944,  ..., -0.2724, -0.2831, -0.0072],\n",
      "        [ 0.2834,  0.0325,  0.0326,  ..., -0.0500, -0.2173,  0.1526],\n",
      "        ...,\n",
      "        [ 0.3411,  0.1256,  0.3957,  ...,  0.4443,  0.0749, -0.0070],\n",
      "        [ 0.3492,  0.2138,  0.4622,  ...,  0.1885,  0.1650, -0.1435],\n",
      "        [ 0.0440,  0.2933,  0.3207,  ...,  0.2266,  0.1173, -0.3502]],\n",
      "       requires_grad=True)\n",
      "out.bias --->  Parameter containing:\n",
      "tensor([ 0.4231,  0.1960, -0.1769,  ..., -0.5050, -0.4997, -0.3650],\n",
      "       requires_grad=True)\n",
      "epoch   3 loss: 3.78121 time 3.06\n",
      "ebd.weight --->  Parameter containing:\n",
      "tensor([[-1.2756, -2.0055, -0.5717,  ...,  0.8393,  1.1702, -2.6981],\n",
      "        [-0.1515,  1.3329, -1.4850,  ...,  2.2697, -1.8759,  1.4899],\n",
      "        [-1.1381, -0.2431,  1.1041,  ..., -0.2832,  0.6269,  0.9724],\n",
      "        ...,\n",
      "        [ 1.0259, -0.3053, -0.4325,  ...,  1.2294, -0.7571,  2.8377],\n",
      "        [-1.7234,  0.0603,  0.5656,  ...,  0.0094,  2.3275, -1.6497],\n",
      "        [-0.5478, -0.0125, -1.4502,  ...,  0.3223,  0.9581, -0.0769]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_ih_l0 --->  Parameter containing:\n",
      "tensor([[ 0.2190,  0.1857,  0.0649,  ..., -0.4026,  0.1928,  0.3373],\n",
      "        [ 0.4531,  0.1109, -0.2746,  ..., -0.6953, -0.3866, -0.0252],\n",
      "        [ 0.6824, -0.3604, -0.2837,  ..., -0.1444, -0.2847, -0.1706],\n",
      "        ...,\n",
      "        [ 0.6609,  0.1865,  0.1232,  ...,  0.1424, -0.3460, -0.4351],\n",
      "        [-0.1478, -0.5938, -0.6088,  ...,  0.1285,  0.0204, -0.3573],\n",
      "        [-0.2498, -0.2709,  0.4595,  ...,  0.2975,  0.4039,  0.5433]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_hh_l0 --->  Parameter containing:\n",
      "tensor([[ 0.2125, -0.3049,  0.2808,  ...,  0.1360,  0.1347,  0.0717],\n",
      "        [ 0.4980, -0.0430,  0.3576,  ...,  0.3866,  0.5014,  0.1156],\n",
      "        [ 0.2467, -0.1812,  0.3288,  ...,  0.4167,  0.3141,  0.2143],\n",
      "        ...,\n",
      "        [ 0.5176,  0.4680,  0.3785,  ...,  0.4259,  0.1554,  0.0423],\n",
      "        [-0.0544,  0.4504, -0.0999,  ..., -0.2756, -0.4389, -0.4863],\n",
      "        [-0.2971, -0.6123, -0.5712,  ..., -0.4752, -0.2950, -0.4419]],\n",
      "       requires_grad=True)\n",
      "rnn.bias_ih_l0 --->  Parameter containing:\n",
      "tensor([-0.4873, -0.5688, -0.5490,  0.4565,  0.0199, -0.6493, -0.1412, -0.1650,\n",
      "         0.1376, -0.1297,  0.5600, -0.1260,  0.2008,  0.2246, -0.3711, -0.6031,\n",
      "        -0.3386,  0.4026, -0.0368, -0.6573,  0.1502, -0.6462, -0.1197, -0.5342,\n",
      "        -0.5911,  0.0202,  0.4929, -0.4480,  0.5740,  0.0488, -0.4119, -0.1472,\n",
      "         0.1501, -0.0098, -0.1871, -0.3505,  0.4827,  0.1424, -0.1489,  0.5292,\n",
      "        -0.4409,  0.2007, -0.0914,  0.2337, -0.2413,  0.0104,  0.5343,  0.1416,\n",
      "        -0.0400, -0.2612,  0.4862, -0.5502,  0.4727,  0.5638, -0.1986,  0.3527,\n",
      "        -0.2218, -0.2083,  0.0592,  0.1823, -0.0130, -0.1720, -0.5601,  0.4444,\n",
      "        -0.4810,  0.4268, -0.5267, -0.6216, -0.0883,  0.0018, -0.2243,  0.4860,\n",
      "         0.7128,  0.4024,  0.6007,  0.6157,  0.4923,  0.0284,  0.1515,  0.1502,\n",
      "        -0.0739,  0.1956, -0.3123,  0.0517, -0.1876, -0.3791, -0.1836,  0.2722,\n",
      "         0.4414,  0.0050,  0.1914, -0.3363,  0.2922, -0.2129,  0.4129,  0.1649,\n",
      "         0.1381, -0.5743,  0.4625, -0.4095,  0.4708,  0.2153,  0.1283, -0.5350,\n",
      "        -0.0317,  0.6062,  0.0961, -0.2374,  0.4668,  0.2910,  0.2952,  0.3042,\n",
      "        -0.2229, -0.5471,  0.1759, -0.1898, -0.3799, -0.6039, -0.0594,  0.1929,\n",
      "         0.1088,  0.4533, -0.3762,  0.5689, -0.5801, -0.5404,  0.1967,  0.3448],\n",
      "       requires_grad=True)\n",
      "rnn.bias_hh_l0 --->  Parameter containing:\n",
      "tensor([-0.4230, -0.5983, -0.4193,  0.5218,  0.0967, -0.6554, -0.0200, -0.1063,\n",
      "         0.0741, -0.2475,  0.4856, -0.0189,  0.1661,  0.2586, -0.2542, -0.4822,\n",
      "        -0.3208,  0.5065, -0.1236, -0.6069, -0.0143, -0.6751, -0.1134, -0.5234,\n",
      "        -0.5819,  0.1075,  0.3751, -0.4342,  0.5303, -0.0252, -0.5076, -0.0616,\n",
      "         0.1572,  0.0644, -0.1301, -0.3027,  0.4842,  0.0539, -0.1885,  0.4927,\n",
      "        -0.4676,  0.1095, -0.1497,  0.2721, -0.2029,  0.0574,  0.4179,  0.1320,\n",
      "        -0.0986, -0.2740,  0.4435, -0.4802,  0.4649,  0.4989, -0.1682,  0.4436,\n",
      "        -0.2362, -0.0797,  0.0434,  0.0891,  0.0738, -0.2144, -0.5651,  0.4031,\n",
      "        -0.3592,  0.4140, -0.5069, -0.4600, -0.1445,  0.0366, -0.4007,  0.5197,\n",
      "         0.6689,  0.3453,  0.5074,  0.5939,  0.4828,  0.0953,  0.1319,  0.1598,\n",
      "         0.0289,  0.1925, -0.3623,  0.0085, -0.0811, -0.4834, -0.1399,  0.2788,\n",
      "         0.5086,  0.1305,  0.2434, -0.3970,  0.4133, -0.1849,  0.4557,  0.1723,\n",
      "         0.0533, -0.5366,  0.5410, -0.5301,  0.5205,  0.2577,  0.0305, -0.4362,\n",
      "         0.0891,  0.6257,  0.1385, -0.1935,  0.4697,  0.2044,  0.1938,  0.3469,\n",
      "        -0.1433, -0.5048,  0.1658, -0.0680, -0.3801, -0.6089,  0.0010,  0.2053,\n",
      "        -0.0231,  0.5208, -0.3716,  0.4217, -0.5993, -0.5951,  0.1073,  0.3061],\n",
      "       requires_grad=True)\n",
      "out.weight --->  Parameter containing:\n",
      "tensor([[-0.2897, -0.3900, -0.4533,  ..., -0.5672, -0.1840, -0.1198],\n",
      "        [-0.2287,  0.4007, -0.1858,  ..., -0.2069, -0.3647,  0.0769],\n",
      "        [ 0.2121, -0.0424, -0.1115,  ...,  0.0031, -0.1617,  0.1278],\n",
      "        ...,\n",
      "        [ 0.4263,  0.1478,  0.4829,  ...,  0.5565,  0.1528,  0.0026],\n",
      "        [ 0.4316,  0.3000,  0.5059,  ...,  0.2896,  0.2566, -0.1819],\n",
      "        [ 0.1039,  0.3609,  0.3965,  ...,  0.2810,  0.1934, -0.3765]],\n",
      "       requires_grad=True)\n",
      "out.bias --->  Parameter containing:\n",
      "tensor([ 0.5654,  0.1322, -0.2605,  ..., -0.6276, -0.6251, -0.4864],\n",
      "       requires_grad=True)\n",
      "epoch   4 loss: 3.53076 time 3.21\n",
      "ebd.weight --->  Parameter containing:\n",
      "tensor([[-1.4107, -2.0087, -0.7358,  ...,  0.8358,  1.2228, -2.7181],\n",
      "        [-0.2999,  1.5310, -1.5138,  ...,  2.3954, -1.8066,  1.6310],\n",
      "        [-1.3161, -0.0891,  1.2133,  ..., -0.1050,  0.7707,  1.1039],\n",
      "        ...,\n",
      "        [ 1.0259, -0.3053, -0.4325,  ...,  1.2294, -0.7571,  2.8377],\n",
      "        [-1.7234,  0.0603,  0.5656,  ...,  0.0094,  2.3275, -1.6497],\n",
      "        [-0.5478, -0.0125, -1.4502,  ...,  0.3223,  0.9581, -0.0769]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_ih_l0 --->  Parameter containing:\n",
      "tensor([[ 0.3081,  0.2266, -0.0694,  ..., -0.5628,  0.1422,  0.3203],\n",
      "        [ 0.4782,  0.1186, -0.4096,  ..., -0.8339, -0.4651,  0.0077],\n",
      "        [ 0.8100, -0.5017, -0.3374,  ..., -0.0877, -0.3531, -0.1085],\n",
      "        ...,\n",
      "        [ 0.7119,  0.2488,  0.0795,  ...,  0.2128, -0.4512, -0.4781],\n",
      "        [-0.2268, -0.6845, -0.6480,  ...,  0.1473, -0.0413, -0.3766],\n",
      "        [-0.2113, -0.3284,  0.5912,  ...,  0.2672,  0.3816,  0.4952]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_hh_l0 --->  Parameter containing:\n",
      "tensor([[ 0.2219, -0.3694,  0.2912,  ...,  0.1651,  0.1710,  0.1154],\n",
      "        [ 0.5751, -0.1848,  0.4146,  ...,  0.4021,  0.6030,  0.1715],\n",
      "        [ 0.2765, -0.2131,  0.3260,  ...,  0.4737,  0.3534,  0.2754],\n",
      "        ...,\n",
      "        [ 0.5760,  0.5376,  0.4215,  ...,  0.4697,  0.1569,  0.0353],\n",
      "        [-0.0537,  0.5684, -0.1241,  ..., -0.2840, -0.4843, -0.5408],\n",
      "        [-0.3786, -0.7633, -0.6857,  ..., -0.6014, -0.3171, -0.4500]],\n",
      "       requires_grad=True)\n",
      "rnn.bias_ih_l0 --->  Parameter containing:\n",
      "tensor([-0.5349, -0.6388, -0.6224,  0.5148,  0.0677, -0.7559, -0.1748, -0.2163,\n",
      "         0.1884, -0.1706,  0.6344, -0.1528,  0.2183,  0.2471, -0.4036, -0.6588,\n",
      "        -0.3623,  0.4568, -0.0312, -0.7602,  0.1674, -0.7659, -0.1470, -0.6279,\n",
      "        -0.6942,  0.0438,  0.5545, -0.5630,  0.6564,  0.0541, -0.4752, -0.1830,\n",
      "         0.1921, -0.0117, -0.2034, -0.3919,  0.5518,  0.1715, -0.1986,  0.5901,\n",
      "        -0.4963,  0.2169, -0.1162,  0.2570, -0.2768, -0.0180,  0.6250,  0.1712,\n",
      "        -0.0604, -0.3748,  0.5579, -0.6390,  0.5179,  0.6435, -0.2462,  0.4055,\n",
      "        -0.2761, -0.2595,  0.0696,  0.2259, -0.0267, -0.2122, -0.6607,  0.5175,\n",
      "        -0.5447,  0.4773, -0.5722, -0.7159, -0.1190,  0.0081, -0.2432,  0.5670,\n",
      "         0.8285,  0.5440,  0.6813,  0.7160,  0.5580,  0.0284,  0.1717,  0.2127,\n",
      "        -0.0854,  0.2322, -0.3153,  0.0530, -0.2042, -0.4630, -0.2357,  0.3254,\n",
      "         0.5160, -0.0131,  0.2533, -0.3745,  0.3385, -0.2881,  0.4750,  0.1859,\n",
      "         0.1535, -0.6606,  0.5324, -0.4454,  0.5315,  0.2370,  0.1320, -0.5939,\n",
      "        -0.1071,  0.7015,  0.1056, -0.3034,  0.5467,  0.2822,  0.3368,  0.3820,\n",
      "        -0.2699, -0.5954,  0.2223, -0.2422, -0.4239, -0.6958, -0.0817,  0.2610,\n",
      "         0.1136,  0.5198, -0.4128,  0.6464, -0.6876, -0.6194,  0.2228,  0.4340],\n",
      "       requires_grad=True)\n",
      "rnn.bias_hh_l0 --->  Parameter containing:\n",
      "tensor([-0.4706, -0.6683, -0.4927,  0.5801,  0.1445, -0.7621, -0.0536, -0.1576,\n",
      "         0.1250, -0.2885,  0.5600, -0.0458,  0.1835,  0.2811, -0.2868, -0.5379,\n",
      "        -0.3445,  0.5607, -0.1180, -0.7098,  0.0029, -0.7948, -0.1406, -0.6172,\n",
      "        -0.6850,  0.1310,  0.4367, -0.5493,  0.6126, -0.0199, -0.5708, -0.0973,\n",
      "         0.1993,  0.0626, -0.1465, -0.3441,  0.5534,  0.0830, -0.2382,  0.5536,\n",
      "        -0.5231,  0.1257, -0.1746,  0.2955, -0.2384,  0.0290,  0.5085,  0.1616,\n",
      "        -0.1190, -0.3876,  0.5151, -0.5690,  0.5101,  0.5786, -0.2158,  0.4964,\n",
      "        -0.2905, -0.1309,  0.0539,  0.1326,  0.0601, -0.2545, -0.6657,  0.4762,\n",
      "        -0.4230,  0.4645, -0.5523, -0.5544, -0.1752,  0.0430, -0.4196,  0.6007,\n",
      "         0.7846,  0.4869,  0.5880,  0.6943,  0.5485,  0.0952,  0.1521,  0.2223,\n",
      "         0.0173,  0.2291, -0.3653,  0.0098, -0.0977, -0.5674, -0.1921,  0.3320,\n",
      "         0.5833,  0.1125,  0.3052, -0.4352,  0.4596, -0.2600,  0.5178,  0.1933,\n",
      "         0.0687, -0.6229,  0.6109, -0.5660,  0.5813,  0.2793,  0.0343, -0.4952,\n",
      "         0.0137,  0.7210,  0.1481, -0.2596,  0.5496,  0.1956,  0.2354,  0.4247,\n",
      "        -0.1903, -0.5531,  0.2122, -0.1205, -0.4241, -0.7008, -0.0213,  0.2733,\n",
      "        -0.0183,  0.5873, -0.4082,  0.4992, -0.7068, -0.6741,  0.1334,  0.3953],\n",
      "       requires_grad=True)\n",
      "out.weight --->  Parameter containing:\n",
      "tensor([[-0.3111, -0.3546, -0.4233,  ..., -0.6734, -0.2283, -0.2058],\n",
      "        [-0.2030,  0.4182, -0.0593,  ..., -0.1937, -0.3864,  0.1545],\n",
      "        [ 0.1681, -0.1176, -0.2490,  ...,  0.0010, -0.1542,  0.1309],\n",
      "        ...,\n",
      "        [ 0.4864,  0.1637,  0.5448,  ...,  0.6355,  0.2086,  0.0091],\n",
      "        [ 0.4905,  0.3597,  0.5362,  ...,  0.3611,  0.3223, -0.2094],\n",
      "        [ 0.1772,  0.4024,  0.4470,  ...,  0.2995,  0.2504, -0.3852]],\n",
      "       requires_grad=True)\n",
      "out.bias --->  Parameter containing:\n",
      "tensor([ 0.6586,  0.1226, -0.2952,  ..., -0.7150, -0.7144, -0.5765],\n",
      "       requires_grad=True)\n",
      "epoch   5 loss: 3.06548 time 3.01\n",
      "ebd.weight --->  Parameter containing:\n",
      "tensor([[-1.5445, -2.0692, -0.8249,  ...,  0.8571,  1.3468, -2.6470],\n",
      "        [-0.4357,  1.6697, -1.4762,  ...,  2.5014, -1.7028,  1.7624],\n",
      "        [-1.5050, -0.0234,  1.2742,  ...,  0.0731,  0.7908,  1.2222],\n",
      "        ...,\n",
      "        [ 1.0259, -0.3053, -0.4325,  ...,  1.2294, -0.7571,  2.8377],\n",
      "        [-1.7234,  0.0603,  0.5656,  ...,  0.0094,  2.3275, -1.6497],\n",
      "        [-0.5478, -0.0125, -1.4502,  ...,  0.3223,  0.9581, -0.0769]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_ih_l0 --->  Parameter containing:\n",
      "tensor([[ 0.3989,  0.2284, -0.2703,  ..., -0.7453,  0.0254,  0.2806],\n",
      "        [ 0.5479,  0.0371, -0.6115,  ..., -0.9785, -0.5908,  0.0246],\n",
      "        [ 0.9056, -0.6211, -0.3913,  ..., -0.0969, -0.4011, -0.0347],\n",
      "        ...,\n",
      "        [ 0.8531,  0.2859,  0.0050,  ...,  0.2370, -0.5735, -0.5278],\n",
      "        [-0.3369, -0.6772, -0.6916,  ...,  0.1744, -0.1217, -0.3595],\n",
      "        [-0.2131, -0.3757,  0.7029,  ...,  0.2972,  0.4488,  0.4643]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_hh_l0 --->  Parameter containing:\n",
      "tensor([[ 0.2236, -0.5223,  0.2483,  ...,  0.1551,  0.1721,  0.1903],\n",
      "        [ 0.6729, -0.3540,  0.4412,  ...,  0.3691,  0.6772,  0.2505],\n",
      "        [ 0.2947, -0.2309,  0.3280,  ...,  0.5267,  0.3771,  0.3188],\n",
      "        ...,\n",
      "        [ 0.6219,  0.5651,  0.4578,  ...,  0.4718,  0.1574,  0.0560],\n",
      "        [-0.0551,  0.6430, -0.1229,  ..., -0.3037, -0.5072, -0.5641],\n",
      "        [-0.4322, -0.8348, -0.7575,  ..., -0.6650, -0.3388, -0.4936]],\n",
      "       requires_grad=True)\n",
      "rnn.bias_ih_l0 --->  Parameter containing:\n",
      "tensor([-0.5416, -0.6765, -0.6788,  0.5629,  0.1259, -0.8381, -0.1964, -0.2598,\n",
      "         0.2081, -0.1951,  0.6780, -0.1675,  0.2247,  0.2562, -0.4407, -0.6951,\n",
      "        -0.3572,  0.5002, -0.0195, -0.8371,  0.1679, -0.8631, -0.1686, -0.6907,\n",
      "        -0.7775,  0.0650,  0.6001, -0.6837,  0.7049,  0.0615, -0.5532, -0.2183,\n",
      "         0.2221, -0.0058, -0.2151, -0.4147,  0.6119,  0.1896, -0.2655,  0.6261,\n",
      "        -0.5370,  0.2225, -0.1503,  0.2799, -0.2947, -0.0556,  0.6984,  0.1951,\n",
      "        -0.0663, -0.4665,  0.6130, -0.6948,  0.5411,  0.7001, -0.2849,  0.4472,\n",
      "        -0.3174, -0.3004,  0.0894,  0.2641, -0.0332, -0.2345, -0.7420,  0.5684,\n",
      "        -0.5925,  0.5168, -0.6110, -0.7868, -0.1317,  0.0184, -0.2714,  0.6338,\n",
      "         0.9221,  0.6903,  0.7204,  0.7641,  0.6076,  0.0352,  0.1854,  0.2395,\n",
      "        -0.1037,  0.2612, -0.3047,  0.0446, -0.2138, -0.5347, -0.2946,  0.3640,\n",
      "         0.5737,  0.0023,  0.3021, -0.4062,  0.3708, -0.3065,  0.5228,  0.2054,\n",
      "         0.1681, -0.7199,  0.5778, -0.4741,  0.5695,  0.2536,  0.1293, -0.6344,\n",
      "        -0.1480,  0.7781,  0.1049, -0.3538,  0.6021,  0.2943,  0.3726,  0.4292,\n",
      "        -0.2790, -0.6237,  0.2561, -0.2954, -0.4448, -0.7668, -0.1079,  0.3163,\n",
      "         0.1125,  0.5683, -0.4366,  0.7128, -0.7733, -0.6712,  0.2502,  0.4944],\n",
      "       requires_grad=True)\n",
      "rnn.bias_hh_l0 --->  Parameter containing:\n",
      "tensor([-0.4773, -0.7061, -0.5491,  0.6282,  0.2027, -0.8443, -0.0752, -0.2011,\n",
      "         0.1446, -0.3129,  0.6036, -0.0605,  0.1899,  0.2902, -0.3239, -0.5742,\n",
      "        -0.3394,  0.6041, -0.1063, -0.7867,  0.0034, -0.8920, -0.1623, -0.6799,\n",
      "        -0.7683,  0.1522,  0.4823, -0.6699,  0.6611, -0.0126, -0.6489, -0.1327,\n",
      "         0.2293,  0.0685, -0.1582, -0.3669,  0.6134,  0.1011, -0.3051,  0.5896,\n",
      "        -0.5638,  0.1314, -0.2087,  0.3184, -0.2563, -0.0086,  0.5819,  0.1855,\n",
      "        -0.1249, -0.4793,  0.5703, -0.6248,  0.5333,  0.6352, -0.2545,  0.5381,\n",
      "        -0.3318, -0.1719,  0.0736,  0.1709,  0.0536, -0.2768, -0.7469,  0.5271,\n",
      "        -0.4708,  0.5040, -0.5912, -0.6252, -0.1878,  0.0533, -0.4478,  0.6675,\n",
      "         0.8782,  0.6332,  0.6271,  0.7424,  0.5981,  0.1020,  0.1657,  0.2492,\n",
      "        -0.0010,  0.2582, -0.3547,  0.0014, -0.1073, -0.6391, -0.2509,  0.3706,\n",
      "         0.6409,  0.1278,  0.3540, -0.4670,  0.4919, -0.2785,  0.5656,  0.2128,\n",
      "         0.0833, -0.6822,  0.6563, -0.5947,  0.6192,  0.2959,  0.0316, -0.5357,\n",
      "        -0.0272,  0.7977,  0.1473, -0.3100,  0.6050,  0.2078,  0.2712,  0.4718,\n",
      "        -0.1994, -0.5814,  0.2460, -0.1736, -0.4451, -0.7719, -0.0476,  0.3287,\n",
      "        -0.0194,  0.6358, -0.4319,  0.5655, -0.7925, -0.7259,  0.1608,  0.4557],\n",
      "       requires_grad=True)\n",
      "out.weight --->  Parameter containing:\n",
      "tensor([[-0.2808, -0.2570, -0.3915,  ..., -0.7804, -0.2793, -0.2183],\n",
      "        [-0.1611,  0.4141,  0.0786,  ..., -0.2095, -0.3688,  0.2410],\n",
      "        [ 0.1431, -0.2247, -0.3485,  ..., -0.0284, -0.1447,  0.1354],\n",
      "        ...,\n",
      "        [ 0.5298,  0.1740,  0.5901,  ...,  0.6931,  0.2494,  0.0141],\n",
      "        [ 0.5330,  0.4009,  0.5576,  ...,  0.4133,  0.3700, -0.2293],\n",
      "        [ 0.2347,  0.4312,  0.4833,  ...,  0.3097,  0.2928, -0.3891]],\n",
      "       requires_grad=True)\n",
      "out.bias --->  Parameter containing:\n",
      "tensor([ 0.7659,  0.1486, -0.3227,  ..., -0.7791, -0.7799, -0.6431],\n",
      "       requires_grad=True)\n",
      "epoch   6 loss: 2.57918 time 3.40\n",
      "ebd.weight --->  Parameter containing:\n",
      "tensor([[-1.6900, -2.0816, -0.8765,  ...,  0.8639,  1.4363, -2.6187],\n",
      "        [-0.5213,  1.7881, -1.4569,  ...,  2.5816, -1.6429,  1.8551],\n",
      "        [-1.5954, -0.0175,  1.3168,  ...,  0.1870,  0.7884,  1.3052],\n",
      "        ...,\n",
      "        [ 1.0259, -0.3053, -0.4325,  ...,  1.2294, -0.7571,  2.8377],\n",
      "        [-1.7234,  0.0603,  0.5656,  ...,  0.0094,  2.3275, -1.6497],\n",
      "        [-0.5478, -0.0125, -1.4502,  ...,  0.3223,  0.9581, -0.0769]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_ih_l0 --->  Parameter containing:\n",
      "tensor([[ 0.4674,  0.2631, -0.4139,  ..., -0.8719, -0.0249,  0.3082],\n",
      "        [ 0.6296, -0.0647, -0.7809,  ..., -0.9985, -0.6665,  0.0827],\n",
      "        [ 0.9638, -0.6884, -0.4008,  ..., -0.1162, -0.4393,  0.0539],\n",
      "        ...,\n",
      "        [ 1.0139,  0.3200, -0.0017,  ...,  0.2073, -0.6933, -0.5714],\n",
      "        [-0.4299, -0.6208, -0.7420,  ...,  0.1819, -0.1825, -0.3241],\n",
      "        [-0.2763, -0.4034,  0.8241,  ...,  0.3385,  0.5801,  0.4867]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_hh_l0 --->  Parameter containing:\n",
      "tensor([[ 0.2168, -0.6397,  0.2140,  ...,  0.1599,  0.1682,  0.2351],\n",
      "        [ 0.7616, -0.4648,  0.4738,  ...,  0.3494,  0.7408,  0.2894],\n",
      "        [ 0.2986, -0.2313,  0.3324,  ...,  0.5691,  0.3924,  0.3608],\n",
      "        ...,\n",
      "        [ 0.6397,  0.5957,  0.4928,  ...,  0.4592,  0.1548,  0.0583],\n",
      "        [-0.0692,  0.7343, -0.1046,  ..., -0.3036, -0.5214, -0.5760],\n",
      "        [-0.4645, -0.8455, -0.7793,  ..., -0.7049, -0.3598, -0.5474]],\n",
      "       requires_grad=True)\n",
      "rnn.bias_ih_l0 --->  Parameter containing:\n",
      "tensor([-0.5463, -0.7144, -0.7189,  0.6001,  0.1825, -0.9038, -0.2074, -0.3096,\n",
      "         0.2114, -0.2151,  0.7179, -0.1719,  0.2246,  0.2639, -0.4992, -0.7363,\n",
      "        -0.3555,  0.5370, -0.0299, -0.8897,  0.1688, -0.9368, -0.1873, -0.7346,\n",
      "        -0.8450,  0.0540,  0.6316, -0.7704,  0.7499,  0.0864, -0.6052, -0.2392,\n",
      "         0.2526, -0.0015, -0.2152, -0.4359,  0.6623,  0.2066, -0.3161,  0.6497,\n",
      "        -0.5664,  0.2291, -0.1756,  0.3042, -0.3135, -0.0750,  0.7511,  0.2157,\n",
      "        -0.0492, -0.5176,  0.6484, -0.7384,  0.5610,  0.7419, -0.3257,  0.4768,\n",
      "        -0.3474, -0.3450,  0.0955,  0.2838, -0.0365, -0.2449, -0.8029,  0.6068,\n",
      "        -0.6281,  0.5438, -0.6466, -0.8234, -0.1443,  0.0298, -0.3120,  0.6908,\n",
      "         1.0181,  0.8163,  0.7542,  0.8070,  0.6493,  0.0359,  0.1991,  0.2521,\n",
      "        -0.1333,  0.2838, -0.3153,  0.0159, -0.2120, -0.5911, -0.3409,  0.3910,\n",
      "         0.6311,  0.0062,  0.3421, -0.4292,  0.3927, -0.3124,  0.5635,  0.2167,\n",
      "         0.1827, -0.7667,  0.6123, -0.4884,  0.6097,  0.2675,  0.1257, -0.6726,\n",
      "        -0.1593,  0.8401,  0.1120, -0.3983,  0.6368,  0.2890,  0.4082,  0.4649,\n",
      "        -0.3199, -0.6558,  0.2763, -0.3257, -0.4766, -0.8266, -0.1308,  0.3554,\n",
      "         0.0880,  0.6114, -0.4446,  0.7692, -0.8397, -0.7106,  0.2665,  0.5332],\n",
      "       requires_grad=True)\n",
      "rnn.bias_hh_l0 --->  Parameter containing:\n",
      "tensor([-0.4820, -0.7440, -0.5892,  0.6655,  0.2593, -0.9099, -0.0863, -0.2509,\n",
      "         0.1479, -0.3330,  0.6435, -0.0648,  0.1898,  0.2979, -0.3823, -0.6154,\n",
      "        -0.3377,  0.6409, -0.1167, -0.8392,  0.0043, -0.9658, -0.1810, -0.7238,\n",
      "        -0.8358,  0.1412,  0.5138, -0.7566,  0.7062,  0.0124, -0.7008, -0.1536,\n",
      "         0.2598,  0.0727, -0.1583, -0.3880,  0.6638,  0.1181, -0.3557,  0.6131,\n",
      "        -0.5931,  0.1379, -0.2339,  0.3426, -0.2752, -0.0280,  0.6346,  0.2061,\n",
      "        -0.1078, -0.5304,  0.6056, -0.6684,  0.5532,  0.6770, -0.2953,  0.5677,\n",
      "        -0.3618, -0.2165,  0.0798,  0.1905,  0.0502, -0.2872, -0.8079,  0.5655,\n",
      "        -0.5063,  0.5310, -0.6267, -0.6619, -0.2005,  0.0647, -0.4884,  0.7245,\n",
      "         0.9742,  0.7592,  0.6610,  0.7853,  0.6398,  0.1027,  0.1794,  0.2617,\n",
      "        -0.0306,  0.2808, -0.3653, -0.0273, -0.1054, -0.6955, -0.2973,  0.3976,\n",
      "         0.6983,  0.1317,  0.3940, -0.4899,  0.5138, -0.2843,  0.6063,  0.2241,\n",
      "         0.0979, -0.7290,  0.6908, -0.6090,  0.6594,  0.3099,  0.0279, -0.5739,\n",
      "        -0.0386,  0.8597,  0.1544, -0.3545,  0.6397,  0.2024,  0.3068,  0.5076,\n",
      "        -0.2403, -0.6134,  0.2662, -0.2039, -0.4768, -0.8316, -0.0705,  0.3678,\n",
      "        -0.0438,  0.6789, -0.4400,  0.6219, -0.8589, -0.7652,  0.1771,  0.4945],\n",
      "       requires_grad=True)\n",
      "out.weight --->  Parameter containing:\n",
      "tensor([[-0.2463, -0.1440, -0.3074,  ..., -0.8143, -0.2771, -0.2407],\n",
      "        [-0.0985,  0.4327,  0.2239,  ..., -0.2476, -0.3230,  0.3370],\n",
      "        [ 0.1152, -0.3469, -0.4741,  ..., -0.0793, -0.1347,  0.1333],\n",
      "        ...,\n",
      "        [ 0.5607,  0.1782,  0.6221,  ...,  0.7374,  0.2787,  0.0180],\n",
      "        [ 0.5564,  0.4049,  0.5634,  ...,  0.4563,  0.3977, -0.2426],\n",
      "        [ 0.2782,  0.4519,  0.5096,  ...,  0.3160,  0.3243, -0.3908]],\n",
      "       requires_grad=True)\n",
      "out.bias --->  Parameter containing:\n",
      "tensor([ 0.8226,  0.2003, -0.3208,  ..., -0.8273, -0.8303, -0.6930],\n",
      "       requires_grad=True)\n",
      "epoch   7 loss: 2.31404 time 3.26\n",
      "ebd.weight --->  Parameter containing:\n",
      "tensor([[-1.7761e+00, -2.0016e+00, -8.9482e-01,  ...,  8.7288e-01,\n",
      "          1.5025e+00, -2.6968e+00],\n",
      "        [-5.7747e-01,  1.8785e+00, -1.4505e+00,  ...,  2.6412e+00,\n",
      "         -1.5970e+00,  1.8969e+00],\n",
      "        [-1.6664e+00,  6.8629e-04,  1.3397e+00,  ...,  2.7780e-01,\n",
      "          7.4457e-01,  1.3702e+00],\n",
      "        ...,\n",
      "        [ 1.0259e+00, -3.0533e-01, -4.3247e-01,  ...,  1.2294e+00,\n",
      "         -7.5713e-01,  2.8377e+00],\n",
      "        [-1.7234e+00,  6.0271e-02,  5.6559e-01,  ...,  9.3640e-03,\n",
      "          2.3275e+00, -1.6497e+00],\n",
      "        [-5.4778e-01, -1.2485e-02, -1.4502e+00,  ...,  3.2230e-01,\n",
      "          9.5811e-01, -7.6889e-02]], requires_grad=True)\n",
      "rnn.weight_ih_l0 --->  Parameter containing:\n",
      "tensor([[ 0.5014,  0.3084, -0.5036,  ..., -0.9997, -0.0363,  0.3148],\n",
      "        [ 0.6507, -0.1685, -0.9695,  ..., -0.9929, -0.7079,  0.1494],\n",
      "        [ 1.0191, -0.7331, -0.3788,  ..., -0.0905, -0.5097,  0.1270],\n",
      "        ...,\n",
      "        [ 1.2083,  0.3682, -0.0598,  ...,  0.1467, -0.8266, -0.6024],\n",
      "        [-0.5060, -0.5898, -0.7315,  ...,  0.1363, -0.2493, -0.3364],\n",
      "        [-0.3542, -0.3424,  0.9130,  ...,  0.3774,  0.6780,  0.5438]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_hh_l0 --->  Parameter containing:\n",
      "tensor([[ 0.2155, -0.7240,  0.1950,  ...,  0.1613,  0.1698,  0.2773],\n",
      "        [ 0.8452, -0.5370,  0.5043,  ...,  0.3430,  0.7828,  0.3077],\n",
      "        [ 0.2944, -0.2247,  0.3380,  ...,  0.5996,  0.3982,  0.3950],\n",
      "        ...,\n",
      "        [ 0.7139,  0.6353,  0.5135,  ...,  0.3699,  0.1693,  0.1046],\n",
      "        [-0.0995,  0.7555, -0.0977,  ..., -0.3089, -0.5452, -0.5840],\n",
      "        [-0.5001, -0.8544, -0.8134,  ..., -0.7184, -0.3855, -0.5959]],\n",
      "       requires_grad=True)\n",
      "rnn.bias_ih_l0 --->  Parameter containing:\n",
      "tensor([-5.4417e-01, -7.4817e-01, -7.4843e-01,  6.2479e-01,  2.4100e-01,\n",
      "        -9.5592e-01, -2.1336e-01, -3.4690e-01,  2.1410e-01, -2.2632e-01,\n",
      "         7.4596e-01, -1.7151e-01,  2.2764e-01,  2.7746e-01, -5.5948e-01,\n",
      "        -7.7282e-01, -3.5400e-01,  5.7475e-01, -4.6679e-02, -9.2650e-01,\n",
      "         1.6903e-01, -9.8939e-01, -2.0911e-01, -7.5971e-01, -8.7384e-01,\n",
      "         4.2719e-02,  6.5175e-01, -8.3051e-01,  7.7445e-01,  1.3161e-01,\n",
      "        -6.4208e-01, -2.5431e-01,  2.7969e-01, -4.9040e-03, -2.1654e-01,\n",
      "        -4.5603e-01,  6.9829e-01,  2.1940e-01, -3.6026e-01,  6.6242e-01,\n",
      "        -5.9078e-01,  2.3833e-01, -1.8635e-01,  3.4410e-01, -3.2960e-01,\n",
      "        -8.5798e-02,  7.9910e-01,  2.3408e-01, -2.9739e-02, -5.1887e-01,\n",
      "         6.7103e-01, -7.7608e-01,  5.8340e-01,  7.6764e-01, -3.5877e-01,\n",
      "         4.9427e-01, -3.6836e-01, -3.8255e-01,  1.1112e-01,  3.0142e-01,\n",
      "        -4.8494e-02, -2.4956e-01, -8.4440e-01,  6.4098e-01, -6.5613e-01,\n",
      "         5.6816e-01, -6.7905e-01, -8.4684e-01, -1.5633e-01,  3.4260e-02,\n",
      "        -3.3625e-01,  7.4713e-01,  1.1059e+00,  9.1613e-01,  7.8107e-01,\n",
      "         8.3804e-01,  6.7908e-01,  3.2824e-02,  2.0760e-01,  2.6156e-01,\n",
      "        -1.4989e-01,  3.0292e-01, -3.4792e-01, -4.9045e-03, -2.0400e-01,\n",
      "        -6.2326e-01, -3.8694e-01,  4.1103e-01,  6.7821e-01, -5.8941e-04,\n",
      "         3.7008e-01, -4.4604e-01,  4.0839e-01, -3.1963e-01,  5.8584e-01,\n",
      "         2.2021e-01,  1.9512e-01, -8.0045e-01,  6.3522e-01, -4.6852e-01,\n",
      "         6.4344e-01,  2.8074e-01,  1.1445e-01, -7.0602e-01, -1.5091e-01,\n",
      "         8.8798e-01,  1.1090e-01, -4.3249e-01,  6.5916e-01,  2.7483e-01,\n",
      "         4.4120e-01,  4.9821e-01, -3.6623e-01, -6.8363e-01,  2.8995e-01,\n",
      "        -3.5308e-01, -5.0202e-01, -8.7231e-01, -1.4474e-01,  3.7353e-01,\n",
      "         7.1819e-02,  6.4313e-01, -4.5327e-01,  8.0388e-01, -8.9189e-01,\n",
      "        -7.2634e-01,  2.7167e-01,  5.6256e-01], requires_grad=True)\n",
      "rnn.bias_hh_l0 --->  Parameter containing:\n",
      "tensor([-0.4799, -0.7777, -0.6187,  0.6901,  0.3178, -0.9621, -0.0922, -0.2882,\n",
      "         0.1506, -0.3442,  0.6715, -0.0645,  0.1929,  0.3115, -0.4427, -0.6519,\n",
      "        -0.3362,  0.6787, -0.1335, -0.8761,  0.0045, -1.0183, -0.2028, -0.7489,\n",
      "        -0.8647,  0.1300,  0.5340, -0.8167,  0.7307,  0.0576, -0.7377, -0.1687,\n",
      "         0.2869,  0.0693, -0.1596, -0.4082,  0.6998,  0.1309, -0.3999,  0.6259,\n",
      "        -0.6175,  0.1472, -0.2447,  0.3826, -0.2912, -0.0388,  0.6826,  0.2245,\n",
      "        -0.0884, -0.5317,  0.6283, -0.7061,  0.5756,  0.7028, -0.3284,  0.5852,\n",
      "        -0.3828, -0.2540,  0.0953,  0.2082,  0.0383, -0.2919, -0.8494,  0.5997,\n",
      "        -0.5344,  0.5554, -0.6592, -0.6853, -0.2125,  0.0691, -0.5127,  0.7808,\n",
      "         1.0620,  0.8590,  0.6878,  0.8163,  0.6696,  0.0997,  0.1880,  0.2712,\n",
      "        -0.0472,  0.2999, -0.3979, -0.0481, -0.0975, -0.7276, -0.3433,  0.4176,\n",
      "         0.7454,  0.1249,  0.4220, -0.5068,  0.5296, -0.2916,  0.6287,  0.2276,\n",
      "         0.1103, -0.7628,  0.7137, -0.5891,  0.6932,  0.3231,  0.0167, -0.6073,\n",
      "        -0.0302,  0.9076,  0.1534, -0.3887,  0.6620,  0.1882,  0.3398,  0.5409,\n",
      "        -0.2866, -0.6413,  0.2799, -0.2313, -0.5023, -0.8773, -0.0844,  0.3859,\n",
      "        -0.0601,  0.7106, -0.4486,  0.6567, -0.9110, -0.7810,  0.1823,  0.5239],\n",
      "       requires_grad=True)\n",
      "out.weight --->  Parameter containing:\n",
      "tensor([[-0.2836, -0.0644, -0.3294,  ..., -0.8305, -0.3488, -0.1621],\n",
      "        [-0.1365,  0.3618,  0.2132,  ..., -0.2082, -0.3666,  0.3649],\n",
      "        [ 0.0623, -0.4444, -0.6244,  ..., -0.0914, -0.1094,  0.1022],\n",
      "        ...,\n",
      "        [ 0.5719,  0.1548,  0.6336,  ...,  0.7770,  0.2905,  0.0226],\n",
      "        [ 0.5551,  0.3493,  0.5450,  ...,  0.4985,  0.3991, -0.2484],\n",
      "        [ 0.3107,  0.4673,  0.5292,  ...,  0.3205,  0.3477, -0.3920]],\n",
      "       requires_grad=True)\n",
      "out.bias --->  Parameter containing:\n",
      "tensor([ 0.8425,  0.1585, -0.3423,  ..., -0.8668, -0.8722, -0.7306],\n",
      "       requires_grad=True)\n",
      "epoch   8 loss: 2.07791 time 2.95\n",
      "ebd.weight --->  Parameter containing:\n",
      "tensor([[-1.8707, -1.9356, -0.8688,  ...,  0.9048,  1.4857, -2.7879],\n",
      "        [-0.6033,  1.9463, -1.4888,  ...,  2.7012, -1.5627,  1.9258],\n",
      "        [-1.7503,  0.0390,  1.2842,  ...,  0.3844,  0.7062,  1.4348],\n",
      "        ...,\n",
      "        [ 1.0259, -0.3053, -0.4325,  ...,  1.2294, -0.7571,  2.8377],\n",
      "        [-1.7234,  0.0603,  0.5656,  ...,  0.0094,  2.3275, -1.6497],\n",
      "        [-0.5478, -0.0125, -1.4502,  ...,  0.3223,  0.9581, -0.0769]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_ih_l0 --->  Parameter containing:\n",
      "tensor([[ 0.5389,  0.3724, -0.5383,  ..., -1.1373, -0.0417,  0.3252],\n",
      "        [ 0.6497, -0.2355, -1.1818,  ..., -1.0160, -0.7455,  0.1830],\n",
      "        [ 1.0639, -0.7961, -0.3719,  ..., -0.0614, -0.5848,  0.2196],\n",
      "        ...,\n",
      "        [ 1.3067,  0.4566, -0.0828,  ...,  0.0486, -0.9388, -0.6466],\n",
      "        [-0.5257, -0.5876, -0.7439,  ...,  0.0775, -0.2797, -0.3335],\n",
      "        [-0.4405, -0.2732,  0.9471,  ...,  0.3838,  0.7421,  0.6318]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_hh_l0 --->  Parameter containing:\n",
      "tensor([[ 0.2303, -0.8136,  0.1771,  ...,  0.1551,  0.1793,  0.3079],\n",
      "        [ 0.9067, -0.5729,  0.5296,  ...,  0.3349,  0.8045,  0.3160],\n",
      "        [ 0.2980, -0.1983,  0.3581,  ...,  0.6257,  0.4099,  0.4190],\n",
      "        ...,\n",
      "        [ 0.7641,  0.6273,  0.5040,  ...,  0.2740,  0.1260,  0.1196],\n",
      "        [-0.1150,  0.7938, -0.0839,  ..., -0.3322, -0.5613, -0.5839],\n",
      "        [-0.5241, -0.8564, -0.8389,  ..., -0.7140, -0.4110, -0.6540]],\n",
      "       requires_grad=True)\n",
      "rnn.bias_ih_l0 --->  Parameter containing:\n",
      "tensor([-0.5398, -0.7788, -0.7725,  0.6500,  0.2783, -1.0019, -0.2190, -0.3741,\n",
      "         0.2176, -0.2242,  0.7749, -0.1686,  0.2257,  0.2970, -0.6245, -0.8102,\n",
      "        -0.3604,  0.6048, -0.0535, -0.9676,  0.1631, -1.0385, -0.2243, -0.7931,\n",
      "        -0.8968,  0.0327,  0.6797, -0.8735,  0.8177,  0.1676, -0.6822, -0.2731,\n",
      "         0.2949, -0.0089, -0.2431, -0.4881,  0.7420,  0.2349, -0.3944,  0.6733,\n",
      "        -0.6123,  0.2445, -0.1871,  0.3802, -0.3531, -0.0921,  0.8379,  0.2635,\n",
      "        -0.0055, -0.4848,  0.6907, -0.8141,  0.6127,  0.7833, -0.3844,  0.5062,\n",
      "        -0.3847, -0.4093,  0.1198,  0.3215, -0.0566, -0.2519, -0.8837,  0.6650,\n",
      "        -0.6861,  0.5876, -0.7096, -0.8725, -0.1525,  0.0360, -0.3746,  0.8022,\n",
      "         1.1957,  0.9848,  0.8055,  0.8620,  0.7081,  0.0287,  0.2044,  0.2691,\n",
      "        -0.1517,  0.3177, -0.4110, -0.0196, -0.2002, -0.6414, -0.4489,  0.4266,\n",
      "         0.6981, -0.0077,  0.3966, -0.4598,  0.4172, -0.3540,  0.5952,  0.2190,\n",
      "         0.1995, -0.8254,  0.6573, -0.4454,  0.6762,  0.2890,  0.0976, -0.7328,\n",
      "        -0.1500,  0.9285,  0.1022, -0.4616,  0.6805,  0.2490,  0.4745,  0.5311,\n",
      "        -0.4106, -0.7137,  0.2864, -0.3594, -0.5546, -0.9156, -0.1577,  0.3725,\n",
      "         0.0824,  0.6690, -0.4640,  0.8336, -0.9365, -0.7572,  0.2777,  0.5805],\n",
      "       requires_grad=True)\n",
      "rnn.bias_hh_l0 --->  Parameter containing:\n",
      "tensor([-4.7555e-01, -8.0830e-01, -6.4281e-01,  7.1533e-01,  3.5504e-01,\n",
      "        -1.0081e+00, -9.7837e-02, -3.1538e-01,  1.5415e-01, -3.4210e-01,\n",
      "         7.0044e-01, -6.1543e-02,  1.9096e-01,  3.3103e-01, -5.0770e-01,\n",
      "        -6.8929e-01, -3.4258e-01,  7.0873e-01, -1.4034e-01, -9.1723e-01,\n",
      "        -1.4555e-03, -1.0675e+00, -2.1797e-01, -7.8233e-01, -8.8759e-01,\n",
      "         1.1989e-01,  5.6197e-01, -8.5973e-01,  7.7390e-01,  9.3546e-02,\n",
      "        -7.7786e-01, -1.8751e-01,  3.0205e-01,  6.5367e-02, -1.8613e-01,\n",
      "        -4.4027e-01,  7.4350e-01,  1.4641e-01, -4.3400e-01,  6.3677e-01,\n",
      "        -6.3903e-01,  1.5337e-01, -2.4546e-01,  4.1865e-01, -3.1478e-01,\n",
      "        -4.5081e-02,  7.2147e-01,  2.5389e-01, -6.4166e-02, -4.9761e-01,\n",
      "         6.4793e-01, -7.4414e-01,  6.0486e-01,  7.1837e-01, -3.5396e-01,\n",
      "         5.9709e-01, -3.9910e-01, -2.8070e-01,  1.0402e-01,  2.2827e-01,\n",
      "         3.0189e-02, -2.9423e-01, -8.8871e-01,  6.2374e-01, -5.6434e-01,\n",
      "         5.7480e-01, -6.8971e-01, -7.1096e-01, -2.0866e-01,  7.0877e-02,\n",
      "        -5.5103e-01,  8.3587e-01,  1.1518e+00,  9.2767e-01,  7.1226e-01,\n",
      "         8.4031e-01,  6.9861e-01,  9.5579e-02,  1.8476e-01,  2.7877e-01,\n",
      "        -4.9009e-02,  3.1467e-01, -4.6098e-01, -6.2770e-02, -9.3702e-02,\n",
      "        -7.4575e-01, -4.0528e-01,  4.3322e-01,  7.6535e-01,  1.1781e-01,\n",
      "         4.4850e-01, -5.2058e-01,  5.3832e-01, -3.2594e-01,  6.3802e-01,\n",
      "         2.2636e-01,  1.1470e-01, -7.8770e-01,  7.3585e-01, -5.6597e-01,\n",
      "         7.2600e-01,  3.3132e-01, -1.9751e-04, -6.3410e-01, -2.9220e-02,\n",
      "         9.4805e-01,  1.4462e-01, -4.1773e-01,  6.8332e-01,  1.6243e-01,\n",
      "         3.7311e-01,  5.7377e-01, -3.3102e-01, -6.7134e-01,  2.7632e-01,\n",
      "        -2.3763e-01, -5.5484e-01, -9.2068e-01, -9.7411e-02,  3.8486e-01,\n",
      "        -4.9433e-02,  7.3653e-01, -4.5934e-01,  6.8637e-01, -9.5565e-01,\n",
      "        -8.1184e-01,  1.8836e-01,  5.4184e-01], requires_grad=True)\n",
      "out.weight --->  Parameter containing:\n",
      "tensor([[-0.2778, -0.0087, -0.2889,  ..., -0.7858, -0.3247, -0.1657],\n",
      "        [-0.1707,  0.3036,  0.1931,  ..., -0.1610, -0.4019,  0.3894],\n",
      "        [ 0.0096, -0.5188, -0.7476,  ..., -0.1116, -0.0793,  0.0825],\n",
      "        ...,\n",
      "        [ 0.5634,  0.1009,  0.6244,  ...,  0.8034,  0.2842,  0.0322],\n",
      "        [ 0.5452,  0.2802,  0.5203,  ...,  0.5301,  0.3907, -0.2495],\n",
      "        [ 0.3345,  0.4786,  0.5437,  ...,  0.3236,  0.3653, -0.3925]],\n",
      "       requires_grad=True)\n",
      "out.bias --->  Parameter containing:\n",
      "tensor([ 0.8407,  0.1233, -0.3592,  ..., -0.9016, -0.9057, -0.7590],\n",
      "       requires_grad=True)\n",
      "epoch   9 loss: 1.86417 time 3.13\n",
      "ebd.weight --->  Parameter containing:\n",
      "tensor([[-1.9791, -1.8866, -0.7774,  ...,  1.0121,  1.4013, -2.8945],\n",
      "        [-0.6396,  2.0415, -1.5567,  ...,  2.7387, -1.5551,  1.9261],\n",
      "        [-1.6946,  0.0693,  1.2020,  ...,  0.4078,  0.6671,  1.4898],\n",
      "        ...,\n",
      "        [ 1.0259, -0.3053, -0.4325,  ...,  1.2294, -0.7571,  2.8377],\n",
      "        [-1.7234,  0.0603,  0.5656,  ...,  0.0094,  2.3275, -1.6497],\n",
      "        [-0.5478, -0.0125, -1.4502,  ...,  0.3223,  0.9581, -0.0769]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_ih_l0 --->  Parameter containing:\n",
      "tensor([[ 0.5396,  0.4334, -0.5359,  ..., -1.2638, -0.0512,  0.3220],\n",
      "        [ 0.6464, -0.2507, -1.2887,  ..., -1.0207, -0.7740,  0.1923],\n",
      "        [ 1.1103, -0.8728, -0.3707,  ..., -0.0291, -0.6209,  0.2928],\n",
      "        ...,\n",
      "        [ 1.2728,  0.5775, -0.0650,  ...,  0.0116, -1.0448, -0.7189],\n",
      "        [-0.5194, -0.5426, -0.7685,  ...,  0.0244, -0.3259, -0.3425],\n",
      "        [-0.5144, -0.2731,  0.8432,  ...,  0.3449,  0.7283,  0.7261]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_hh_l0 --->  Parameter containing:\n",
      "tensor([[ 0.2528, -0.8578,  0.1792,  ...,  0.1367,  0.2027,  0.3409],\n",
      "        [ 0.9695, -0.5848,  0.5710,  ...,  0.3397,  0.8210,  0.3071],\n",
      "        [ 0.2955, -0.1865,  0.3814,  ...,  0.6511,  0.4189,  0.4283],\n",
      "        ...,\n",
      "        [ 0.8104,  0.6509,  0.4939,  ...,  0.2164,  0.0848,  0.0831],\n",
      "        [-0.1215,  0.8477, -0.0671,  ..., -0.3546, -0.5706, -0.5870],\n",
      "        [-0.5007, -0.8568, -0.8846,  ..., -0.7458, -0.4496, -0.6642]],\n",
      "       requires_grad=True)\n",
      "rnn.bias_ih_l0 --->  Parameter containing:\n",
      "tensor([-0.5362, -0.8063, -0.7967,  0.6753,  0.2899, -1.0470, -0.2243, -0.3868,\n",
      "         0.2228, -0.2189,  0.8024, -0.1681,  0.2238,  0.3036, -0.6849, -0.8372,\n",
      "        -0.3692,  0.6289, -0.0532, -1.0176,  0.1517, -1.0815, -0.2414, -0.8200,\n",
      "        -0.9396,  0.0328,  0.6940, -0.9109,  0.8790,  0.1685, -0.6965, -0.3001,\n",
      "         0.3024, -0.0133, -0.2565, -0.5040,  0.7893,  0.2518, -0.4123,  0.6865,\n",
      "        -0.6320,  0.2502, -0.1825,  0.3708, -0.3693, -0.0856,  0.8602,  0.2893,\n",
      "         0.0224, -0.4578,  0.7059, -0.8465,  0.6439,  0.8037, -0.4006,  0.5173,\n",
      "        -0.4024, -0.4276,  0.1261,  0.3364, -0.0582, -0.2521, -0.9255,  0.6882,\n",
      "        -0.7126,  0.6042, -0.7315, -0.9110, -0.1489,  0.0393, -0.4292,  0.8415,\n",
      "         1.2609,  1.0460,  0.8286,  0.8795,  0.7310,  0.0241,  0.1965,  0.2727,\n",
      "        -0.1642,  0.3325, -0.4822, -0.0312, -0.2068, -0.6759, -0.5000,  0.4416,\n",
      "         0.7009, -0.0020,  0.4176, -0.4686,  0.4220, -0.4007,  0.6070,  0.2194,\n",
      "         0.2021, -0.8463,  0.6799, -0.4474,  0.7028,  0.2993,  0.0900, -0.7487,\n",
      "        -0.1600,  0.9652,  0.1070, -0.4806,  0.7060,  0.2220,  0.4994,  0.5616,\n",
      "        -0.4642, -0.7352,  0.2680, -0.3521, -0.6147, -0.9581, -0.1571,  0.3574,\n",
      "         0.1096,  0.6947, -0.4793,  0.8598, -0.9703, -0.7871,  0.2812,  0.6065],\n",
      "       requires_grad=True)\n",
      "rnn.bias_hh_l0 --->  Parameter containing:\n",
      "tensor([-0.4719, -0.8358, -0.6670,  0.7406,  0.3667, -1.0532, -0.1031, -0.3281,\n",
      "         0.1594, -0.3368,  0.7279, -0.0611,  0.1891,  0.3376, -0.5681, -0.7163,\n",
      "        -0.3513,  0.7329, -0.1400, -0.9672, -0.0128, -1.1105, -0.2351, -0.8092,\n",
      "        -0.9305,  0.1200,  0.5762, -0.8971,  0.8353,  0.0945, -0.7922, -0.2145,\n",
      "         0.3095,  0.0610, -0.1996, -0.4562,  0.7908,  0.1633, -0.4519,  0.6500,\n",
      "        -0.6587,  0.1590, -0.2409,  0.4092, -0.3309, -0.0386,  0.7437,  0.2797,\n",
      "        -0.0363, -0.4706,  0.6632, -0.7765,  0.6361,  0.7388, -0.3702,  0.6082,\n",
      "        -0.4168, -0.2990,  0.1103,  0.2432,  0.0286, -0.2944, -0.9305,  0.6469,\n",
      "        -0.5908,  0.5914, -0.7116, -0.7495, -0.2051,  0.0742, -0.6057,  0.8751,\n",
      "         1.2170,  0.9889,  0.7353,  0.8578,  0.7215,  0.0909,  0.1768,  0.2823,\n",
      "        -0.0615,  0.3294, -0.5322, -0.0744, -0.1002, -0.7803, -0.4563,  0.4482,\n",
      "         0.7681,  0.1235,  0.4695, -0.5293,  0.5432, -0.3726,  0.6498,  0.2268,\n",
      "         0.1173, -0.8087,  0.7584, -0.5680,  0.7525,  0.3417, -0.0077, -0.6500,\n",
      "        -0.0392,  0.9848,  0.1495, -0.4367,  0.7089,  0.1354,  0.3980,  0.6043,\n",
      "        -0.3846, -0.6929,  0.2579, -0.2304, -0.6149, -0.9631, -0.0967,  0.3697,\n",
      "        -0.0222,  0.7622, -0.4747,  0.7125, -0.9894, -0.8418,  0.1918,  0.5678],\n",
      "       requires_grad=True)\n",
      "out.weight --->  Parameter containing:\n",
      "tensor([[-2.9279e-01,  1.2152e-02, -3.0198e-01,  ..., -7.9129e-01,\n",
      "         -3.5997e-01, -1.4271e-01],\n",
      "        [-1.6718e-01,  2.8798e-01,  2.1183e-01,  ..., -9.6760e-02,\n",
      "         -3.9746e-01,  4.2240e-01],\n",
      "        [-3.7553e-02, -5.4582e-01, -7.9630e-01,  ..., -1.2157e-01,\n",
      "         -7.2130e-04,  2.9048e-02],\n",
      "        ...,\n",
      "        [ 5.5319e-01,  6.1505e-02,  6.1439e-01,  ...,  8.2350e-01,\n",
      "          2.7737e-01,  3.9899e-02],\n",
      "        [ 5.3697e-01,  2.2550e-01,  5.0060e-01,  ...,  5.5319e-01,\n",
      "          3.8372e-01, -2.4972e-01],\n",
      "        [ 3.5219e-01,  4.8682e-01,  5.5438e-01,  ...,  3.2481e-01,\n",
      "          3.7831e-01, -3.9205e-01]], requires_grad=True)\n",
      "out.bias --->  Parameter containing:\n",
      "tensor([ 0.8247,  0.1272, -0.4047,  ..., -0.9290, -0.9313, -0.7807],\n",
      "       requires_grad=True)\n",
      "epoch  10 loss: 1.78329 time 3.50\n"
     ]
    }
   ],
   "source": [
    "# 构建网络模型\n",
    "class TextGenerator(nn.Module):\n",
    "    # word_len: 词表大小\n",
    "    def __init__(self, word_len):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        # 初始化词嵌入层\n",
    "        # num_embeddings:词表大小\n",
    "        # embedding_dim: 词向量维度\n",
    "        embedding_dim = 128\n",
    "        self.ebd = nn.Embedding(num_embeddings=word_len, embedding_dim=embedding_dim)\n",
    "        # print(self.ebd(torch.tensor(4)))\n",
    "        \n",
    "        # 循环网络 \n",
    "        # input_size: 词向量维度\n",
    "        # hidden_size: 隐藏层大小,输出维度(随便写),神经元个数,中间状态个数\n",
    "        # num_layers: 网络层个数\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=128, num_layers=1) \n",
    "        \n",
    "        # 输出层, 输出值个数是word_len的长度，应为要从整个此表中预测一个词的概率\n",
    "        self.out = nn.Linear(128, word_len)\n",
    "      \n",
    "    \n",
    "    def get_embd(self):\n",
    "        return self.ebd\n",
    "\n",
    "    # 前向传播\n",
    "    # inputs: 输入的样本  (btach_size, seq_len)\n",
    "    # hn: 中间隐藏层状态  (num_layers, batch_size, hidden_size)\n",
    "    def forward(self, inputs, hn): \n",
    "        \n",
    "        # 词嵌入\n",
    "        embed = self.ebd(inputs)\n",
    "        # print('inputs-> ',inputs.shape)\n",
    "        # print('embed-> ', embed.shape)\n",
    "\n",
    "        # 正则化\n",
    "        embed = self.dropout(embed)\n",
    "        \n",
    "        # 送入rnn\n",
    "        # (btach_size, seq_len, word_dim) -> (seq_len, btach_size, word_dim)\n",
    "        # output: 包含每个时刻中间状态\n",
    "        # hn: 最后一个时刻的中间状态\n",
    "        # print('-->111111 ',inputs.shape, embed.shape, hn.shape)\n",
    "        output, hn = self.rnn(embed.transpose(0,1), hn)\n",
    "        \n",
    "        # 放入全连接层  output -> out\n",
    "        out = self.out(output)\n",
    "\n",
    "        # hn = self.out(hn)\n",
    "        return out, hn\n",
    "       \n",
    "        \n",
    "    def init_hn(self,batch_size):\n",
    "        # print('batch_size-> ', batch_size)\n",
    "        # (num_layers, batch_size, hidden_size)\n",
    "        return torch.zeros(1,batch_size,128)\n",
    "\n",
    "    \n",
    "    \n",
    "def test03():\n",
    "    index_to_word, word_to_index, word_count, corpus_idx,_ = build_vocab()   \n",
    "    lyrics = LyricsDataset(corpus_idx, 5) # 5:句子长度\n",
    "   \n",
    "    batch_size = 1\n",
    "    lyrics_dataloader = DataLoader(lyrics, shuffle=False, batch_size=batch_size)\n",
    "    model = TextGenerator(word_count)\n",
    "    \n",
    "    for x, y in lyrics_dataloader:\n",
    "        h0 = model.init_hn(batch_size)\n",
    "        print('h0.shape-> ',h0.shape) #(num_layers, batch_size, hidden_size)\n",
    "        print('x.shape-> ', x.shape)  #(btach_size, seq_len)\n",
    "        print('y.shape-> ', y.shape)  #(btach_size, seq_len)\n",
    "       \n",
    "        output, hn = model(x, h0)\n",
    "        print('output.shape->',output.shape) #(seq_len, btach_size, 词表长度)\n",
    "        print('hn.shape->', hn.shape)\n",
    "\n",
    "        break\n",
    "\n",
    "# test03()\n",
    "\n",
    "\n",
    "\n",
    "### 训练\n",
    "def train():\n",
    "    # 构建词典\n",
    "    index_to_word, word_to_index, word_count, corpus_idx,_ = build_vocab()\n",
    "    # 数据集\n",
    "    num_chars = 32  # 句子长度\n",
    "    lyrics = LyricsDataset(corpus_idx, num_chars)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = TextGenerator(word_count)\n",
    "    emdb = model.get_embd()\n",
    "    \n",
    "    # 损失函数\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "     # 优化方法\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "    \n",
    "    # 训练轮数\n",
    "    epoch = 10\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch_idx in range(epoch):\n",
    "        # print('词向量 --->',epoch_idx ,'--- ',emdb(torch.tensor(4)))\n",
    "        for name,param in model.named_parameters():\n",
    "            print(name,'---> ',param)\n",
    "            \n",
    "\n",
    "        batch_size = 333\n",
    " \n",
    "         # 数据加载器\n",
    "        lyrics_dataloader = DataLoader(lyrics, shuffle=True, batch_size=batch_size)\n",
    "        # 训练时间\n",
    "        start = time.time()\n",
    "        # 迭代次数\n",
    "        iter_num = 0\n",
    "        # 训练损失\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for x, y in lyrics_dataloader:\n",
    "            # x形状 (btach_size, seq_len)\n",
    "            \n",
    "            # 初始隐藏状态  batch_size == x.shape[0]\n",
    "            hn = model.init_hn(x.shape[0])\n",
    "            \n",
    "            # 模型计算\n",
    "            # out形状 (seq_len, btach_size, 词表长度)\n",
    "            out, hn = model(x, hn)\n",
    "            # print('out.shape-> ',out.shape) #[seq_len, batch_size, 词表长度]\n",
    "            # print('y.shape-> ',y.shape,y) #[batch_size, seq_len]\n",
    "            # print('out before-> ', out)\n",
    "            \n",
    "            # out形状 [seq_len, batch_size, 词表长度(分类个数)] -> [batch_size * seq_len, 词表长度(分类个数)]\n",
    "            out = out.permute(1,0,2)\n",
    "            out = out.reshape(out.shape[0]*out.shape[1], out.shape[2])\n",
    "            # print(out.shape)\n",
    "            # print('out after-> ', out)\n",
    "\n",
    "            \n",
    "            # y形状 (batch_size, seq_len) -> (batch_size * seq_len)\n",
    "            # print('y before->',y)\n",
    "            y = y.reshape(y.shape[0] * y.shape[1])\n",
    "            # print('y after->',y)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = loss_fun(out, y)\n",
    "            \n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 参数更新\n",
    "            optimizer.step()\n",
    "            \n",
    "            iter_num += 1\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        message = 'epoch %3s loss: %.5f time %.2f' % \\\n",
    "                  (epoch_idx + 1,\n",
    "                   total_loss / iter_num,\n",
    "                   time.time() - start)\n",
    "        print(message)  \n",
    "        \n",
    "    # 模型存储\n",
    "    torch.save(model.state_dict(), 'model/text-generator_10.pth')\n",
    "\n",
    "# 训练\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "876a1bb8-149f-4421-8635-6091d75fdd52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6032,  1.0777,  1.0448,  0.3063,  1.0159, -1.1750,  0.1273, -0.6190,\n",
      "         0.0452, -0.4296, -1.4291,  0.5964,  1.1408,  1.9881, -0.6465, -0.1233,\n",
      "        -0.5359,  0.4323, -0.9345, -0.5459, -0.5024, -0.6420,  0.1008,  1.9956,\n",
      "        -1.7389, -1.7927,  0.5973,  1.9554,  0.1012,  0.3456,  0.4889, -0.0502,\n",
      "        -0.8623,  2.0572,  0.5201, -1.3454, -0.9763, -1.9414,  1.2468,  0.3165,\n",
      "         0.8494,  1.0635, -1.0093,  0.9467,  0.8961, -0.3868,  0.6270,  1.0438,\n",
      "         1.2811,  0.4118, -1.7610, -0.4277,  0.6290,  1.3352, -1.0936, -0.1874,\n",
      "        -1.0669, -1.2879, -0.7912, -0.0576,  1.5061, -0.9747, -0.2678, -0.1020,\n",
      "         0.8284, -0.8102,  0.2684, -1.2154, -0.2392,  0.1568, -1.9914, -1.6981,\n",
      "        -1.4385,  0.4117, -0.5493, -0.6133, -1.1851, -0.9440, -1.6551,  0.8597,\n",
      "        -1.1864, -2.1758, -0.7756, -0.4431, -0.6054, -1.7768, -0.9309,  0.0692,\n",
      "        -0.4577,  0.4310, -1.3327, -0.9335, -0.3493, -0.0701, -0.1699, -0.4641,\n",
      "        -0.4948, -0.8849,  0.5000, -1.0277,  0.2530,  0.9317,  0.5788,  2.6387,\n",
      "        -0.5986,  0.8174, -0.3882, -0.6506,  0.0719, -0.2444, -1.4631,  0.7323,\n",
      "        -0.4060, -0.3494,  1.3497,  3.4400,  0.3964, -0.6023,  0.7322,  1.9974,\n",
      "        -1.2726,  1.4009,  1.4204, -0.5516, -0.1455, -1.6746, -0.0279, -0.4123],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "你 ( 4 )是 ( 63 )谁 ( 321 )让 ( 19 )晶莹 ( 76 )的 ( 17 )泪滴 ( 77 )  ( 39 )谁 ( 321 )让 ( 19 )它 ( 178 )停留 ( 282 )的 ( 17 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )语 ( 241 )沉默 ( 242 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )语 ( 241 )沉默 ( 242 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )语 ( 241 )沉默 ( 242 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )语 ( 241 )沉默 ( 242 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )语 ( 241 )沉默 ( 242 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )"
     ]
    }
   ],
   "source": [
    "# 预测函数\n",
    "# text :输入的句子\n",
    "# sentence_length: 要预测的单词个数\n",
    "def predict(text, sentence_length):\n",
    "    # 构建词典\n",
    "    index_to_word, word_to_index, word_count, _ ,_= build_vocab()\n",
    "    # 构建模型\n",
    "    model = TextGenerator(word_count)\n",
    "    # 加载参数\n",
    "    model.load_state_dict(torch.load('model/text-generator_10.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    emdb = model.get_embd()\n",
    "    print(emdb(torch.tensor(4)))\n",
    "    \n",
    "    #分词\n",
    "    words = jieba.lcut(text)\n",
    "    word_idx_s = [] #分好的词放到这里\n",
    "    generate_sentence = [] #预测的值放到这里\n",
    "    for word in words:\n",
    "        word_idx = word_to_index[word]\n",
    "        word_idx_s.append(word_idx)\n",
    "        # 存放预测的结果\n",
    "        generate_sentence.append(word_idx)\n",
    "  \n",
    "    # print('---> ',word_idx,index_to_word[word_idx])\n",
    "    \n",
    "    # 初始隐藏状态\n",
    "    hn = model.init_hn(1)\n",
    "    # print(h0)\n",
    "        \n",
    "    # 开始预测\n",
    "    for _ in range(sentence_length):\n",
    "        # print(word_idx_s)\n",
    "        out, hn = model(torch.tensor([word_idx_s]), hn)\n",
    "        # print(out.shape)\n",
    "        \n",
    "        # 取最后一个预测的词\n",
    "        out = out[out.shape[0] - 1]\n",
    "        # print('out.shape-> ',out.shape)\n",
    "\n",
    "        # 选择分数最大的词作为预测词\n",
    "        word_idx = torch.argmax(out,dim=1).item()\n",
    "        # print('--> ',word_idx)\n",
    "  \n",
    "        # 预测词放入到预测列表\n",
    "        generate_sentence.append(word_idx)\n",
    "        \n",
    "        word_idx_s = [word_idx]\n",
    "    \n",
    "    # 打印预测的词\n",
    "    for idx in generate_sentence:\n",
    "        print(index_to_word[idx],'(',idx,')', end='')\n",
    "        pass\n",
    "        \n",
    "    \n",
    "\n",
    "predict('你是谁',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfba437-a46a-4a57-bab6-d93340056532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
