{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d527632-e2d7-4bc6-87a0-799a92a4ece6",
   "metadata": {},
   "source": [
    "# 神经网络 pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0256ba93-5799-47ad-b788-aa0a1b18b712",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32acd5-c5e6-4ad5-a99e-f4295123d420",
   "metadata": {},
   "source": [
    "# 一个三层的神经网络系统\n",
    "## 1.输入层  \n",
    " * 样本数据是一个两个维度的向量,也就是说每个样本数据都两个特征\n",
    " * X = $[x_1^{(0)},x_2^{(0)}]$    其中$(0)$表示第0层(也就是输入层)\n",
    " * Y 表示X这个样本的标签值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdac4c8-984d-4a0e-90f6-99de3e967206",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.隐藏层，总共一个隐藏层，隐藏层准备了三个神经元\n",
    " * 第一个神经元   \n",
    "    $$ \n",
    "        z_1^{(1)} =\n",
    "            w_{11}^{(1)} * x_1^{(0)} + \n",
    "            w_{12}^{(1)} * x_2^{(0)} +\n",
    "                b_1^{(1)}     \n",
    "    $$\n",
    "    \n",
    "    $$       \n",
    "        a_1^{(1)}= relu(z_1^{(1)})\n",
    "    $$\n",
    "    \n",
    "    * $w_{11}^{(1)}$  第(1)层，第1个神经元的第1个权重\n",
    "    * $x_1^{(0)}$  第(0)层第1个特征值\n",
    "    * $w_{12}^{(1)}$ 第(1)层，第1个神经元的第2个权重\n",
    "    * $x_2^{(0)}$ 第(0)层第2个特征值\n",
    "    * $b_1^{(1)}$ 第(1)层，第1个神经元的偏执\n",
    "    * $z_1^{(1)}$ 第(1)层，第1个神经元计算的原值(没有经过激活函数)\n",
    "    * $a_1^{(1)}$ 第(1)层，第1个神经元的激活值(relu是激活函数)\n",
    "    * $relu(z_1^{(1)})$ 用relu激活函数把$a_1^{(1)}$激活"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c2235-e6ac-40c1-bd4f-77b406bc5216",
   "metadata": {},
   "source": [
    "* 第二个神经元  \n",
    "    $$ \n",
    "        w_{21}^{(1)} * x_1^{(0)} + \n",
    "        w_{22}^{(1)} * x_2^{(0)} + \n",
    "            b_2^{(1)} = z_2^{(1)} \n",
    "    $$ \n",
    "    \n",
    "    $$       \n",
    "        a_2^{(1)}= relu(z_2^{(1)})\n",
    "    $$\n",
    " * 第三个神经元  \n",
    "    $$ \n",
    "        w_{31}^{(1)} * x_1^{(0)} + \n",
    "        w_{32}^{(1)} * x_2^{(0)} + \n",
    "            b_3^{(1)} = z_3^{(1)} \n",
    "    $$ \n",
    "    $$       \n",
    "        a_3^{(1)}= relu(z_3^{(1)})\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f7817-ccdc-4f2c-a8e9-30ae5f38632a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.输出层，用一个神经元表示\n",
    " * 输出层神经元表示\n",
    "   $$ \n",
    "       w_{11}^{(2)} * a_1^{(1)} + \n",
    "       w_{12}^{(2)} * a_2^{(1)} +\n",
    "       w_{13}^{(2)} * a_3^{(1)} +\n",
    "        b_1^{(2)} \n",
    "        = z_1^{(2)} \n",
    "    $$\n",
    "    \n",
    "    <font color='red'> \n",
    "    $$\n",
    "       a_1^{(2)}= relu(z_1^{(2)}) = y \n",
    "    $$\n",
    "    </font>\n",
    "    \n",
    "   * $w_{11}^{(2)}$ 第(2)层，第1个神经元的第1个权重； 输出层神经元接收的第1个输入值$a_1^{(1)}$的权重\n",
    "   * $w_{12}^{(2)}$ 第(2)层，第1个神经元的第2个权重； 输出层神经元接收的第2个输入值$a_2^{(1)}$的权重\n",
    "   * $w_{13}^{(2)}$ 第(2)层，第1个神经元的第3个权重； 输出层神经元接收的第3个输入值$a_3^{(1)}$的权重\n",
    "\n",
    "   * $a_1^{(1)}$ 第(1)层(上一层)，第1个神经元的输入值；输出层神经元接收的第1个输入值\n",
    "   * $a_2^{(1)}$ 第(1)层(上一层)，第2个神经元的输入值；输出层神经元接收的第2个输入值\n",
    "   * $a_3^{(1)}$ 第(1)层(上一层)，第3个神经元的输入值；输出层神经元接收的第3个输入值\n",
    "   \n",
    "   * $b_1^{(2)}$ 第(2)层第1个神经元的偏执；输出层神经元的偏执\n",
    "   * $z_1^{(2)}$ 第(2)层第1个神经元的原函数值；输出层神经元的函数原值\n",
    "   * $a_1^{(2)}$ 输出层神经元被激活的值；这里就是整个神经网络的输出值了(<font color='red'>预测值</font>),用y表示\n",
    "   * $relu(z_1^{(2)}) = a_1^{(2)} = y $ 用relu激活函数把$z_1^{(2)}$激活"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc4ebf5-6229-4a6f-bc30-5f2ed85c9c8f",
   "metadata": {},
   "source": [
    "## 4.损失函数\n",
    "$l_1 = \\cfrac 12 (y-Y)^2$\n",
    "* y 是预测值 <font color='red'> \n",
    "                $ y = a_1^{(2)}= relu(z_1^{(2)}) $\n",
    "            </font>\n",
    "    \n",
    "* Y 是标签值\n",
    "* 多个样本可用$(y_1,Y_1)  , (y_2,Y_2)$等表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c1503-0dbb-46ca-8557-5424b8050536",
   "metadata": {},
   "source": [
    "## 5.用梯度下降更新各个参数\n",
    "* 隐藏层3个神经元需要更新的参数  \n",
    "  * 第1个神经   $ w_{11}^{(1)}、w_{12}^{(1)}、b_1^{(1)} $\n",
    "  * 第2个神经   $ w_{21}^{(1)}、w_{22}^{(1)}、b_2^{(1)} $\n",
    "  * 第3个神经   $ w_{31}^{(1)}、w_{32}^{(1)}、b_3^{(1)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a0148-e761-4bbe-9bc5-03cd063adf62",
   "metadata": {},
   "source": [
    "* 输出层1个神经元需要更新的参数  \n",
    "  * $ w_{11}^{(2)}、w_{12}^{(2)}、b_1^{(2)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e55764-4d34-4f83-b313-03bcd4d7ea20",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 求待更新参数的偏导(梯度的分量) ,比如隐藏层中第一个神经元的第一个权重$w_{11}^{(1)}$\n",
    "   <font color='red' size=4>\n",
    "    $$\n",
    "             \\frac{\\partial l_1}{\\partial w_{11}^{(1)}}= \n",
    "                   \\frac{\\mathrm{d}l_1} {\\mathrm{d}y}\n",
    "                   \\frac{\\partial y}{\\partial z_1^{(2)}}\n",
    "                   \\frac{\\partial z_1^{(2)}}{\\partial a_1^{(1)}}\n",
    "                   \\frac{\\partial a_1^{(1)}}{\\partial z_1^{(1)}}\n",
    "                   \\frac{\\partial z_1^{(1)}}{\\partial w_{11}^{(1)}}\n",
    "    $$\n",
    "    </font>\n",
    "   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ba163-0c32-430d-9e96-f7d9956135bb",
   "metadata": {},
   "source": [
    "* 其中各个函数表达式如下\n",
    "    * $l_1 = \\dfrac 12 (y-Y)^2$\n",
    "    \n",
    "    * $ y = relu(z_1^{(2)}) $\n",
    "    \n",
    "    * $ z_1^{(2)} =  \n",
    "        w_{11}^{(2)} * a_1^{(1)} + \n",
    "        w_{12}^{(2)} * a_2^{(1)} +\n",
    "        w_{13}^{(2)} * a_3^{(1)} +\n",
    "        b_1^{(2)} \n",
    "      $\n",
    "      \n",
    "    * $ a_1^{(1)}= relu(z_1^{(1)}) $  \n",
    "    \n",
    "    * $ z_1^{(1)} =\n",
    "            w_{11}^{(1)} * x_1^{(0)} + \n",
    "            w_{12}^{(1)} * x_2^{(0)} +\n",
    "            b_1^{(1)}     \n",
    "      $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe943b-9b4d-4d30-a461-097c5160989c",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 链式求导中各个每项求导结果如下  \n",
    "    * <font color='red' size=2> \n",
    "        $\\cfrac{\\mathrm{d}l_1} {\\mathrm{d}y}= y-Y$\n",
    "      </font>\n",
    "      \n",
    "    * <font color='red' size=2> \n",
    "        $\\cfrac{\\partial y}{\\partial z_1^{(2)}}= 1 $\n",
    "      </font>\n",
    "    \n",
    "    * <font color='red' size=2> \n",
    "        $\\cfrac{\\partial z_1^{(2)}}{\\partial a_1^{(1)}}= w_{11}^{(2)}$\n",
    "      </font>\n",
    "    \n",
    "    * <font color='red' size=2> \n",
    "        $\\cfrac{\\partial a_1^{(1)}}{\\partial z_1^{(1)}}= 1$\n",
    "      </font>\n",
    "    \n",
    "    * <font color='red' size=2> \n",
    "        $\\cfrac{\\partial z_1^{(1)}}{\\partial w_{11}^{(1)}}= x_1^{(0)}$\n",
    "      </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d194b0-6413-4b15-8338-9efbe5826f6a",
   "metadata": {},
   "source": [
    "* 最终求偏导值如下\n",
    " <font color='red' size=4>\n",
    "    $$\n",
    "             \\frac{\\partial l_1}{\\partial w_{11}^{(1)}}= \n",
    "                   (y-Y) * 1 * w_{11}^{(2)} * 1 * x_1^{(0)}\n",
    "    $$\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106df26-0c88-4fd8-a654-424c3d262465",
   "metadata": {},
   "source": [
    "* 训练过程中，$y、Y、x_1^{(0)}$都是已知量，可使用如下公式更新参数  \n",
    "\n",
    "    <font color='red' size=3>\n",
    "         $w_{11}^{(1)新} = w_{11}^{(1)旧} - \\eta \\frac{\\partial l_1}{\\partial w_{11}^{(1)}}  $\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fac263-4e60-45af-8c7f-dc3ea734d898",
   "metadata": {},
   "source": [
    "# 开始编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e873b92-5a36-4507-8741-ea09157d50b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.输入层数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1687c2a4-2e9c-492f-be10-c6863c84bc47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1.1准备一个有两个特征值的样本\n",
    "x1 = torch.tensor(2.) #样本的第一个特征值\n",
    "x2 = torch.tensor(3.) #样本的第二个特征值\n",
    "Y = torch.tensor(5.) #样本的标签值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06876181-495b-43af-a6f1-2951e15f517d",
   "metadata": {},
   "source": [
    "### 2.初始化隐藏层数据(初始化各个神经元权重和偏执项参数，一般随机就可以)\n",
    " 这里为了方便演示，把所有偏执设置为0，后面不对偏执进行更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a6ac3eaa-ee36-4ad1-9d1c-c5962d26def2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.1第1个神经元的权重值 requires_grad=True,表示这个参数需要求梯度\n",
    "w11 = torch.tensor(1.,requires_grad=True)  #第1个权重参数\n",
    "w12 = torch.tensor(0.,requires_grad=True)  #第2个权重参数\n",
    "\n",
    "# 2.2第2个神经元的权重值\n",
    "w21 = torch.tensor(1.,requires_grad=True)  #第1个权重参数\n",
    "w22 = torch.tensor(0.,requires_grad=True)  #第2个权重参数\n",
    "\n",
    "# 2.3第3个神经元的权重值\n",
    "w31 = torch.tensor(0.,requires_grad=True) #第1个权重参数\n",
    "w32 = torch.tensor(0.,requires_grad=True) #第2个权重参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034a51b-0ea0-4674-922d-39d017a97f28",
   "metadata": {},
   "source": [
    "### 3.初始化输出层神经元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3f63a86d-a51d-42d6-87be-dce92bed6f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3.1 输出层只有一个神经元，有三个输入来接收隐藏层的三个输出数据\n",
    "w1 = torch.tensor(1.,requires_grad=True)  #神经元第1个权重参数\n",
    "w2 = torch.tensor(1.,requires_grad=True)  #神经元第2个权重参数\n",
    "w3 = torch.tensor(0.,requires_grad=True)  #神经元第3个权重参数 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe179767-0d92-4067-8863-6a087eaf9bb9",
   "metadata": {},
   "source": [
    "### 4.前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b6d8a970-76f2-44dd-96b3-620ee8e24d35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 tensor(2.7916, grad_fn=<AddBackward0>)\n",
      "a2 tensor(2.0260, grad_fn=<AddBackward0>)\n",
      "a3 0\n",
      "y tensor(4.8368, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.0266, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 激活函数\n",
    "def relu(x):\n",
    "    if x>0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 4.1 隐藏层第一个神经元\n",
    "z1 = w11 * x1 + w12 * x2  #原值\n",
    "a1 = relu(z1) #激活值\n",
    "print('a1',a1)\n",
    "\n",
    "\n",
    "# 4.2 隐藏层第二个神经元\n",
    "z2 = w21 * x1 + w22 * x2  #原值\n",
    "a2 = relu(z2) #激活值\n",
    "print('a2',a2)\n",
    "\n",
    "# 4.3 隐藏层第三个神经元\n",
    "z3 = w31 * x1 + w32 * x2  #原值\n",
    "a3 = relu(z3) #激活值\n",
    "print('a3',a3)\n",
    "\n",
    "# 4.4 输出层神经元\n",
    "y = w1 * a1 + w2 * a2 + w3 * a3\n",
    "y = relu(y) #激活\n",
    "print('y',y)\n",
    "\n",
    "# 4.5 定义损失函数\n",
    "loss = y - Y \n",
    "loss = pow(loss, 2)  # loss的2次幂除以2\n",
    "# 打印损失值，可以使用这个值来结束训练\n",
    "print('loss',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a0b35-e4bf-4770-91ea-0bff2d550ed1",
   "metadata": {},
   "source": [
    "### 5.反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d11f0d2c-932e-4bec-9834-b541923e3869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ????? 这个也不知道是干啥的\n",
    "w11.retain_grad()\n",
    "\n",
    "# 5.1 求梯度，求梯度中各个分量值，该函数值计算梯度只分量，不负责更新\n",
    "loss.backward() #???? 不是幂等的，每次求都更新了？？？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ff902698-c056-4b4b-9063-ebc22b6d0081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w11.grad: tensor(-0.6553)\n",
      "w12.grad: None\n",
      "w21.grad: None\n",
      "w22.grad: None\n",
      "w31.grad: None\n",
      "w32.grad: None\n",
      "w1.grad: None\n",
      "w2.grad: None\n",
      "w3.grad: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h6/d6w_x4k97m5d1zdjxsr25p4h0000gn/T/ipykernel_56004/4218388784.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print('w12.grad:',w12.grad)\n",
      "/var/folders/h6/d6w_x4k97m5d1zdjxsr25p4h0000gn/T/ipykernel_56004/4218388784.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print('w21.grad:',w21.grad)\n",
      "/var/folders/h6/d6w_x4k97m5d1zdjxsr25p4h0000gn/T/ipykernel_56004/4218388784.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print('w22.grad:',w22.grad)\n",
      "/var/folders/h6/d6w_x4k97m5d1zdjxsr25p4h0000gn/T/ipykernel_56004/4218388784.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print('w1.grad:',w1.grad)\n",
      "/var/folders/h6/d6w_x4k97m5d1zdjxsr25p4h0000gn/T/ipykernel_56004/4218388784.py:9: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print('w2.grad:',w2.grad)\n",
      "/var/folders/h6/d6w_x4k97m5d1zdjxsr25p4h0000gn/T/ipykernel_56004/4218388784.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print('w3.grad:',w3.grad)\n"
     ]
    }
   ],
   "source": [
    "# 参数的梯度值(一个分量)\n",
    "print('w11.grad:',w11.grad)\n",
    "print('w12.grad:',w12.grad)\n",
    "print('w21.grad:',w21.grad)\n",
    "print('w22.grad:',w22.grad)\n",
    "print('w31.grad:',w31.grad) #为啥神经元是0时，梯度返回None???????\n",
    "print('w32.grad:',w32.grad) #为啥神经元是0时，梯度返回None???????\n",
    "print('w1.grad:',w1.grad)\n",
    "print('w2.grad:',w2.grad)\n",
    "print('w3.grad:',w3.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4db3fca9-7ec3-40e6-9b86-0e70f645cfab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h6/d6w_x4k97m5d1zdjxsr25p4h0000gn/T/ipykernel_56004/3208886059.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  w12 = w12 - a * w12.grad\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 5.3 隐藏层神经元参数更新\u001b[39;00m\n\u001b[1;32m      4\u001b[0m w11 \u001b[38;5;241m=\u001b[39m w11 \u001b[38;5;241m-\u001b[39m a \u001b[38;5;241m*\u001b[39m w11\u001b[38;5;241m.\u001b[39mgrad\n\u001b[0;32m----> 5\u001b[0m w12 \u001b[38;5;241m=\u001b[39m w12 \u001b[38;5;241m-\u001b[39m a \u001b[38;5;241m*\u001b[39m w12\u001b[38;5;241m.\u001b[39mgrad\n\u001b[1;32m      6\u001b[0m w21 \u001b[38;5;241m=\u001b[39m w21 \u001b[38;5;241m-\u001b[39m a \u001b[38;5;241m*\u001b[39m w21\u001b[38;5;241m.\u001b[39mgrad\n\u001b[1;32m      7\u001b[0m w22 \u001b[38;5;241m=\u001b[39m w22 \u001b[38;5;241m-\u001b[39m a \u001b[38;5;241m*\u001b[39m w22\u001b[38;5;241m.\u001b[39mgrad\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# 5.2 参数更新\n",
    "a = 0.5 # 学习率\n",
    "# 5.3 隐藏层神经元参数更新\n",
    "w11 = w11 - a * w11.grad\n",
    "w12 = w12 - a * w12.grad\n",
    "w21 = w21 - a * w21.grad\n",
    "w22 = w22 - a * w22.grad\n",
    "w31 = w31 - a * w31.grad\n",
    "w32 = w32 - a * w32.grad\n",
    "\n",
    "# 5.4 输出神经元参数更新\n",
    "w1 = w1 - a * w1.grad\n",
    "w2 = w2 - a * w2.grad\n",
    "w3 = w3 - a * w3.grad\n",
    "\n",
    "print('w11:',w11)\n",
    "print('w12:',w12)\n",
    "print('w21:',w21)\n",
    "print('w22:',w22)\n",
    "print('w31:',w31) \n",
    "print('w32:',w32) \n",
    "print('w1:',w1)\n",
    "print('w2:',w2)\n",
    "print('w3:',w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "509b2fac-e7a4-42db-aeba-6143f8ede3b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0266, grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看损失值，当小于某个值时可以结束训练\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f574b6-f597-42da-bbf8-92de9c062f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
