{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9c6538-d96f-46cd-ba1f-0b033bebd696",
   "metadata": {},
   "source": [
    "# 文本生成案例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5c78c-7e3f-4f58-a84c-4cab3f555de9",
   "metadata": {},
   "source": [
    "### 词嵌入层 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd143701-92ec-45ec-ad84-6c348ed086af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北京', '东奥', '的', '进度条', '已经', '过半', '，', '不少', '外国', '运动员', '在', '完成', '自己', '的', '比赛', '后', '踏上', '归途', '。']\n",
      "Embedding(18, 4)\n",
      "北京 -> tensor([0.3457, 0.3825, 0.5380, 1.7388], grad_fn=<EmbeddingBackward0>)\n",
      "东奥 -> tensor([0.3363, 0.7970, 1.7221, 0.0421], grad_fn=<EmbeddingBackward0>)\n",
      "的 -> tensor([-0.2941, -0.8324,  1.8048, -0.0365], grad_fn=<EmbeddingBackward0>)\n",
      "进度条 -> tensor([ 0.7872, -0.1584,  0.8657, -1.6459], grad_fn=<EmbeddingBackward0>)\n",
      "已经 -> tensor([ 0.7600, -1.7611,  1.2300,  0.3257], grad_fn=<EmbeddingBackward0>)\n",
      "过半 -> tensor([-1.1398,  0.8186, -2.8867,  2.1403], grad_fn=<EmbeddingBackward0>)\n",
      "， -> tensor([ 0.2748, -0.1309, -0.6391,  0.2122], grad_fn=<EmbeddingBackward0>)\n",
      "不少 -> tensor([ 1.1781,  0.7161, -1.8390, -1.4239], grad_fn=<EmbeddingBackward0>)\n",
      "外国 -> tensor([-0.5269,  0.2707,  0.8005, -0.5764], grad_fn=<EmbeddingBackward0>)\n",
      "运动员 -> tensor([ 1.9003, -0.2485, -0.8052,  0.1101], grad_fn=<EmbeddingBackward0>)\n",
      "在 -> tensor([1.5074, 0.5562, 1.0836, 1.3871], grad_fn=<EmbeddingBackward0>)\n",
      "完成 -> tensor([ 0.2636, -1.2331,  0.1731,  0.2015], grad_fn=<EmbeddingBackward0>)\n",
      "自己 -> tensor([ 1.3232, -0.3706, -0.5867, -0.0190], grad_fn=<EmbeddingBackward0>)\n",
      "的 -> tensor([-0.2941, -0.8324,  1.8048, -0.0365], grad_fn=<EmbeddingBackward0>)\n",
      "比赛 -> tensor([ 0.5163, -0.2148, -1.3204,  1.8485], grad_fn=<EmbeddingBackward0>)\n",
      "后 -> tensor([ 1.2490, -1.0645,  0.9838,  0.3320], grad_fn=<EmbeddingBackward0>)\n",
      "踏上 -> tensor([ 0.6138, -0.7303,  0.3944,  0.3472], grad_fn=<EmbeddingBackward0>)\n",
      "归途 -> tensor([-0.9553, -0.6905,  0.5758, -0.0264], grad_fn=<EmbeddingBackward0>)\n",
      "。 -> tensor([-1.6899,  1.0526,  0.5242,  0.1932], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import jieba #分词工具 pip install jieba\n",
    "\n",
    "def test01():\n",
    "    text = '北京东奥的进度条已经过半，不少外国运动员在完成自己的比赛后踏上归途。'\n",
    "    \n",
    "    # 1.分词\n",
    "    words = jieba.lcut(text) \n",
    "    print(words)\n",
    "    \n",
    "    # 2.构建词表\n",
    "    index_to_word = {} #用索引区词\n",
    "    word_to_index = {} #用word取词索引\n",
    "    # 去重\n",
    "    unique_words = list(set(words))\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        index_to_word[idx] = word\n",
    "        word_to_index[word] = idx\n",
    "    \n",
    "    # print(index_to_word)\n",
    "    \n",
    "    ##########\n",
    "    # 3.构建词嵌入层    num_embeddings:词表大小;   embedding_dim:词向量维度\n",
    "    embed = nn.Embedding(num_embeddings=len(index_to_word), embedding_dim=4)\n",
    "    print(embed)\n",
    "    \n",
    "    # 4.句子用词向量表示\n",
    "    for word in words:\n",
    "        # \n",
    "        idx = word_to_index[word]\n",
    "        # 获取这个词的词向量表示\n",
    "        word_vec = embed(torch.tensor(idx))\n",
    "        print(word, '->', word_vec)\n",
    "        \n",
    "    \n",
    "test01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78949956-8b5c-44df-9d01-88f26ddef21c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8623, -0.9483,  0.9983]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.8623, -0.9483,  0.9983]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# rnn输入单个单词\n",
    "def test02(): \n",
    "\n",
    "    # input_size: 单个词向量的维度\n",
    "    # hidden_size: 隐藏层大小，也就是隐藏层中神经元个数，同时也是这个隐藏层输出数据的个数(每个神经元输出一个标量，多个神经元就输出多个标量)\n",
    "    # num_layers: 隐藏层的个数\n",
    "    input_size = 128\n",
    "    hidden_size = 3\n",
    "    num_layers = 1\n",
    "    rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "    \n",
    "    # 模拟要输入的一个词向量\n",
    "    # inputs形状 (seq_len, batch_size, input_size)\n",
    "    # seq_len: 句子长度，句子中词向量个数\n",
    "    seq_len = 1\n",
    "    batch_size = 1\n",
    "    inputs = torch.randn(seq_len,batch_size,input_size)\n",
    "    \n",
    "    # 初始隐藏层\n",
    "    # 隐藏层形状 (num_layers, batch_size, hidden_size)\n",
    "    hn = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "    \n",
    "    # 将这个词向量送到rnn中\n",
    "    # output表示每个输入词向量对应的中间状态，h0表示最后一个词向量的中间状态\n",
    "    # 在Pytorch中定义的RNN，其实是没有y这个的输出。\n",
    "    # Pytorch版本的两个输出，output=[h1, h2, h3, h4], hn = h4\n",
    "    # 如果想要得到输出层y，可以自行加一个全连接层\n",
    "    output, hn = rnn(inputs, hn)\n",
    "    print(output)\n",
    "    print(hn)\n",
    "    \n",
    "test02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b4bb02e-916f-4b50-8501-d73bc53a6452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4931, -0.6627, -0.9259,  0.2352, -0.8524, -0.6563, -0.9168,\n",
      "          -0.9999]],\n",
      "\n",
      "        [[ 0.9473, -0.9958,  0.2207, -0.9973, -0.6312,  0.9970, -0.9940,\n",
      "          -0.7935]],\n",
      "\n",
      "        [[-0.9988,  0.4833,  0.9190, -0.0450,  0.8556, -0.2926,  0.3449,\n",
      "           0.9977]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[-0.9988,  0.4833,  0.9190, -0.0450,  0.8556, -0.2926,  0.3449,\n",
      "           0.9977]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# rnn输入句子\n",
    "def test03(): \n",
    "    # input_size: 单个词向量的维度\n",
    "    # hidden_size: 隐藏层大小，也就是隐藏层中神经元个数，同时也是这个隐藏层输出数据的个数(每个神经元输出一个标量，多个神经元就输出多个标量)\n",
    "    # num_layers: 隐藏层的个数\n",
    "    input_size = 128\n",
    "    hidden_size = 8\n",
    "    num_layers = 1\n",
    "    rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "    \n",
    "    # 模拟要输入的一个词向量\n",
    "    # seq_len: 句子长度，句子中词向量个数\n",
    "    # inputs形状 (seq_len, batch_size, input_size)\n",
    "    seq_len = 3 #3个词向量\n",
    "    batch_size = 1\n",
    "    inputs = torch.randn(seq_len,batch_size,input_size)\n",
    "    \n",
    "    # 初始隐藏层\n",
    "    # 隐藏层形状 (num_layers, batch_size, hidden_size)\n",
    "    hn = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "    \n",
    "    # 将句子送到rnn中\n",
    "    # output表示每个输入词向量对应的中间状态，h0表示最后一个词向量的中间状态\n",
    "    # 在Pytorch中定义的RNN，其实是没有y这个的输出。\n",
    "    # Pytorch版本的两个输出，output=[h1, h2, h3, h4], hn = h4\n",
    "    # 如果想要得到输出层y，可以自行加一个全连接层\n",
    "    output, hn = rnn(inputs, hn)\n",
    "    print(output)\n",
    "    print(hn)\n",
    "    \n",
    "test03()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8af9a-155f-4f34-978a-4237443fb499",
   "metadata": {},
   "source": [
    "# 文本生成---周杰伦歌词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "92fbfa6b-9303-4dca-bd5b-4221ac59f841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import jieba\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# 构建词典\n",
    "def build_vocab():\n",
    "    fname = 'data/jaychou_lyrics.txt'\n",
    "    \n",
    "    # 1.文本数据清晰\n",
    "    clean_sentences = [] #存放的是清理好的句子\n",
    "    for line in open(fname, 'r'):\n",
    "        # 去除某些内容\n",
    "        line = line.replace('〖韩语Rap译文〗','') \n",
    "        # 只保留特定符号 (中文、英文、数字、部分标点符号)\n",
    "        line = re.sub(r'[^\\u4e00-\\u9fa5 a-zA-Z0-9!?,]', '', line) \n",
    "        # 连续空格替换成1个\n",
    "        line = re.sub(r'[ ]{2,}', ' ', line)\n",
    "        # 去除两侧空格、换行\n",
    "        line = line.strip()\n",
    "        # 去除单字的行\n",
    "        if len(line) <= 1:\n",
    "            continue\n",
    "        \n",
    "        #去掉重复行\n",
    "        if line not in clean_sentences:\n",
    "            clean_sentences.append(line)\n",
    "        \n",
    "    # print(clean_sentences)\n",
    "    # 2.分词并构建重要词表映射\n",
    "    index_to_word = [] # 索引到词的映射 ['词1'，'词2'，'词3']\n",
    "    all_sentences = [] # 存放所有句子分词后的形式[[句子1分词],[句子2分词],[]]\n",
    "    for line in clean_sentences:\n",
    "        #分词\n",
    "        words = jieba.lcut(line)\n",
    "        # 分好词的句子存放到all_sentences中\n",
    "        all_sentences.append(words)\n",
    "        \n",
    "        ####构造index_to_word 索引到词的 词表映射\n",
    "        for word in words:\n",
    "            if word not in index_to_word:\n",
    "                index_to_word.append(word)\n",
    "    \n",
    "    # print(index_to_word)\n",
    "    ##### 构建词到索引的词表映射\n",
    "    word_to_index = {word:idx for idx, word in enumerate(index_to_word)}\n",
    "    \n",
    "    ##### 将输入的句子转换为索引表示\n",
    "    # 把all_sentences中的每个词变成索引 ->放到 corpus_index\n",
    "    corpus_index = []\n",
    "    for line in all_sentences:\n",
    "        temp = []\n",
    "        for word in line:\n",
    "            temp.append(word_to_index[word])\n",
    "        \n",
    "        # 每个句子最后加个空格 ' ' ?????\n",
    "        temp.append(word_to_index[' '])\n",
    "        #corpus_index.append(temp)\n",
    "        corpus_index.extend(temp) # 变成一维数组? 其实无所谓，做能做成datasets就行\n",
    "    \n",
    "    # [0, 1, 2, 39,   0, 3, 4, 5, 6, 7, 39,    ]\n",
    "    # print(corpus_index)\n",
    "    # 词的数量\n",
    "    word_count = len(index_to_word)\n",
    "    return index_to_word, word_to_index, word_count,corpus_index,all_sentences\n",
    "   \n",
    "    \n",
    "# test\n",
    "def test00():\n",
    "    index_to_word, word_to_index, word_count,corpus_index, all_sentences = build_vocab()\n",
    "    print('word_count-> ',word_count)\n",
    "    print('index_to_word-> ',index_to_word[:10])\n",
    "    print('word_to_index-> ',dict(list(word_to_index.items())[:7]))\n",
    "    \n",
    "    print('corpus_index-> ',corpus_index[:10]) #显示前10个词\n",
    "    print('all_sentences-> ',all_sentences[:2]) #显示前2个句子\n",
    "\n",
    "# test00()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "3de7e2cd-7c29-4a4f-ba95-6084f31a987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据集对象\n",
    "class LyricsDataset:\n",
    "    # corpus_idx: 所有句子的索引表示\n",
    "    # num_chars: 用它表示句子的长度，也就是一个句子的单词个数，但实际情况句子长度不是固定的，这里就假设固定了\n",
    "    #           \n",
    "    def __init__(self, corpus_index, num_chars):\n",
    "        # 所有句子的索引表示\n",
    "        self.corpus_index = corpus_index\n",
    "        # 句子固定长度\n",
    "        self.num_chars = num_chars\n",
    "        # 词的个数\n",
    "        self.word_count = len(self.corpus_index)\n",
    "        # 整个数据集中有多少个句子\n",
    "        self.number = self.word_count // self.num_chars\n",
    "    \n",
    "    # 整个数据集中有多少个句子\n",
    "    def __len__(self):\n",
    "        return self.number\n",
    "    \n",
    "    # 获取一个样本, idx:表示取第几个样本\n",
    "    def __getitem__(self, idx):\n",
    "        # 修改索引值: [0, self.word_count - 1]\n",
    "        start = min(max(idx,0), self.word_count - self.num_chars - 2)\n",
    "        \n",
    "        # 假设样本      x = [0, 1, 9, 8, ... ]\n",
    "        # 那么目标值就是 y = [1, 9, 8, ... ], 正好跟上面的x错开一位\n",
    "        # start = start * self.num_chars # 要不要加这一句 ?????\n",
    "        x = self.corpus_index[start: start + self.num_chars]\n",
    "        y = self.corpus_index[start+1: start+1  + self.num_chars]\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "    \n",
    "    \n",
    "def test02():\n",
    "        _, _, _, corpus_idx,_ = build_vocab()\n",
    "        # 数据集\n",
    "        lyrics = LyricsDataset(corpus_idx, 5) # 句子长度是5\n",
    "        # 加载数据\n",
    "        dataloader = DataLoader(lyrics, batch_size=2)\n",
    "\n",
    "        \n",
    "        print('corpus_index-> ',corpus_idx[:15])\n",
    "        for x,y in dataloader:\n",
    "            print(x)\n",
    "            print(y)\n",
    "            break\n",
    "        \n",
    "# test02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "a41954f5-f2d1-4b8c-bdfe-3fcb32be4528",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--->  <generator object Module.parameters at 0x147f7d000>\n",
      "epoch   1 loss: 6.79065 time 3.09\n",
      "--->  <generator object Module.parameters at 0x147f7d000>\n",
      "epoch   2 loss: 4.55815 time 3.04\n",
      "--->  <generator object Module.parameters at 0x147f7d000>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[594], line 172\u001b[0m\n\u001b[1;32m    169\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/text-generator_10.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m train()\n",
      "Cell \u001b[0;32mIn[594], line 154\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# 参数更新\u001b[39;00m\n\u001b[1;32m    156\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构建网络模型\n",
    "class TextGenerator(nn.Module):\n",
    "    # word_len: 词表大小\n",
    "    def __init__(self, word_len):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        # 初始化词嵌入层\n",
    "        # num_embeddings:词表大小\n",
    "        # embedding_dim: 词向量维度\n",
    "        embedding_dim = 128\n",
    "        self.ebd = nn.Embedding(num_embeddings=word_len, embedding_dim=embedding_dim)\n",
    "        # print(self.ebd(torch.tensor(4)))\n",
    "        \n",
    "        # 循环网络 \n",
    "        # input_size: 词向量维度\n",
    "        # hidden_size: 隐藏层大小,输出维度(随便写),神经元个数,中间状态个数\n",
    "        # num_layers: 网络层个数\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=128, num_layers=1) \n",
    "        \n",
    "        # 输出层, 输出值个数是word_len的长度，应为要从整个此表中预测一个词的概率\n",
    "        self.out = nn.Linear(128, word_len)\n",
    "      \n",
    "    \n",
    "    def get_embd(self):\n",
    "        return self.ebd\n",
    "\n",
    "    # 前向传播\n",
    "    # inputs: 输入的样本  (btach_size, seq_len)\n",
    "    # hn: 中间隐藏层状态  (num_layers, batch_size, hidden_size)\n",
    "    def forward(self, inputs, hn): \n",
    "        \n",
    "        # 词嵌入\n",
    "        embed = self.ebd(inputs)\n",
    "        # print('inputs-> ',inputs.shape)\n",
    "        # print('embed-> ', embed.shape)\n",
    "\n",
    "        # 正则化\n",
    "        embed = self.dropout(embed)\n",
    "        \n",
    "        # 送入rnn\n",
    "        # (btach_size, seq_len, word_dim) -> (seq_len, btach_size, word_dim)\n",
    "        # output: 包含每个时刻中间状态\n",
    "        # hn: 最后一个时刻的中间状态\n",
    "        # print('-->111111 ',inputs.shape, embed.shape, hn.shape)\n",
    "        output, hn = self.rnn(embed.transpose(0,1), hn)\n",
    "        \n",
    "        # 放入全连接层  output -> out\n",
    "        out = self.out(output)\n",
    "\n",
    "        # hn = self.out(hn)\n",
    "        return out, hn\n",
    "       \n",
    "        \n",
    "    def init_hn(self,batch_size):\n",
    "        # print('batch_size-> ', batch_size)\n",
    "        # (num_layers, batch_size, hidden_size)\n",
    "        return torch.zeros(1,batch_size,128)\n",
    "\n",
    "    \n",
    "    \n",
    "def test03():\n",
    "    index_to_word, word_to_index, word_count, corpus_idx,_ = build_vocab()   \n",
    "    lyrics = LyricsDataset(corpus_idx, 5) # 5:句子长度\n",
    "   \n",
    "    batch_size = 1\n",
    "    lyrics_dataloader = DataLoader(lyrics, shuffle=False, batch_size=batch_size)\n",
    "    model = TextGenerator(word_count)\n",
    "    \n",
    "    for x, y in lyrics_dataloader:\n",
    "        h0 = model.init_hn(batch_size)\n",
    "        print('h0.shape-> ',h0.shape) #(num_layers, batch_size, hidden_size)\n",
    "        print('x.shape-> ', x.shape)  #(btach_size, seq_len)\n",
    "        print('y.shape-> ', y.shape)  #(btach_size, seq_len)\n",
    "       \n",
    "        output, hn = model(x, h0)\n",
    "        print('output.shape->',output.shape) #(seq_len, btach_size, 词表长度)\n",
    "        print('hn.shape->', hn.shape)\n",
    "\n",
    "        break\n",
    "\n",
    "# test03()\n",
    "\n",
    "\n",
    "\n",
    "### 训练\n",
    "def train():\n",
    "    # 构建词典\n",
    "    index_to_word, word_to_index, word_count, corpus_idx,_ = build_vocab()\n",
    "    # 数据集\n",
    "    num_chars = 32  # 句子长度\n",
    "    lyrics = LyricsDataset(corpus_idx, num_chars)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = TextGenerator(word_count)\n",
    "    emdb = model.get_embd()\n",
    "    \n",
    "    # 损失函数\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "     # 优化方法\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "    \n",
    "    # 训练轮数\n",
    "    epoch = 10\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch_idx in range(epoch):\n",
    "        # print('词向量 --->',epoch_idx ,'--- ',emdb(torch.tensor(4)))\n",
    "        for name,param in model.named_parameters():\n",
    "            print(name,'---> ',param)\n",
    "            \n",
    "\n",
    "        batch_size = 333\n",
    " \n",
    "         # 数据加载器\n",
    "        lyrics_dataloader = DataLoader(lyrics, shuffle=True, batch_size=batch_size)\n",
    "        # 训练时间\n",
    "        start = time.time()\n",
    "        # 迭代次数\n",
    "        iter_num = 0\n",
    "        # 训练损失\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for x, y in lyrics_dataloader:\n",
    "            # x形状 (btach_size, seq_len)\n",
    "            \n",
    "            # 初始隐藏状态  batch_size == x.shape[0]\n",
    "            hn = model.init_hn(x.shape[0])\n",
    "            \n",
    "            # 模型计算\n",
    "            # out形状 (seq_len, btach_size, 词表长度)\n",
    "            out, hn = model(x, hn)\n",
    "            # print('out.shape-> ',out.shape) #[seq_len, batch_size, 词表长度]\n",
    "            # print('y.shape-> ',y.shape,y) #[batch_size, seq_len]\n",
    "            # print('out before-> ', out)\n",
    "            \n",
    "            # out形状 [seq_len, batch_size, 词表长度(分类个数)] -> [batch_size * seq_len, 词表长度(分类个数)]\n",
    "            out = out.permute(1,0,2)\n",
    "            out = out.reshape(out.shape[0]*out.shape[1], out.shape[2])\n",
    "            # print(out.shape)\n",
    "            # print('out after-> ', out)\n",
    "\n",
    "            \n",
    "            # y形状 (batch_size, seq_len) -> (batch_size * seq_len)\n",
    "            # print('y before->',y)\n",
    "            y = y.reshape(y.shape[0] * y.shape[1])\n",
    "            # print('y after->',y)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = loss_fun(out, y)\n",
    "            \n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 参数更新\n",
    "            optimizer.step()\n",
    "            \n",
    "            iter_num += 1\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        message = 'epoch %3s loss: %.5f time %.2f' % \\\n",
    "                  (epoch_idx + 1,\n",
    "                   total_loss / iter_num,\n",
    "                   time.time() - start)\n",
    "        print(message)  \n",
    "        \n",
    "    # 模型存储\n",
    "    torch.save(model.state_dict(), 'model/text-generator_10.pth')\n",
    "\n",
    "# 训练\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "876a1bb8-149f-4421-8635-6091d75fdd52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6032,  1.0777,  1.0448,  0.3063,  1.0159, -1.1750,  0.1273, -0.6190,\n",
      "         0.0452, -0.4296, -1.4291,  0.5964,  1.1408,  1.9881, -0.6465, -0.1233,\n",
      "        -0.5359,  0.4323, -0.9345, -0.5459, -0.5024, -0.6420,  0.1008,  1.9956,\n",
      "        -1.7389, -1.7927,  0.5973,  1.9554,  0.1012,  0.3456,  0.4889, -0.0502,\n",
      "        -0.8623,  2.0572,  0.5201, -1.3454, -0.9763, -1.9414,  1.2468,  0.3165,\n",
      "         0.8494,  1.0635, -1.0093,  0.9467,  0.8961, -0.3868,  0.6270,  1.0438,\n",
      "         1.2811,  0.4118, -1.7610, -0.4277,  0.6290,  1.3352, -1.0936, -0.1874,\n",
      "        -1.0669, -1.2879, -0.7912, -0.0576,  1.5061, -0.9747, -0.2678, -0.1020,\n",
      "         0.8284, -0.8102,  0.2684, -1.2154, -0.2392,  0.1568, -1.9914, -1.6981,\n",
      "        -1.4385,  0.4117, -0.5493, -0.6133, -1.1851, -0.9440, -1.6551,  0.8597,\n",
      "        -1.1864, -2.1758, -0.7756, -0.4431, -0.6054, -1.7768, -0.9309,  0.0692,\n",
      "        -0.4577,  0.4310, -1.3327, -0.9335, -0.3493, -0.0701, -0.1699, -0.4641,\n",
      "        -0.4948, -0.8849,  0.5000, -1.0277,  0.2530,  0.9317,  0.5788,  2.6387,\n",
      "        -0.5986,  0.8174, -0.3882, -0.6506,  0.0719, -0.2444, -1.4631,  0.7323,\n",
      "        -0.4060, -0.3494,  1.3497,  3.4400,  0.3964, -0.6023,  0.7322,  1.9974,\n",
      "        -1.2726,  1.4009,  1.4204, -0.5516, -0.1455, -1.6746, -0.0279, -0.4123],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "你 ( 4 )是 ( 63 )谁 ( 321 )让 ( 19 )晶莹 ( 76 )的 ( 17 )泪滴 ( 77 )  ( 39 )谁 ( 321 )让 ( 19 )它 ( 178 )停留 ( 282 )的 ( 17 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )语 ( 241 )沉默 ( 242 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )语 ( 241 )沉默 ( 242 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )语 ( 241 )沉默 ( 242 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )语 ( 241 )沉默 ( 242 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )语 ( 241 )沉默 ( 242 )  ( 39 )你 ( 4 )的 ( 17 )完美主义 ( 83 )  ( 39 )"
     ]
    }
   ],
   "source": [
    "# 预测函数\n",
    "# text :输入的句子\n",
    "# sentence_length: 要预测的单词个数\n",
    "def predict(text, sentence_length):\n",
    "    # 构建词典\n",
    "    index_to_word, word_to_index, word_count, _ ,_= build_vocab()\n",
    "    # 构建模型\n",
    "    model = TextGenerator(word_count)\n",
    "    # 加载参数\n",
    "    model.load_state_dict(torch.load('model/text-generator_10.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    emdb = model.get_embd()\n",
    "    print(emdb(torch.tensor(4)))\n",
    "    \n",
    "    #分词\n",
    "    words = jieba.lcut(text)\n",
    "    word_idx_s = [] #分好的词放到这里\n",
    "    generate_sentence = [] #预测的值放到这里\n",
    "    for word in words:\n",
    "        word_idx = word_to_index[word]\n",
    "        word_idx_s.append(word_idx)\n",
    "        # 存放预测的结果\n",
    "        generate_sentence.append(word_idx)\n",
    "  \n",
    "    # print('---> ',word_idx,index_to_word[word_idx])\n",
    "    \n",
    "    # 初始隐藏状态\n",
    "    hn = model.init_hn(1)\n",
    "    # print(h0)\n",
    "        \n",
    "    # 开始预测\n",
    "    for _ in range(sentence_length):\n",
    "        # print(word_idx_s)\n",
    "        out, hn = model(torch.tensor([word_idx_s]), hn)\n",
    "        # print(out.shape)\n",
    "        \n",
    "        # 取最后一个预测的词\n",
    "        out = out[out.shape[0] - 1]\n",
    "        # print('out.shape-> ',out.shape)\n",
    "\n",
    "        # 选择分数最大的词作为预测词\n",
    "        word_idx = torch.argmax(out,dim=1).item()\n",
    "        # print('--> ',word_idx)\n",
    "  \n",
    "        # 预测词放入到预测列表\n",
    "        generate_sentence.append(word_idx)\n",
    "        \n",
    "        word_idx_s = [word_idx]\n",
    "    \n",
    "    # 打印预测的词\n",
    "    for idx in generate_sentence:\n",
    "        print(index_to_word[idx],'(',idx,')', end='')\n",
    "        pass\n",
    "        \n",
    "    \n",
    "\n",
    "predict('你是谁',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfba437-a46a-4a57-bab6-d93340056532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
