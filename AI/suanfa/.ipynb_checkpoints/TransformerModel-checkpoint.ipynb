{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4defbe95-09cf-4299-98f2-ceae724d991f",
   "metadata": {},
   "source": [
    "# 一、 ========= model.py =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f848c128-c072-4010-9535-306caf581c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################### model.py #####################\n",
    "###################################################\n",
    "import math\n",
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# 嵌入层封装\n",
    "class Embeddings(nn.Module):\n",
    "    # vocab_size: 词表大小\n",
    "    # d_model: 词向量维度\n",
    "    def __init__(self, vocab_size, d_model, padding_idx=0):\n",
    "        super().__init__()\n",
    "        # lookup table Embeding层\n",
    "        self.lut = nn.Embedding(vocab_size, d_model, padding_idx,)\n",
    "        # 词向量维度\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    # 根据词id返回词向量\n",
    "    # x: 入参(句子个数, 句子的id表示)，形状是(句子个数，句子长度)\n",
    "    #    例如: [[1,2,4],[2,4,5]]\n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 2 , '入参形状需要是2维的(句子个数，句子长度)'\n",
    "        \n",
    "        # 返回句子的向量表示,形状(句子个数,句子长度,词向量维度)\n",
    "        # 这里返回的结果，每个元素都乘以了 根号下词向量维度\n",
    "        # 后面对模型词嵌入层参数初始化时，用的方法是xavier,该方法随机初始化参数满足N(0, 1/d_model),乘以 math.sqrt(self.d_model)，可以拉回到 N(0, 1) 分布\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "\n",
    "    \n",
    "# 优化后的位置编码\n",
    "# annotated-transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # max_len: 位置编码最大长度，也是句子长度\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 位置编码为什要加dropout ？？？？？\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 初始化位置编码\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # 位置和除数\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * math.log(10000) / d_model)\n",
    "        \n",
    "        angle = position * div_term\n",
    "        \n",
    "        # 填充pe矩阵值  0::2 从0开始步长是2  1::2 从1开始步长是2\n",
    "        pe[:, 0::2] = torch.sin(angle)\n",
    "        pe[:, 1::2] = torch.cos(angle)\n",
    "        \n",
    "        # pe = torch.linspace(-1,1,max_len).unsqueeze(1)\n",
    "        # pe = pe @ torch.ones(1,max_len)[:,:d_model]\n",
    "\n",
    "        # 同 self.pe = pe  \n",
    "        self.register_buffer('pe', pe) # 表示该值不用梯度更新\n",
    "        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 3 , '入参形状必须是 (batch, sentence_len, d_model)'\n",
    "        \n",
    "        # ## 把某个位置元素设置为0，这个位置是批量入参时的padding  临时功能 #####################\n",
    "        # pe = self.pe[:x.size(1)].masked_fill((x==0), 0)\n",
    "        # return x + pe\n",
    "\n",
    "        #### 正经功能 #####        \n",
    "        # 入参和位置编码相加\n",
    "        # 入参x的形状     (batch, sentence_len, d_model)\n",
    "        # 位置编码形状是   (max_len, d_model)\n",
    "        # 相加的时候位置编码只取 入参的sentens_len(句子长度)长度\n",
    "        # print('PositionalEncoding.forward--> ',x.shape, self.pe.shape)\n",
    "        # print('-->',self.pe[:x.size(1)].shape)\n",
    "        return x + self.pe[:x.size(1)]    \n",
    "    \n",
    "        # 位置编码为什么要加dropout ?????\n",
    "        #return self.dropout(x + self.pe[:x.size(1)])\n",
    "\n",
    "\n",
    "# 封装掩码 \n",
    "# 用来屏蔽批量入参时，句子长短不一致, 批量产生的掩码\n",
    "# 只返回掩码\n",
    "def get_padding_mask(x, padding_idx):\n",
    "    assert x.ndim == 2, '期望维度为2'\n",
    "    \n",
    "    # 入参x形状是 (批次，句子长度), 中间加一个维度，用来和带词向量的入参匹配\n",
    "    # (B,L) -> (B,L,d_model)\n",
    "    return (x == padding_idx).unsqueeze(1)\n",
    "   \n",
    "    \n",
    "\n",
    "# 计算注意力\n",
    "# q、k、v的形状都是(batch,sen_len,d_model)\n",
    "def attention(q,k,v, mask=None, dropout=None):\n",
    "    # 获取q、k、v的最后一个维度,即d_model\n",
    "    d_model = q.size(-1)\n",
    "    # 计算注意力分数 q乘k的转置\n",
    "    scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_model)\n",
    "    \n",
    "    # 按照掩码mask对scores进行屏蔽\n",
    "    if mask is not None:\n",
    "        # 列掩码\n",
    "        # print('mask->',mask.shape)\n",
    "        # print('scores->',scores.shape)\n",
    "        scores = scores.masked_fill(mask, -1e9)\n",
    "    \n",
    "    # 用softmax计算出分数的比重\n",
    "    softmax_scores = torch.softmax(scores, dim=-1)\n",
    "    \n",
    "    # dropout\n",
    "    if dropout is not None:\n",
    "        softmax_scores = dropout(softmax_scores)\n",
    "    \n",
    "    # 返回带注意力的数据\n",
    "    context = torch.matmul(softmax_scores, v)\n",
    "    \n",
    "    return context,  softmax_scores\n",
    "\n",
    "\n",
    "\n",
    "# 多头注意力\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout=0):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 头的数量要可以被词向量维度整除\n",
    "        assert d_model % n_head == 0\n",
    "        # 新头的维度\n",
    "        self.d_k = d_model // n_head\n",
    "        # 新头的个数\n",
    "        self.n_head = n_head\n",
    "        \n",
    "        # q k v多头之前先进行一次线性变换(原论文是先分头再线性变换，实验证明两者差不多)\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 多头拼接成原维度后再进行一次线性变换\n",
    "        self.linear = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    # 这里如果是自注意力，那么qkv就是x，入参直接写x就行了，为啥搞了3个入参?????\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        assert q.ndim == 3, ' 形状要求(batch,words_len,d_model)'\n",
    "        assert k.ndim == 3, ' 形状要求(batch,words_len,d_model)'\n",
    "        assert v.ndim == 3, ' 形状要求(batch,words_len,d_model)'\n",
    "\n",
    "        \n",
    "        # 残差值，残差和规划应该放到外边，这里就凑活把\n",
    "        residual = q #把q当成入参了，放到做外边可以把带词向量的入参当入参\n",
    "        \n",
    "        # 批次\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # 1.先线性变换\n",
    "        q = self.W_Q(q)\n",
    "        k = self.W_K(k)\n",
    "        v = self.W_V(v)\n",
    "        \n",
    "        # 分头前\n",
    "        # print(q)\n",
    "            \n",
    "        # 2.分头 \n",
    "        # (batch,words_len,d_model) -> (batch, words_len, n_head, d_k)\n",
    "        q = q.view(batch_size, -1, self.n_head, self.d_k)\n",
    "        # (batch, words_len, n_head, d_k) -> (batch, n_head, words_len, d_k) \n",
    "        q = q.transpose(1,2)\n",
    "        \n",
    "        # 分头后\n",
    "        # print(q)\n",
    "\n",
    "            \n",
    "        k = k.view(batch_size, -1, self.n_head, self.d_k)\n",
    "        k = k.transpose(1,2)\n",
    "        \n",
    "        v = v.view(batch_size, -1, self.n_head, self.d_k)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        # 这里需要把掩码做扩维度吗？不可以自动广播吗 ??????\n",
    "        # 因为头是从(batch,words_len,d_model)变成了(batch,n_head,words_len,d_k)，所以掩码在头的维度扩一维\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) \n",
    "        \n",
    "        # 计算注意力\n",
    "        context, softmax_scores = attention(q,k,v, mask=mask, dropout=self.dropout)\n",
    "        # print(context)\n",
    "        \n",
    "        # 掩码的形状和 注意力分数的形状， 注意力分数只关注词的个数\n",
    "        # print('mask.shape-> ',mask.shape, 'softmax_scores.shape->',  softmax_scores.shape)\n",
    "\n",
    "        # 把多头合并成一个头\n",
    "        # (batch, n_head, words_len, d_k) -> (batch, words_len, n_head ,d_k) -> (batch, words_len, n_head * d_k)\n",
    "        context = context.transpose(1,2).reshape(batch_size, -1, self.n_head * self.d_k)\n",
    "        # print(context)\n",
    "\n",
    "        # 然后再过一个线性层\n",
    "        context = self.linear(context)\n",
    "        \n",
    "        # print('residua-> ',residual)\n",
    "        # 残差和层归一化(这个操作最好放到外边)\n",
    "        return self.norm(residual+ context), softmax_scores \n",
    "\n",
    "    \n",
    "\n",
    "# 前馈神经网络\n",
    "class FeedForward(nn.Module):\n",
    "    # d_model: 输入维度  d_ff:输出维度\n",
    "    def __init__(self, d_model, d_ff, dropout=0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        \n",
    "        residual = x\n",
    "\n",
    "        x = self.w_1(x).relu()\n",
    "        x = self.dropout(x)\n",
    "        # print('+'*60)\n",
    "\n",
    "        x = self.w_2(x)\n",
    "        \n",
    "        # 加上残差和归一化\n",
    "        return self.norm(residual + x)\n",
    "\n",
    "    \n",
    "## 编码子层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff, dropout=0):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadedAttention(d_model, n_head, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "\n",
    "        # 1.先计算多头注意力\n",
    "        x,softmax_scores = self.mha(q=x, k=x, v=x, mask=mask)\n",
    "        # print(x.shape)\n",
    "        # 2.在进入前馈神经网络\n",
    "        return self.ff(x)\n",
    "    \n",
    "\n",
    "# 深度拷贝\n",
    "def clones(layer, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
    "\n",
    "## 编码层 好几个EncoderLayer\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        # 有N个EncoderLayer\n",
    "        self.layers = clones(layer,N)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# 获取解码器中的句子掩码，不是批量掩码\n",
    "# words_len 单个句子长度(预测的目标值)\n",
    "def get_subsequent_mask(words_len):\n",
    "    # 形状(batch,words_len,words_len)\n",
    "    mask_shape = (1,words_len,words_len)\n",
    "    \n",
    "    # diagonal=1: 保留主对角线往上1行的数据(不包括主对角线)，不保留的数据都是0 \n",
    "    subsequent_mask = torch.triu(torch.ones(mask_shape), diagonal=1).type(torch.uint8)\n",
    "    \n",
    "    return subsequent_mask == 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 解码器子层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff, dropout=0):\n",
    "        super().__init__()\n",
    "        # 自注意力，多头注意力，qkv同源\n",
    "        self.self_mha = MultiHeadedAttention(d_model=d_model, n_head=n_head, dropout=dropout)\n",
    "        # 交叉注意力，多头注意，q来自解码器，kv来自解码器\n",
    "        self.mha = MultiHeadedAttention(d_model=d_model, n_head=n_head, dropout=dropout)\n",
    "        # 前馈神经网络\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "    \n",
    "    # tgt_mask 是解码层的mask，是叠加了批量掩码和单词掩码的   \n",
    "    # memory 解码器生成的，用来产生kv的中间量\n",
    "    # src_mask 解码器的批量掩码，交叉注意力时，q乘k的转置，需要把k后面的列屏蔽掉，跟解码器是同一个批量掩码\n",
    "    # training: 当前是否处于训练中\n",
    "    def forward(self, x, tgt_mask, memory, src_mask, training=True):\n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        assert tgt_mask.ndim == 3, '形状要求(batch, sentence_len, sentence_len)'\n",
    "        assert memory.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        assert src_mask.ndim == 3 , '形状要求(batch, sentence_len, sentence_len)'\n",
    "            \n",
    "             \n",
    "        ##   掩码形状保持和入参一致  x形状(batch, sentence_len, d_model),x.size(1)表示词个数     ###\n",
    "        if training != True:\n",
    "            tgt_mask = tgt_mask[:, :, :x.size(1)]\n",
    "        ######################################################################################    \n",
    "            \n",
    "        # 1.自多头注意力  \n",
    "        x,softmax_scores = self.self_mha(q=x, k=x, v=x, mask=tgt_mask)\n",
    "        \n",
    "        \n",
    "        ## 如果当前是预测,此时x只需要传递最后一个词就可以了,因为预测词也是一个一个生成的, 但是在循环layer的时候上面的掩码也得变 ###\n",
    "        if training != True:\n",
    "            x = x[:, -1, :].unsqueeze(1)\n",
    "         ######################################################################################\n",
    "        \n",
    "            \n",
    "        # 2.交叉注意力,q有解码器生成， kv有解码器产生，\n",
    "        x,softmax_scores = self.mha(q=x, k=memory, v=memory, mask=src_mask)\n",
    "\n",
    "        \n",
    "        # 做前馈神经网络\n",
    "        return self.feed_forward(x)\n",
    "    \n",
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        # N个DecoderLayer\n",
    "        self.layers = clones(layer,N)\n",
    "    \n",
    "    def forward(self, x, tgt_mask, memory, src_mask, training=True):        \n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        assert tgt_mask.ndim == 3, '形状要求(batch, sentence_len, sentence_len)'\n",
    "        \n",
    "        assert memory.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        assert src_mask.ndim == 3 , '形状要求(batch, sentence_len, sentence_len)'\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, tgt_mask, memory, src_mask, training)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    # vocab_size: 目标语言的词表大小\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        # \n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "\n",
    "        # x形状 (batch,words_len, d_model) ->(batch, words_len, vocab_size)\n",
    "        x = self.linear(x)\n",
    "        # return torch.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# 模型\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    def forward(self, src_x, src_mask, tgt_x, tgt_mask):\n",
    "        src_embed = self.src_embed(src_x)\n",
    "        tgt_embed = self.tgt_embed(tgt_x)\n",
    "        \n",
    "        memory = self.encoder(src_embed, src_mask)\n",
    "        output = self.decoder(tgt_embed, tgt_mask, memory, src_mask)\n",
    "        \n",
    "        return self.generator(output)\n",
    "\n",
    "    # 编码\n",
    "    def encode(self, src_x, src_mask):\n",
    "        return self.encoder(self.src_embed(src_x), src_mask)\n",
    "\n",
    "    # 解码\n",
    "    def decode(self, tgt_x, tgt_mask, memory, src_mask, training=True):\n",
    "        return self.decoder(self.tgt_embed(tgt_x),tgt_mask, memory, src_mask, training)\n",
    "    \n",
    "\n",
    "# 实例化模型    \n",
    "def make_model(src_vocab_size, tgt_vocab_size, d_model, n_head, d_ff, N, dropout):\n",
    "    \n",
    "    # 编码器\n",
    "    encoderLayer = EncoderLayer(d_model, n_head, d_ff, dropout)\n",
    "    encoder = Encoder(encoderLayer, N)\n",
    "   \n",
    "    # 解码层\n",
    "    decoderLayer = DecoderLayer(d_model, n_head, d_ff, dropout)\n",
    "    decoder = Decoder(decoderLayer, N)\n",
    "    \n",
    "    # 位置\n",
    "    position = PositionalEncoding(d_model=d_model, dropout=DROPOUT)\n",
    "\n",
    "    # 解码嵌入, Sequential可以让Embeddings执行完后再执行position\n",
    "    src_embed = nn.Sequential(Embeddings(src_vocab_size, d_model=d_model), position)\n",
    "    \n",
    "    # 编码嵌入\n",
    "    tgt_embed = nn.Sequential(Embeddings(tgt_vocab_size, d_model=d_model), position)\n",
    "    \n",
    "    # 生成器\n",
    "    generator = Generator(d_model, tgt_vocab_size)\n",
    "   \n",
    "    # 生成模型\n",
    "    model = Transformer(encoder, decoder, src_embed, tgt_embed, generator)\n",
    "    \n",
    "    # 初始化模型参数\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            # 正态分布初始化\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    # 返回模型\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6e11bed-878b-4c7d-ab88-4b37a039ee05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 8])\n",
      "tensor([[[0.0633, 0.0712, 0.1933, 0.0571, 0.1171, 0.2794, 0.0425, 0.1761],\n",
      "         [0.0503, 0.0687, 0.2257, 0.0777, 0.1174, 0.2185, 0.0426, 0.1991],\n",
      "         [0.0541, 0.0662, 0.1884, 0.0632, 0.1085, 0.2600, 0.0446, 0.2149],\n",
      "         [0.0680, 0.0641, 0.1552, 0.0454, 0.0959, 0.3157, 0.0479, 0.2077]],\n",
      "\n",
      "        [[0.0653, 0.0737, 0.1399, 0.1017, 0.1368, 0.2984, 0.0340, 0.1503],\n",
      "         [0.0564, 0.0443, 0.3953, 0.0783, 0.1172, 0.2050, 0.0235, 0.0801],\n",
      "         [0.0539, 0.0356, 0.4700, 0.0827, 0.0974, 0.1640, 0.0227, 0.0736],\n",
      "         [0.0621, 0.0512, 0.3756, 0.0740, 0.1287, 0.2115, 0.0237, 0.0731]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[5, 2, 5, 5],\n",
      "        [5, 2, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "###### 测试\n",
    "if __name__ == '__main__':\n",
    "    src_vocab_size = 6\n",
    "    tgt_vocab_size = 8\n",
    "    d_model = 6\n",
    "    n_head = 2\n",
    "    d_ff = 1024\n",
    "    N = 6 \n",
    "    dropout = 0\n",
    "    \n",
    "    model = make_model(src_vocab_size, tgt_vocab_size, d_model, n_head, d_ff, N, dropout)\n",
    "\n",
    "    # print(model)\n",
    "    # 输入数据\n",
    "    src_x = torch.tensor([\n",
    "        [1,2,3],\n",
    "        [3,5,0]\n",
    "    ])\n",
    "    # 解码层的掩码\n",
    "    src_mask = get_padding_mask(src_x, padding_idx=0)\n",
    "    \n",
    "    # 目标数据\n",
    "    tgt_x = torch.tensor([\n",
    "        [2,3,4,5],\n",
    "        [1,2,0,0]\n",
    "    ])\n",
    "    tgt_pad_mask = get_padding_mask(tgt_x, padding_idx=0)\n",
    "    subsequent_mask = get_subsequent_mask(words_len= tgt_x.size(-1))\n",
    "    # 句子掩码和pading掩码的叠加\n",
    "    tgt_mask = tgt_pad_mask | subsequent_mask\n",
    "    \n",
    "    \n",
    "    # 运算\n",
    "    predict = model(src_x, src_mask, tgt_x, tgt_mask)\n",
    "    print(predict.shape)\n",
    "    print(predict)\n",
    "    print(torch.argmax(predict, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd04b6-0065-4525-9471-43eebd31a3f0",
   "metadata": {},
   "source": [
    "# 二、 ========= config.py ============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "080bbf31-6e9b-4389-9079-9e03640058c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "MULTI_GPU: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "BASE_PATH = os.path.dirname('.')\n",
    "\n",
    "# 训练数据文件路径\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, './data/inputs/demo/train.json')\n",
    "# 校验数据文件路径\n",
    "VAL_PATH = os.path.join(BASE_PATH, './data/inputs/demo/val.json')\n",
    "\n",
    "\n",
    "# 存放中文词表的文件\n",
    "ZH_VOCAB_PATH = os.path.join(BASE_PATH, './data/vocab/zh.txt')\n",
    "# 存放英文词表的文件\n",
    "EN_VOCAB_PATH = os.path.join(BASE_PATH, './data/vocab/en.txt')\n",
    "\n",
    "# 特殊字符在词表中的id值\n",
    "PAD_ID = 0  # 屏蔽词id\n",
    "UNK_ID = 1  # 不知道的词id\n",
    "SOS_ID = 2  # 开始标记id\n",
    "EOS_ID = 3  # 结束标记id\n",
    "\n",
    "# 子层编码和解码个数\n",
    "N = 3\n",
    "# 词向量维度\n",
    "D_MODEL = 32\n",
    "# 头数\n",
    "N_HEAD = 2\n",
    "# feedforward 维度\n",
    "D_FF = 128\n",
    "\n",
    "DROPOUT = 0\n",
    "# # 批次\n",
    "BATCH_SIZE = 3\n",
    "BATCH_SIZE_GPU0 = 1\n",
    "\n",
    "# 学习率\n",
    "# LR = 0.0003\n",
    "# 训练次数\n",
    "EPOCHS = 100\n",
    "\n",
    "# 生成句子最大长度 \n",
    "MAX_LEN = 6\n",
    "\n",
    "# 标签平滑\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# 运行设备\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 多GPU\n",
    "MULTI_GPU = False\n",
    "if torch.cuda.device_count() > 1:\n",
    "    MULTI_GPU = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(DEVICE)\n",
    "    print('MULTI_GPU:', MULTI_GPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ff2f6-1b60-43be-8e45-8d56640da198",
   "metadata": {},
   "source": [
    "# 三、 =====  utils.py ====== 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f90a1c30-abb7-4f91-afb7-df6b50c62686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "[8, 4, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "################################# utils.py ################################\n",
    "############################################################################\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "\n",
    "# 安装 sacrebleu\n",
    "# pip install sacrebleu\n",
    "import sacrebleu\n",
    "\n",
    "# 翻译评估\n",
    "# 预测的句子  hyp =  ['我爱吃苹果。', '这本书非常有趣。', '他是一位优秀的演员。']\n",
    "# 参考句子    refs = [['我喜欢吃苹果。', '我喜欢吃水果。'],\n",
    "#                    ['这本书很有意思。', '这本书很好玩。'],\n",
    "#                    ['他是一个出色的演员。', '他是一名杰出的演员。']]\n",
    "def bleu_score(hyp, refs):\n",
    "    bleu = sacrebleu.corpus_bleu(hyp, refs, tokenize='zh')\n",
    "    # 保留2位小数\n",
    "    return round(bleu.score, 2)\n",
    "\n",
    "\n",
    "# 中文分词\n",
    "def divided_zh(sentence):\n",
    "    return jieba.lcut(sentence)\n",
    "\n",
    "# 英文分词\n",
    "def divided_en(sentence):\n",
    "    # 匹配单词和标点符号\n",
    "    pattern = r'\\w+|[^\\w\\s]'\n",
    "    return re.findall(pattern, sentence)\n",
    "\n",
    "# 获取词表\n",
    "def get_vocab(lang='en'):\n",
    "    if lang == 'en':\n",
    "        file_path = EN_VOCAB_PATH\n",
    "    elif lang == 'zh':\n",
    "        file_path = ZH_VOCAB_PATH\n",
    "    \n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        lines = file.read()\n",
    "        \n",
    "    id2vocab = lines.split('\\n')\n",
    "    vocab2id = {v:k for k,v in enumerate(id2vocab)}\n",
    "    \n",
    "    return id2vocab, vocab2id\n",
    "\n",
    "\n",
    "# ### 学习率调整策略 原论文\n",
    "# def lr_lambda_fn(step, model_size, factor, warmup):\n",
    "#     if step == 0:\n",
    "#         step = 1\n",
    "\n",
    "#     return factor * (\n",
    "#         model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "#     )\n",
    "\n",
    "### 自定义策略 自定义\n",
    "def lr_lambda_fn(step, wramup):\n",
    "    lr = 0\n",
    "    if step <= wramup:\n",
    "        lr = step / wramup * 10\n",
    "    else:\n",
    "        lr = wramup / step * 10\n",
    "        \n",
    "    # print('lr-> ',lr)\n",
    "    return max(lr, 0.01)\n",
    "\n",
    "\n",
    "# 查看GPU显存占用情况\n",
    "def print_memory():\n",
    "    # 获取当前可用的GPU数量\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    # 遍历每个GPU，输出GPU的占用情况\n",
    "    for i in range(num_gpus):\n",
    "        gpu = torch.cuda.get_device_name(i)\n",
    "        utilization = round(torch.cuda.max_memory_allocated(i) / 1024**3, 2)  # 显存使用量（以GB为单位）\n",
    "        print(f\"GPU {i}: {gpu}, Memory Utilization: {utilization} GB\")\n",
    "\n",
    "# 调用\n",
    "print_memory()\n",
    "print('--' * 10)\n",
    "\n",
    "\n",
    "# 测试\n",
    "if __name__ == '__main__':\n",
    "    # print(divided_zh('我爱中国'))\n",
    "    # print(divided_en('I love china!'))\n",
    "    \n",
    "    # id2vocab, vocab2id = get_vocab('zh')\n",
    "    # print(id2vocab)\n",
    "    # print(vocab2id)\n",
    "    \n",
    "    target = '我喜欢阅读.'\n",
    "    # 分词\n",
    "    target_vocabs = divided_zh(target)\n",
    "    # 取出整个中文词表\n",
    "    zh_id2vocab, zh_vocab2id = get_vocab('zh')\n",
    "    \n",
    "    # 取句子对应的索引id形式  取每个词对应的id，如果这个词不存在则id默认为UNK_ID\n",
    "    src_x = [zh_vocab2id.get(word, UNK_ID) for word in target_vocabs]\n",
    "    print(src_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "2f654b51-794d-4eb9-865e-c07d0918cc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 学习率调整策略\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def lr_lambda_fn(step, model_size, factor, warmup):\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "factor = 0.1\n",
    "warmup = 100\n",
    "\n",
    "rates = []\n",
    "steps = range(0, 1000)\n",
    "for step in steps:\n",
    "    r = lr_lambda_fn(step, d_model, factor, warmup)\n",
    "    rates.append(r)\n",
    "\n",
    "plt.plot(steps, rates)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### 自定义策略\n",
    "def lr_lambda_fn(step, wramup):\n",
    "    lr = 0\n",
    "    if step <= wramup:\n",
    "        lr = step / wramup * 5\n",
    "    else:\n",
    "        lr = wramup / step * 5\n",
    "    return max(lr, 0.1)\n",
    "\n",
    "rates = []\n",
    "total_epoch = 1000\n",
    "steps = range(total_epoch)\n",
    "for step in steps:\n",
    "    r = lr_lambda_fn(step, total_epoch/10)\n",
    "    rates.append(r)\n",
    "\n",
    "plt.plot(steps, rates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ce46b8eb-27bd-43ba-b0d0-9e194cb0226b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_parallel.py    GPU负载均衡\n",
    "import torch\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "from torch.nn.parallel.parallel_apply import parallel_apply\n",
    "from torch.nn.parallel._functions import Scatter\n",
    "\n",
    "def scatter(inputs, target_gpus, chunk_sizes, dim=0):\n",
    "    def scatter_map(obj):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            try:\n",
    "                return Scatter.apply(target_gpus, chunk_sizes, dim, obj)\n",
    "            except Exception:\n",
    "                print('obj', obj.size())\n",
    "                print('dim', dim)\n",
    "                print('chunk_sizes', chunk_sizes)\n",
    "                quit()\n",
    "        if isinstance(obj, tuple) and len(obj) > 0:\n",
    "            return list(zip(*map(scatter_map, obj)))\n",
    "        if isinstance(obj, list) and len(obj) > 0:\n",
    "            return list(map(list, zip(*map(scatter_map, obj))))\n",
    "        if isinstance(obj, dict) and len(obj) > 0:\n",
    "            return list(map(type(obj), zip(*map(scatter_map, obj.items()))))\n",
    "        return [obj for targets in target_gpus]\n",
    "    try:\n",
    "        return scatter_map(inputs)\n",
    "    finally:\n",
    "        scatter_map = None\n",
    "\n",
    "def scatter_kwargs(inputs, kwargs, target_gpus, chunk_sizes, dim=0):\n",
    "    \"\"\"Scatter with support for kwargs dictionary\"\"\"\n",
    "    inputs = scatter(inputs, target_gpus, chunk_sizes, dim) if inputs else []\n",
    "    kwargs = scatter(kwargs, target_gpus, chunk_sizes, dim) if kwargs else []\n",
    "    if len(inputs) < len(kwargs):\n",
    "        inputs.extend([() for _ in range(len(kwargs) - len(inputs))])\n",
    "    elif len(kwargs) < len(inputs):\n",
    "        kwargs.extend([{} for _ in range(len(inputs) - len(kwargs))])\n",
    "    inputs = tuple(inputs)\n",
    "    kwargs = tuple(kwargs)\n",
    "    return inputs, kwargs\n",
    "\n",
    "class BalancedDataParallel(DataParallel):\n",
    "    def __init__(self, gpu0_bsz, *args, **kwargs):\n",
    "        self.gpu0_bsz = gpu0_bsz\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, *inputs, **kwargs):\n",
    "        if not self.device_ids:\n",
    "            return self.module(*inputs, **kwargs)\n",
    "        if self.gpu0_bsz == 0:\n",
    "            device_ids = self.device_ids[1:]\n",
    "        else:\n",
    "            device_ids = self.device_ids\n",
    "        inputs, kwargs = self.scatter(inputs, kwargs, device_ids)\n",
    "        if len(self.device_ids) == 1:\n",
    "            return self.module(*inputs[0], **kwargs[0])\n",
    "        replicas = self.replicate(self.module, self.device_ids)\n",
    "        if self.gpu0_bsz == 0:\n",
    "            replicas = replicas[1:]\n",
    "        outputs = self.parallel_apply(replicas, device_ids, inputs, kwargs)\n",
    "        return self.gather(outputs, self.output_device)\n",
    "\n",
    "    def parallel_apply(self, replicas, device_ids, inputs, kwargs):\n",
    "        return parallel_apply(replicas, inputs, kwargs, device_ids)\n",
    "\n",
    "    def scatter(self, inputs, kwargs, device_ids):\n",
    "        bsz = inputs[0].size(self.dim)\n",
    "        num_dev = len(self.device_ids)\n",
    "        gpu0_bsz = self.gpu0_bsz\n",
    "        bsz_unit = (bsz - gpu0_bsz) // (num_dev - 1)\n",
    "        if gpu0_bsz < bsz_unit:\n",
    "            chunk_sizes = [gpu0_bsz] + [bsz_unit] * (num_dev - 1)\n",
    "            delta = bsz - sum(chunk_sizes)\n",
    "            for i in range(delta):\n",
    "                chunk_sizes[i + 1] += 1\n",
    "            if gpu0_bsz == 0:\n",
    "                chunk_sizes = chunk_sizes[1:]\n",
    "        else:\n",
    "            return super().scatter(inputs, kwargs, device_ids)\n",
    "        return scatter_kwargs(inputs, kwargs, device_ids, chunk_sizes, dim=self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b56f4-41f6-43a4-8beb-144e1d207ece",
   "metadata": {},
   "source": [
    "# 四、==== data_processor.py ====== 构建词表, 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "28cf9521-a9c2-4b87-8189-722a8f58d01f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_vocab_set count: 26\n",
      "zh_vocab_set count: 20\n"
     ]
    }
   ],
   "source": [
    "####################### data_precessor.py ############################\n",
    "######################################################################\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 生成词表\n",
    "def generate_vocab():\n",
    "    # 中文特殊词  [屏蔽词，不知道的词，开头，结尾]\n",
    "    en_vocab_set = ['<pad>','<unk>','<sos>','<eos>']\n",
    "    # 英文特殊词  [屏蔽词，不知道的词，开头，结尾]\n",
    "    zh_vocab_set = ['<pad>','<unk>','<sos>','<eos>']\n",
    "\n",
    "    en_vocab_list = []\n",
    "    zh_vocab_list = []\n",
    "\n",
    "    # 解析训练数据\n",
    "    with open(TRAIN_PATH, encoding='utf-8') as file:\n",
    "        lines = json.loads(file.read())\n",
    "        \n",
    "        for en_sentence, zh_sentence in lines:\n",
    "            # 收集英文分词后都数据\n",
    "            en_vocab_list += divided_en(en_sentence)\n",
    "            # 收集中文分词后都数据\n",
    "            zh_vocab_list += divided_zh(zh_sentence)\n",
    "    \n",
    "    \n",
    "    # 词表去重\n",
    "    # zh_vocab_list中的词去重排序  [('词1',5),('词2',3)]\n",
    "    zh_vocab_sort_kv = Counter(zh_vocab_list).most_common()\n",
    "    # 去重后的词表  ['词1', '词2', '词3']\n",
    "    zh_vocab_set += [k.lower() for k,v in zh_vocab_sort_kv]\n",
    "    \n",
    "    en_vocab_sort_kv = Counter(en_vocab_list).most_common()\n",
    "    # 去重后的词表  ['词1', '词2', '词3']\n",
    "    en_vocab_set += [k.lower() for k,v in en_vocab_sort_kv]\n",
    "    \n",
    "    print('en_vocab_set count:',len(en_vocab_set))\n",
    "    print('zh_vocab_set count:',len(zh_vocab_set))\n",
    "\n",
    "    \n",
    "    # Python join() 方法用于将序列中的元素以指定的字符连接生成一个新的字符串。\n",
    "    # 生成的词表写到文件中\n",
    "    with open(EN_VOCAB_PATH, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(en_vocab_set))\n",
    "        \n",
    "    with open(ZH_VOCAB_PATH, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(zh_vocab_set))\n",
    "    \n",
    "    \n",
    "# 生成词表    \n",
    "if __name__ == '__main__':\n",
    "    generate_vocab()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a165d-ea49-4662-a460-7c7843fac69f",
   "metadata": {},
   "source": [
    "# 5 ======== data_loader.py ======== 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2edab71f-11df-4785-bbdf-666a288e7bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################### data_loader.py ####################\n",
    "#######################################################\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, type='train'):\n",
    "        super().__init__()\n",
    "        if type == 'train':\n",
    "            file_path = TRAIN_PATH\n",
    "        elif type == 'val':\n",
    "            file_path = VAL_PATH\n",
    "            \n",
    "        with open(file_path, encoding='utf-8') as file:\n",
    "            self.lines = json.loads(file.read())\n",
    "            \n",
    "            # 词表\n",
    "            _, self.en_vocab2id = get_vocab('en')\n",
    "            _, self.zh_vocab2id = get_vocab('zh')\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "            \n",
    "    \n",
    "    # 取第 index 单条样本\n",
    "    def __getitem__(self, index):\n",
    "        en_text, zh_text = self.lines[index]\n",
    "            \n",
    "        # 取出的句子进行分词，并转成索引表示\n",
    "        source = [self.en_vocab2id.get(word.lower(), UNK_ID) for word in divided_en(en_text)]\n",
    "        target = [self.zh_vocab2id.get(word.lower(), UNK_ID) for word in divided_zh(zh_text)]\n",
    "        \n",
    "        return source, target, zh_text\n",
    "\n",
    "    \n",
    "    # 数据对齐和整理\n",
    "    # batch是 Dataset 中返回的样本数据，有三个 source, target, zh_text\n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        batch_source, batch_target, batch_tgt_text = zip(*batch)\n",
    "    \n",
    "        src_x = pad_sequence(sequences = [torch.LongTensor([SOS_ID]+src+[EOS_ID]) for src in batch_source],\n",
    "                          batch_first = True,\n",
    "                          padding_value = PAD_ID)\n",
    "        \n",
    "        # 批量句子掩码\n",
    "        src_mask = get_padding_mask(src_x, padding_idx=PAD_ID)\n",
    "    \n",
    "       \n",
    "        tgt_full = pad_sequence(sequences = [torch.LongTensor([SOS_ID]+src+[EOS_ID]) for src in batch_target],\n",
    "                          batch_first = True,\n",
    "                          padding_value = PAD_ID)\n",
    "        # 目标输入值 不包括句子的最后一个字符\n",
    "        tgt_x = tgt_full[:, :-1]\n",
    "        # 目标预测值 不包括句子的第一个字符\n",
    "        tgt_y = tgt_full[:, 1:]\n",
    "        \n",
    "        tgt_pad_mask = get_padding_mask(tgt_x, PAD_ID)\n",
    "        tgt_subsqueent_mask = get_subsequent_mask(words_len=tgt_x.size(-1))\n",
    "        tgt_mask = tgt_pad_mask | tgt_subsqueent_mask\n",
    "        \n",
    "        return src_x, src_mask, tgt_x, tgt_mask, tgt_y, batch_tgt_text \n",
    "    \n",
    "\n",
    "\n",
    "# train_dataset = Dataset('train')\n",
    "# train_loader = data.DataLoader(train_dataset, collate_fn=train_dataset.collate_fn, batch_size=BATCH_SIZE)\n",
    "\n",
    "# val_dataset = Dataset('val')\n",
    "# val_loader = data.DataLoader(val_dataset, collate_fn=val_dataset.collate_fn, batch_size=BATCH_SIZE)\n",
    "     \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     dataset = Dataset('val')    \n",
    "    \n",
    "#     # 加上collate_fn函数后，loader的返回值就是这个函数的返回值了\n",
    "#     loader = data.DataLoader(dataset, collate_fn=dataset.collate_fn, batch_size=2)\n",
    "#     print(next(iter(loader)))\n",
    "    \n",
    "\n",
    "#     exit()\n",
    "#     # 解包操作\n",
    "#     batch = [\n",
    "#         [[1,2,3], ['a','b','c'], ['v','vb','vc']],\n",
    "#         [[4,5,6], ['d','e','f'], ['v','vb','vc']],\n",
    "#     ]\n",
    "#     a,b,c = zip(*batch) # 解包操作\n",
    "#     print(a)\n",
    "#     print(b)\n",
    "#     print(c)\n",
    "    \n",
    "#     print('*'*60)\n",
    "    \n",
    "#     # 等长填充\n",
    "#     batch_src = [\n",
    "#         [4,5,6],\n",
    "#         [6,7]\n",
    "#     ]\n",
    "\n",
    "#     # 句子等长填充，比如批量获取两个句子，每个句子的长度不一样，这里可以把短的用0填充\n",
    "#     src_pad = pad_sequence(sequences = [torch.LongTensor(src) for src in batch_src],\n",
    "#                           batch_first = True,\n",
    "#                           padding_value = 0)\n",
    "#     print(src_pad)\n",
    "    \n",
    "#     # 句子前后加上开头和末尾标识\n",
    "#     src_pad = pad_sequence(sequences = [torch.LongTensor([SOS_ID]+src+[EOS_ID]) for src in batch_src],\n",
    "#                           batch_first = True,\n",
    "#                           padding_value = 0)\n",
    "    \n",
    "#     print(src_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96066919-464d-473e-b935-10f0235ae623",
   "metadata": {},
   "source": [
    "# 6. ===== train.py ===== 训练文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "9a3b1791-ad06-4867-b58b-3da499d27493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数量:  90932\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 0 ; train_loss: 0.6513150930404663 ; val_loss: 1.3156877756118774 val_bleu_score: 73.18\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 1 ; train_loss: 0.6416800022125244 ; val_loss: 1.316165566444397 val_bleu_score: 73.18\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 2 ; train_loss: 0.6294446587562561 ; val_loss: 1.3182246685028076 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 3 ; train_loss: 0.6306788325309753 ; val_loss: 1.3189828395843506 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 4 ; train_loss: 0.6278448700904846 ; val_loss: 1.3197681903839111 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 5 ; train_loss: 0.6275219917297363 ; val_loss: 1.3200998306274414 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 6 ; train_loss: 0.6292347311973572 ; val_loss: 1.3200722932815552 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 7 ; train_loss: 0.6214706897735596 ; val_loss: 1.3194302320480347 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 8 ; train_loss: 0.627885639667511 ; val_loss: 1.318920373916626 val_bleu_score: 19.42\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 9 ; train_loss: 0.6348007917404175 ; val_loss: 1.3183839321136475 val_bleu_score: 19.42\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 10 ; train_loss: 0.6288576126098633 ; val_loss: 1.3180779218673706 val_bleu_score: 19.42\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 11 ; train_loss: 0.630075991153717 ; val_loss: 1.3175790309906006 val_bleu_score: 19.42\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 12 ; train_loss: 0.6284704208374023 ; val_loss: 1.3170266151428223 val_bleu_score: 19.42\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 13 ; train_loss: 0.622518002986908 ; val_loss: 1.3163518905639648 val_bleu_score: 19.42\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 14 ; train_loss: 0.6273171901702881 ; val_loss: 1.3155757188796997 val_bleu_score: 19.42\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 15 ; train_loss: 0.6202929019927979 ; val_loss: 1.3151415586471558 val_bleu_score: 19.42\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 16 ; train_loss: 0.6204012632369995 ; val_loss: 1.3144683837890625 val_bleu_score: 19.42\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 17 ; train_loss: 0.623026430606842 ; val_loss: 1.314125418663025 val_bleu_score: 19.42\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 18 ; train_loss: 0.6242157816886902 ; val_loss: 1.3141260147094727 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 19 ; train_loss: 0.6193196773529053 ; val_loss: 1.313736915588379 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 20 ; train_loss: 0.614749550819397 ; val_loss: 1.3133270740509033 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 21 ; train_loss: 0.6219642758369446 ; val_loss: 1.3132703304290771 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 22 ; train_loss: 0.6220229268074036 ; val_loss: 1.3128936290740967 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 23 ; train_loss: 0.6164356470108032 ; val_loss: 1.312363624572754 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 24 ; train_loss: 0.6237744688987732 ; val_loss: 1.3120713233947754 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 25 ; train_loss: 0.6196194887161255 ; val_loss: 1.3119316101074219 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 26 ; train_loss: 0.6202461123466492 ; val_loss: 1.3120019435882568 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 27 ; train_loss: 0.6229760646820068 ; val_loss: 1.31246018409729 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 28 ; train_loss: 0.6216526627540588 ; val_loss: 1.3132905960083008 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 29 ; train_loss: 0.61899733543396 ; val_loss: 1.313941478729248 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 30 ; train_loss: 0.6192647218704224 ; val_loss: 1.315117597579956 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 31 ; train_loss: 0.6184870004653931 ; val_loss: 1.3163747787475586 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 32 ; train_loss: 0.6182245016098022 ; val_loss: 1.31766939163208 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 33 ; train_loss: 0.6188652515411377 ; val_loss: 1.3188419342041016 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 34 ; train_loss: 0.6188142895698547 ; val_loss: 1.32020902633667 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 35 ; train_loss: 0.618297815322876 ; val_loss: 1.321397304534912 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 36 ; train_loss: 0.6180446743965149 ; val_loss: 1.3224599361419678 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 37 ; train_loss: 0.6148710250854492 ; val_loss: 1.3233473300933838 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 38 ; train_loss: 0.6156210899353027 ; val_loss: 1.3243048191070557 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 39 ; train_loss: 0.6210317611694336 ; val_loss: 1.325300693511963 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 40 ; train_loss: 0.6182148456573486 ; val_loss: 1.3262560367584229 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 41 ; train_loss: 0.6167190670967102 ; val_loss: 1.3272550106048584 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 42 ; train_loss: 0.6104156970977783 ; val_loss: 1.3276822566986084 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 43 ; train_loss: 0.6180623769760132 ; val_loss: 1.3281805515289307 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 44 ; train_loss: 0.6173948049545288 ; val_loss: 1.328406810760498 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 45 ; train_loss: 0.6159911155700684 ; val_loss: 1.3287575244903564 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 46 ; train_loss: 0.6184740662574768 ; val_loss: 1.3287866115570068 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 47 ; train_loss: 0.6110113859176636 ; val_loss: 1.3286802768707275 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 48 ; train_loss: 0.6170842051506042 ; val_loss: 1.3285926580429077 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 49 ; train_loss: 0.612677812576294 ; val_loss: 1.3282660245895386 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 50 ; train_loss: 0.6147201657295227 ; val_loss: 1.328216791152954 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 51 ; train_loss: 0.6172004342079163 ; val_loss: 1.3282554149627686 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 52 ; train_loss: 0.6142585873603821 ; val_loss: 1.3285646438598633 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 53 ; train_loss: 0.6142937541007996 ; val_loss: 1.3291680812835693 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 54 ; train_loss: 0.6149662733078003 ; val_loss: 1.330170750617981 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 55 ; train_loss: 0.6111867427825928 ; val_loss: 1.3311429023742676 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 56 ; train_loss: 0.61229008436203 ; val_loss: 1.3321077823638916 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 57 ; train_loss: 0.6120935678482056 ; val_loss: 1.3328238725662231 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 58 ; train_loss: 0.6126118302345276 ; val_loss: 1.3331143856048584 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 59 ; train_loss: 0.6132078170776367 ; val_loss: 1.33359956741333 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 60 ; train_loss: 0.6141991019248962 ; val_loss: 1.3340206146240234 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 61 ; train_loss: 0.6128454208374023 ; val_loss: 1.3343549966812134 val_bleu_score: 44.61\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 62 ; train_loss: 0.6103538274765015 ; val_loss: 1.3345441818237305 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 63 ; train_loss: 0.610239565372467 ; val_loss: 1.3346033096313477 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 64 ; train_loss: 0.6141009330749512 ; val_loss: 1.3346275091171265 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 65 ; train_loss: 0.6121159195899963 ; val_loss: 1.3346807956695557 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 66 ; train_loss: 0.6131252646446228 ; val_loss: 1.3347623348236084 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 67 ; train_loss: 0.6103982925415039 ; val_loss: 1.3348475694656372 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 68 ; train_loss: 0.6133648157119751 ; val_loss: 1.3350920677185059 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 69 ; train_loss: 0.6104439496994019 ; val_loss: 1.3353018760681152 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 70 ; train_loss: 0.6103514432907104 ; val_loss: 1.3355727195739746 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 71 ; train_loss: 0.6132461428642273 ; val_loss: 1.335557460784912 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 72 ; train_loss: 0.6101111769676208 ; val_loss: 1.33562433719635 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 73 ; train_loss: 0.6095142364501953 ; val_loss: 1.3357093334197998 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 74 ; train_loss: 0.6110969185829163 ; val_loss: 1.3359681367874146 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 75 ; train_loss: 0.6101144552230835 ; val_loss: 1.3359341621398926 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 76 ; train_loss: 0.6107181906700134 ; val_loss: 1.335860252380371 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 77 ; train_loss: 0.6086744070053101 ; val_loss: 1.3355128765106201 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 78 ; train_loss: 0.60854572057724 ; val_loss: 1.3349673748016357 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 79 ; train_loss: 0.6136084198951721 ; val_loss: 1.3345861434936523 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 80 ; train_loss: 0.61005699634552 ; val_loss: 1.33418869972229 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 81 ; train_loss: 0.6103852391242981 ; val_loss: 1.3337702751159668 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 82 ; train_loss: 0.6104168891906738 ; val_loss: 1.3334871530532837 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 83 ; train_loss: 0.6078708171844482 ; val_loss: 1.3331981897354126 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 84 ; train_loss: 0.6121559739112854 ; val_loss: 1.3329262733459473 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 85 ; train_loss: 0.6094124913215637 ; val_loss: 1.3327012062072754 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 86 ; train_loss: 0.6072860360145569 ; val_loss: 1.3326646089553833 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 87 ; train_loss: 0.609335720539093 ; val_loss: 1.3326983451843262 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 88 ; train_loss: 0.6081067323684692 ; val_loss: 1.3326599597930908 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 89 ; train_loss: 0.6106683611869812 ; val_loss: 1.3325456380844116 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 90 ; train_loss: 0.6095864772796631 ; val_loss: 1.3323848247528076 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 91 ; train_loss: 0.6091271638870239 ; val_loss: 1.332265853881836 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 92 ; train_loss: 0.6107757091522217 ; val_loss: 1.3323478698730469 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 93 ; train_loss: 0.6077354550361633 ; val_loss: 1.3323026895523071 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 94 ; train_loss: 0.6086711287498474 ; val_loss: 1.3324156999588013 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 95 ; train_loss: 0.6076508164405823 ; val_loss: 1.3324768543243408 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 96 ; train_loss: 0.6102774739265442 ; val_loss: 1.3326759338378906 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 97 ; train_loss: 0.6084932088851929 ; val_loss: 1.3327854871749878 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 98 ; train_loss: 0.609145998954773 ; val_loss: 1.332803726196289 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 99 ; train_loss: 0.6088117361068726 ; val_loss: 1.3333077430725098 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 100 ; train_loss: 0.610021710395813 ; val_loss: 1.3337723016738892 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 101 ; train_loss: 0.6081242561340332 ; val_loss: 1.3342289924621582 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 102 ; train_loss: 0.6041814088821411 ; val_loss: 1.3344964981079102 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 103 ; train_loss: 0.6045222878456116 ; val_loss: 1.3346381187438965 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 104 ; train_loss: 0.6089399456977844 ; val_loss: 1.3346264362335205 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 105 ; train_loss: 0.6082701683044434 ; val_loss: 1.3346641063690186 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 106 ; train_loss: 0.6075369119644165 ; val_loss: 1.3345696926116943 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 107 ; train_loss: 0.6094556450843811 ; val_loss: 1.334442138671875 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 108 ; train_loss: 0.6050841808319092 ; val_loss: 1.3342502117156982 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 109 ; train_loss: 0.604768693447113 ; val_loss: 1.334021806716919 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 110 ; train_loss: 0.6093233823776245 ; val_loss: 1.3339520692825317 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 111 ; train_loss: 0.6075310111045837 ; val_loss: 1.3337736129760742 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 112 ; train_loss: 0.6069244146347046 ; val_loss: 1.333600401878357 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 113 ; train_loss: 0.6048081517219543 ; val_loss: 1.333503246307373 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 114 ; train_loss: 0.6044421792030334 ; val_loss: 1.3336535692214966 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 115 ; train_loss: 0.606017529964447 ; val_loss: 1.3337706327438354 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 116 ; train_loss: 0.6046896576881409 ; val_loss: 1.3338727951049805 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 117 ; train_loss: 0.60796719789505 ; val_loss: 1.3338721990585327 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 118 ; train_loss: 0.609140932559967 ; val_loss: 1.3340561389923096 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 119 ; train_loss: 0.6055843234062195 ; val_loss: 1.3341346979141235 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 120 ; train_loss: 0.6074914932250977 ; val_loss: 1.334329605102539 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 121 ; train_loss: 0.6053515672683716 ; val_loss: 1.3344779014587402 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 122 ; train_loss: 0.6039464473724365 ; val_loss: 1.3346447944641113 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 123 ; train_loss: 0.6070343255996704 ; val_loss: 1.334934115409851 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 124 ; train_loss: 0.6029945015907288 ; val_loss: 1.3352200984954834 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 125 ; train_loss: 0.6078961491584778 ; val_loss: 1.335602045059204 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 126 ; train_loss: 0.6069267988204956 ; val_loss: 1.3359044790267944 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 127 ; train_loss: 0.6057871580123901 ; val_loss: 1.3360440731048584 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 128 ; train_loss: 0.6047070622444153 ; val_loss: 1.336120367050171 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 129 ; train_loss: 0.6049678325653076 ; val_loss: 1.336106538772583 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 130 ; train_loss: 0.6080369353294373 ; val_loss: 1.3360381126403809 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 131 ; train_loss: 0.6056202054023743 ; val_loss: 1.336022138595581 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 132 ; train_loss: 0.6056893467903137 ; val_loss: 1.3359181880950928 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 133 ; train_loss: 0.6053363084793091 ; val_loss: 1.3358664512634277 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 134 ; train_loss: 0.6054607033729553 ; val_loss: 1.3358242511749268 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 135 ; train_loss: 0.6043131947517395 ; val_loss: 1.3356988430023193 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 136 ; train_loss: 0.6068587899208069 ; val_loss: 1.3356651067733765 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 137 ; train_loss: 0.6050643920898438 ; val_loss: 1.3355424404144287 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 138 ; train_loss: 0.6042449474334717 ; val_loss: 1.3353554010391235 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 139 ; train_loss: 0.604831874370575 ; val_loss: 1.3351292610168457 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 140 ; train_loss: 0.6061094999313354 ; val_loss: 1.3351770639419556 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 141 ; train_loss: 0.6048106551170349 ; val_loss: 1.335211992263794 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 142 ; train_loss: 0.6038490533828735 ; val_loss: 1.3352675437927246 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 143 ; train_loss: 0.6062937378883362 ; val_loss: 1.3353220224380493 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 144 ; train_loss: 0.6042459607124329 ; val_loss: 1.3354194164276123 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 145 ; train_loss: 0.6048001050949097 ; val_loss: 1.3355841636657715 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 146 ; train_loss: 0.6032059192657471 ; val_loss: 1.3357343673706055 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 147 ; train_loss: 0.6062313914299011 ; val_loss: 1.3359992504119873 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 148 ; train_loss: 0.6044989228248596 ; val_loss: 1.3361214399337769 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 149 ; train_loss: 0.6050665378570557 ; val_loss: 1.3364057540893555 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 150 ; train_loss: 0.602472722530365 ; val_loss: 1.3364346027374268 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 151 ; train_loss: 0.6062437891960144 ; val_loss: 1.3364009857177734 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 152 ; train_loss: 0.6045069098472595 ; val_loss: 1.3362312316894531 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 153 ; train_loss: 0.6033712029457092 ; val_loss: 1.3359339237213135 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 154 ; train_loss: 0.6032516956329346 ; val_loss: 1.335667371749878 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 155 ; train_loss: 0.6035260558128357 ; val_loss: 1.335390567779541 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 156 ; train_loss: 0.6057581305503845 ; val_loss: 1.3352022171020508 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 157 ; train_loss: 0.6036186814308167 ; val_loss: 1.3350017070770264 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 158 ; train_loss: 0.605656087398529 ; val_loss: 1.3349714279174805 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 159 ; train_loss: 0.6051183342933655 ; val_loss: 1.3348673582077026 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 160 ; train_loss: 0.6054465770721436 ; val_loss: 1.3348629474639893 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 161 ; train_loss: 0.6050167083740234 ; val_loss: 1.3349008560180664 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 162 ; train_loss: 0.6035474538803101 ; val_loss: 1.3350002765655518 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 163 ; train_loss: 0.6034208536148071 ; val_loss: 1.3350555896759033 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 164 ; train_loss: 0.6042901873588562 ; val_loss: 1.3351562023162842 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 165 ; train_loss: 0.6042934060096741 ; val_loss: 1.3353837728500366 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 166 ; train_loss: 0.6030380725860596 ; val_loss: 1.335737943649292 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 167 ; train_loss: 0.6024154424667358 ; val_loss: 1.3359816074371338 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 168 ; train_loss: 0.6034152507781982 ; val_loss: 1.3363150358200073 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 169 ; train_loss: 0.6021839380264282 ; val_loss: 1.336507797241211 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 170 ; train_loss: 0.6029701828956604 ; val_loss: 1.3365778923034668 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 171 ; train_loss: 0.6025307178497314 ; val_loss: 1.336555004119873 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 172 ; train_loss: 0.6023796796798706 ; val_loss: 1.3364601135253906 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 173 ; train_loss: 0.6018478274345398 ; val_loss: 1.3363111019134521 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 174 ; train_loss: 0.6052339673042297 ; val_loss: 1.3362135887145996 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 175 ; train_loss: 0.6031433939933777 ; val_loss: 1.336087703704834 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 176 ; train_loss: 0.6026477813720703 ; val_loss: 1.3359265327453613 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 177 ; train_loss: 0.6037095189094543 ; val_loss: 1.335741400718689 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 178 ; train_loss: 0.6042185425758362 ; val_loss: 1.3356443643569946 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 179 ; train_loss: 0.6011639833450317 ; val_loss: 1.3354486227035522 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 180 ; train_loss: 0.6033272743225098 ; val_loss: 1.3352746963500977 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 181 ; train_loss: 0.6016601920127869 ; val_loss: 1.3350245952606201 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 182 ; train_loss: 0.604033350944519 ; val_loss: 1.334853172302246 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 183 ; train_loss: 0.6030282974243164 ; val_loss: 1.3347002267837524 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 184 ; train_loss: 0.6027001738548279 ; val_loss: 1.3345509767532349 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 185 ; train_loss: 0.6033053994178772 ; val_loss: 1.3344699144363403 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 186 ; train_loss: 0.6026134490966797 ; val_loss: 1.334470272064209 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 187 ; train_loss: 0.6014466285705566 ; val_loss: 1.334472417831421 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 188 ; train_loss: 0.6025380492210388 ; val_loss: 1.3345128297805786 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 189 ; train_loss: 0.6021233797073364 ; val_loss: 1.3345564603805542 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 190 ; train_loss: 0.6024183034896851 ; val_loss: 1.3345882892608643 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 191 ; train_loss: 0.6026384830474854 ; val_loss: 1.3347117900848389 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 192 ; train_loss: 0.601084291934967 ; val_loss: 1.3348982334136963 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 193 ; train_loss: 0.6022781133651733 ; val_loss: 1.3350529670715332 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 194 ; train_loss: 0.6022395491600037 ; val_loss: 1.3353108167648315 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 195 ; train_loss: 0.600775420665741 ; val_loss: 1.3355565071105957 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 196 ; train_loss: 0.6021957993507385 ; val_loss: 1.3358519077301025 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 197 ; train_loss: 0.602860689163208 ; val_loss: 1.3361554145812988 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 198 ; train_loss: 0.6012036204338074 ; val_loss: 1.3364791870117188 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 199 ; train_loss: 0.6020299792289734 ; val_loss: 1.336739420890808 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 200 ; train_loss: 0.6011050343513489 ; val_loss: 1.3370029926300049 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 201 ; train_loss: 0.6005193591117859 ; val_loss: 1.3370895385742188 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 202 ; train_loss: 0.6021296381950378 ; val_loss: 1.337172269821167 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 203 ; train_loss: 0.6006973385810852 ; val_loss: 1.3372540473937988 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 204 ; train_loss: 0.6016057729721069 ; val_loss: 1.3372368812561035 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 205 ; train_loss: 0.6017282605171204 ; val_loss: 1.337254524230957 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 206 ; train_loss: 0.5998404026031494 ; val_loss: 1.3372271060943604 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 207 ; train_loss: 0.6003355979919434 ; val_loss: 1.3371381759643555 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 208 ; train_loss: 0.6015213131904602 ; val_loss: 1.3369715213775635 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 209 ; train_loss: 0.6019281148910522 ; val_loss: 1.3367586135864258 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 210 ; train_loss: 0.601235568523407 ; val_loss: 1.3366516828536987 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 211 ; train_loss: 0.5997352600097656 ; val_loss: 1.3363564014434814 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 212 ; train_loss: 0.6017569303512573 ; val_loss: 1.3361451625823975 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 213 ; train_loss: 0.6026374697685242 ; val_loss: 1.335955023765564 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 214 ; train_loss: 0.6029210090637207 ; val_loss: 1.3358745574951172 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 215 ; train_loss: 0.6014870405197144 ; val_loss: 1.3358744382858276 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 216 ; train_loss: 0.5996181964874268 ; val_loss: 1.335658311843872 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 217 ; train_loss: 0.6007867455482483 ; val_loss: 1.3354015350341797 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 218 ; train_loss: 0.6000491976737976 ; val_loss: 1.3350601196289062 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 219 ; train_loss: 0.6016379594802856 ; val_loss: 1.3347339630126953 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 220 ; train_loss: 0.5997239947319031 ; val_loss: 1.3342922925949097 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 221 ; train_loss: 0.6020957231521606 ; val_loss: 1.333857774734497 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 222 ; train_loss: 0.6005872488021851 ; val_loss: 1.3334221839904785 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 223 ; train_loss: 0.6005975008010864 ; val_loss: 1.3330109119415283 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 224 ; train_loss: 0.5999859571456909 ; val_loss: 1.3324881792068481 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 225 ; train_loss: 0.5996647477149963 ; val_loss: 1.3319029808044434 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 226 ; train_loss: 0.6006388664245605 ; val_loss: 1.3312976360321045 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 227 ; train_loss: 0.6016404628753662 ; val_loss: 1.330782175064087 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 228 ; train_loss: 0.6005902886390686 ; val_loss: 1.330256700515747 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 229 ; train_loss: 0.6009160280227661 ; val_loss: 1.3297531604766846 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 230 ; train_loss: 0.6004922389984131 ; val_loss: 1.3293367624282837 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 231 ; train_loss: 0.6013903617858887 ; val_loss: 1.3290208578109741 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 232 ; train_loss: 0.5999096632003784 ; val_loss: 1.3287297487258911 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 233 ; train_loss: 0.5993398427963257 ; val_loss: 1.3285282850265503 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 234 ; train_loss: 0.6005975008010864 ; val_loss: 1.3283995389938354 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 235 ; train_loss: 0.599693238735199 ; val_loss: 1.3282427787780762 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 236 ; train_loss: 0.5999205112457275 ; val_loss: 1.3280057907104492 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 237 ; train_loss: 0.6006487607955933 ; val_loss: 1.3277431726455688 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 238 ; train_loss: 0.6006705164909363 ; val_loss: 1.327529788017273 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 239 ; train_loss: 0.599294900894165 ; val_loss: 1.32728111743927 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 240 ; train_loss: 0.6005510687828064 ; val_loss: 1.3270487785339355 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 241 ; train_loss: 0.5999710559844971 ; val_loss: 1.3268260955810547 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 242 ; train_loss: 0.6006715893745422 ; val_loss: 1.326687216758728 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 243 ; train_loss: 0.6007566452026367 ; val_loss: 1.3265411853790283 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 244 ; train_loss: 0.6008515357971191 ; val_loss: 1.326478123664856 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 245 ; train_loss: 0.5996146202087402 ; val_loss: 1.3264291286468506 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 246 ; train_loss: 0.601205050945282 ; val_loss: 1.3264856338500977 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 247 ; train_loss: 0.6011263132095337 ; val_loss: 1.3266277313232422 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 248 ; train_loss: 0.5988436341285706 ; val_loss: 1.326699137687683 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 249 ; train_loss: 0.599785566329956 ; val_loss: 1.326751947402954 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 250 ; train_loss: 0.6003572940826416 ; val_loss: 1.3268053531646729 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 251 ; train_loss: 0.5997684597969055 ; val_loss: 1.3268539905548096 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 252 ; train_loss: 0.5995689630508423 ; val_loss: 1.3269270658493042 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 253 ; train_loss: 0.5997081398963928 ; val_loss: 1.3269753456115723 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 254 ; train_loss: 0.5992385149002075 ; val_loss: 1.3270258903503418 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 255 ; train_loss: 0.59922856092453 ; val_loss: 1.3270184993743896 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 256 ; train_loss: 0.5997776985168457 ; val_loss: 1.3270102739334106 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 257 ; train_loss: 0.5996931195259094 ; val_loss: 1.3270400762557983 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 258 ; train_loss: 0.6002794504165649 ; val_loss: 1.3269777297973633 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 259 ; train_loss: 0.5992423892021179 ; val_loss: 1.3268498182296753 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 260 ; train_loss: 0.6003036499023438 ; val_loss: 1.3267414569854736 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 261 ; train_loss: 0.5999491214752197 ; val_loss: 1.32657790184021 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 262 ; train_loss: 0.5996118783950806 ; val_loss: 1.3264884948730469 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 263 ; train_loss: 0.5990341901779175 ; val_loss: 1.3261798620224 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 264 ; train_loss: 0.5989980697631836 ; val_loss: 1.3256683349609375 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 265 ; train_loss: 0.5986774563789368 ; val_loss: 1.3250395059585571 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 266 ; train_loss: 0.59885573387146 ; val_loss: 1.324589490890503 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 267 ; train_loss: 0.5995919704437256 ; val_loss: 1.3243118524551392 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 268 ; train_loss: 0.599990963935852 ; val_loss: 1.3240540027618408 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 269 ; train_loss: 0.5987652540206909 ; val_loss: 1.3237758874893188 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 270 ; train_loss: 0.5997403860092163 ; val_loss: 1.3235782384872437 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 271 ; train_loss: 0.5990983843803406 ; val_loss: 1.3233457803726196 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 272 ; train_loss: 0.5986254811286926 ; val_loss: 1.323078989982605 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 273 ; train_loss: 0.5987828373908997 ; val_loss: 1.3229008913040161 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 274 ; train_loss: 0.5994594693183899 ; val_loss: 1.3227519989013672 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 275 ; train_loss: 0.5989916920661926 ; val_loss: 1.322709083557129 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 276 ; train_loss: 0.5983291268348694 ; val_loss: 1.322628378868103 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 277 ; train_loss: 0.6001176834106445 ; val_loss: 1.3226593732833862 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 278 ; train_loss: 0.5989022254943848 ; val_loss: 1.322648525238037 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 279 ; train_loss: 0.5993788242340088 ; val_loss: 1.3225922584533691 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 280 ; train_loss: 0.5987153053283691 ; val_loss: 1.3224294185638428 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 281 ; train_loss: 0.5987240076065063 ; val_loss: 1.3222702741622925 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 282 ; train_loss: 0.5981696844100952 ; val_loss: 1.322129726409912 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 283 ; train_loss: 0.5984765291213989 ; val_loss: 1.3219456672668457 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 284 ; train_loss: 0.5989669561386108 ; val_loss: 1.3217939138412476 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 285 ; train_loss: 0.5982807874679565 ; val_loss: 1.321638584136963 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 286 ; train_loss: 0.5990472435951233 ; val_loss: 1.3213536739349365 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 287 ; train_loss: 0.5993870496749878 ; val_loss: 1.3211958408355713 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 288 ; train_loss: 0.5981932282447815 ; val_loss: 1.3210458755493164 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 289 ; train_loss: 0.5989993810653687 ; val_loss: 1.320845603942871 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 290 ; train_loss: 0.5988354682922363 ; val_loss: 1.320666790008545 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 291 ; train_loss: 0.5988017320632935 ; val_loss: 1.3204786777496338 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 292 ; train_loss: 0.5982973575592041 ; val_loss: 1.3203719854354858 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 293 ; train_loss: 0.5982457399368286 ; val_loss: 1.320383906364441 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 294 ; train_loss: 0.5983158349990845 ; val_loss: 1.3203752040863037 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 295 ; train_loss: 0.5978625416755676 ; val_loss: 1.3203325271606445 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 296 ; train_loss: 0.5982339382171631 ; val_loss: 1.3202784061431885 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 297 ; train_loss: 0.5982663631439209 ; val_loss: 1.32027268409729 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 298 ; train_loss: 0.5979915857315063 ; val_loss: 1.3204398155212402 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 299 ; train_loss: 0.5986228585243225 ; val_loss: 1.3204612731933594 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 300 ; train_loss: 0.5980188846588135 ; val_loss: 1.3204272985458374 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 301 ; train_loss: 0.5988497138023376 ; val_loss: 1.3203320503234863 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 302 ; train_loss: 0.598821759223938 ; val_loss: 1.3204386234283447 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 303 ; train_loss: 0.5987273454666138 ; val_loss: 1.3205311298370361 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 304 ; train_loss: 0.5982821583747864 ; val_loss: 1.3205395936965942 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 305 ; train_loss: 0.5979088544845581 ; val_loss: 1.3205249309539795 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 306 ; train_loss: 0.5986489653587341 ; val_loss: 1.3203685283660889 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 307 ; train_loss: 0.598453938961029 ; val_loss: 1.3202073574066162 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 308 ; train_loss: 0.5989719033241272 ; val_loss: 1.319979190826416 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 309 ; train_loss: 0.5985598564147949 ; val_loss: 1.3197782039642334 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 310 ; train_loss: 0.5988694429397583 ; val_loss: 1.3196097612380981 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 311 ; train_loss: 0.5979440808296204 ; val_loss: 1.3193260431289673 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 312 ; train_loss: 0.5984280109405518 ; val_loss: 1.3190268278121948 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 313 ; train_loss: 0.597811222076416 ; val_loss: 1.3187742233276367 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 314 ; train_loss: 0.5980451107025146 ; val_loss: 1.31858491897583 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 315 ; train_loss: 0.5976526737213135 ; val_loss: 1.3183858394622803 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 316 ; train_loss: 0.5980502367019653 ; val_loss: 1.3181815147399902 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 317 ; train_loss: 0.5982447862625122 ; val_loss: 1.3180128335952759 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 318 ; train_loss: 0.5979154109954834 ; val_loss: 1.3178274631500244 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 319 ; train_loss: 0.5974906086921692 ; val_loss: 1.317651391029358 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 320 ; train_loss: 0.5979987382888794 ; val_loss: 1.317477822303772 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 321 ; train_loss: 0.5982421040534973 ; val_loss: 1.31723153591156 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 322 ; train_loss: 0.5978277325630188 ; val_loss: 1.316965937614441 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 323 ; train_loss: 0.5973696708679199 ; val_loss: 1.316755771636963 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 324 ; train_loss: 0.5976972579956055 ; val_loss: 1.3166038990020752 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 325 ; train_loss: 0.5973057150840759 ; val_loss: 1.3164451122283936 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 326 ; train_loss: 0.5973990559577942 ; val_loss: 1.3164666891098022 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 327 ; train_loss: 0.5977590084075928 ; val_loss: 1.3166306018829346 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 328 ; train_loss: 0.597541868686676 ; val_loss: 1.3168092966079712 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 329 ; train_loss: 0.5972790122032166 ; val_loss: 1.3171337842941284 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 330 ; train_loss: 0.5981044173240662 ; val_loss: 1.3175559043884277 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 331 ; train_loss: 0.5979333519935608 ; val_loss: 1.317916989326477 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 332 ; train_loss: 0.5971886515617371 ; val_loss: 1.3180944919586182 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 333 ; train_loss: 0.5975708365440369 ; val_loss: 1.3182998895645142 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 334 ; train_loss: 0.5973803997039795 ; val_loss: 1.3185597658157349 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 335 ; train_loss: 0.5976097583770752 ; val_loss: 1.3187434673309326 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 336 ; train_loss: 0.5971202254295349 ; val_loss: 1.3189657926559448 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 337 ; train_loss: 0.5981283783912659 ; val_loss: 1.3191077709197998 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 338 ; train_loss: 0.5973947644233704 ; val_loss: 1.3192877769470215 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 339 ; train_loss: 0.597565770149231 ; val_loss: 1.3193155527114868 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 340 ; train_loss: 0.5970171093940735 ; val_loss: 1.3193254470825195 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 341 ; train_loss: 0.5972521901130676 ; val_loss: 1.319353699684143 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 342 ; train_loss: 0.5976633429527283 ; val_loss: 1.3193410634994507 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 343 ; train_loss: 0.5968023538589478 ; val_loss: 1.3193845748901367 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 344 ; train_loss: 0.5973925590515137 ; val_loss: 1.3194479942321777 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 345 ; train_loss: 0.597643256187439 ; val_loss: 1.3196107149124146 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 346 ; train_loss: 0.5969685316085815 ; val_loss: 1.3199152946472168 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 347 ; train_loss: 0.5970519185066223 ; val_loss: 1.3201701641082764 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 348 ; train_loss: 0.5975764393806458 ; val_loss: 1.3204247951507568 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 349 ; train_loss: 0.5971279144287109 ; val_loss: 1.3206565380096436 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 350 ; train_loss: 0.5968565940856934 ; val_loss: 1.320655107498169 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 351 ; train_loss: 0.59715735912323 ; val_loss: 1.320420742034912 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 352 ; train_loss: 0.5972156524658203 ; val_loss: 1.320177674293518 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 353 ; train_loss: 0.5970681309700012 ; val_loss: 1.31990385055542 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 354 ; train_loss: 0.5965840220451355 ; val_loss: 1.3196378946304321 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 355 ; train_loss: 0.597495436668396 ; val_loss: 1.3194148540496826 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 356 ; train_loss: 0.5968803763389587 ; val_loss: 1.3191485404968262 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 357 ; train_loss: 0.5967144966125488 ; val_loss: 1.3188508749008179 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 358 ; train_loss: 0.5968588590621948 ; val_loss: 1.3185713291168213 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 359 ; train_loss: 0.5973246097564697 ; val_loss: 1.3183954954147339 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 360 ; train_loss: 0.5968412756919861 ; val_loss: 1.3183144330978394 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 361 ; train_loss: 0.5972138047218323 ; val_loss: 1.3184545040130615 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 362 ; train_loss: 0.5970162153244019 ; val_loss: 1.3186163902282715 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 363 ; train_loss: 0.5961989164352417 ; val_loss: 1.3186509609222412 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 364 ; train_loss: 0.5965321660041809 ; val_loss: 1.3188096284866333 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 365 ; train_loss: 0.5974478125572205 ; val_loss: 1.3188303709030151 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 366 ; train_loss: 0.5966535806655884 ; val_loss: 1.3188450336456299 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 367 ; train_loss: 0.5967954397201538 ; val_loss: 1.3188683986663818 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 368 ; train_loss: 0.5969464182853699 ; val_loss: 1.3188968896865845 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 369 ; train_loss: 0.5968503952026367 ; val_loss: 1.3189507722854614 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 370 ; train_loss: 0.5972374677658081 ; val_loss: 1.3190467357635498 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 371 ; train_loss: 0.5969018340110779 ; val_loss: 1.3191293478012085 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 372 ; train_loss: 0.5971720218658447 ; val_loss: 1.31929612159729 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 373 ; train_loss: 0.596621572971344 ; val_loss: 1.3194485902786255 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 374 ; train_loss: 0.5970088243484497 ; val_loss: 1.3196463584899902 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 375 ; train_loss: 0.5959293246269226 ; val_loss: 1.3198180198669434 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 376 ; train_loss: 0.5962697863578796 ; val_loss: 1.3199529647827148 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 377 ; train_loss: 0.5966894030570984 ; val_loss: 1.320068359375 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 378 ; train_loss: 0.5965142250061035 ; val_loss: 1.320192575454712 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 379 ; train_loss: 0.5966135263442993 ; val_loss: 1.3201420307159424 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 380 ; train_loss: 0.596185564994812 ; val_loss: 1.3200763463974 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 381 ; train_loss: 0.5960701107978821 ; val_loss: 1.3199081420898438 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 382 ; train_loss: 0.5969851016998291 ; val_loss: 1.3196717500686646 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 383 ; train_loss: 0.5960543751716614 ; val_loss: 1.3194708824157715 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 384 ; train_loss: 0.5965138077735901 ; val_loss: 1.3192875385284424 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 385 ; train_loss: 0.5961194038391113 ; val_loss: 1.3191850185394287 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 386 ; train_loss: 0.596362292766571 ; val_loss: 1.3190861940383911 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 387 ; train_loss: 0.5966605544090271 ; val_loss: 1.3190748691558838 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 388 ; train_loss: 0.5965463519096375 ; val_loss: 1.318984031677246 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 389 ; train_loss: 0.5959935188293457 ; val_loss: 1.3189376592636108 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 390 ; train_loss: 0.5962682962417603 ; val_loss: 1.318900465965271 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 391 ; train_loss: 0.5961787104606628 ; val_loss: 1.3188612461090088 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 392 ; train_loss: 0.595879316329956 ; val_loss: 1.3188306093215942 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 393 ; train_loss: 0.5964441895484924 ; val_loss: 1.3187944889068604 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 394 ; train_loss: 0.596576452255249 ; val_loss: 1.3187191486358643 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 395 ; train_loss: 0.5961804389953613 ; val_loss: 1.318551778793335 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 396 ; train_loss: 0.596096932888031 ; val_loss: 1.3183931112289429 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 397 ; train_loss: 0.5957517623901367 ; val_loss: 1.3182623386383057 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 398 ; train_loss: 0.5965604782104492 ; val_loss: 1.3181551694869995 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 399 ; train_loss: 0.5960017442703247 ; val_loss: 1.3179798126220703 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 400 ; train_loss: 0.5960659980773926 ; val_loss: 1.3178514242172241 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 401 ; train_loss: 0.5969744920730591 ; val_loss: 1.3176122903823853 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 402 ; train_loss: 0.5962647199630737 ; val_loss: 1.3174511194229126 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 403 ; train_loss: 0.5960262417793274 ; val_loss: 1.317331075668335 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 404 ; train_loss: 0.5959767699241638 ; val_loss: 1.3172180652618408 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 405 ; train_loss: 0.5962903499603271 ; val_loss: 1.3171569108963013 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 406 ; train_loss: 0.5963284969329834 ; val_loss: 1.3171923160552979 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 407 ; train_loss: 0.5962680578231812 ; val_loss: 1.3170727491378784 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 408 ; train_loss: 0.596432089805603 ; val_loss: 1.3168959617614746 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 409 ; train_loss: 0.5957048535346985 ; val_loss: 1.3166818618774414 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 410 ; train_loss: 0.5963354110717773 ; val_loss: 1.3166542053222656 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 411 ; train_loss: 0.5957236289978027 ; val_loss: 1.3166868686676025 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 412 ; train_loss: 0.5959424376487732 ; val_loss: 1.3166699409484863 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 413 ; train_loss: 0.5959197878837585 ; val_loss: 1.3166781663894653 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 414 ; train_loss: 0.595833957195282 ; val_loss: 1.3166751861572266 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 415 ; train_loss: 0.5967236757278442 ; val_loss: 1.3166481256484985 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 416 ; train_loss: 0.5966363549232483 ; val_loss: 1.3166813850402832 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 417 ; train_loss: 0.5952399969100952 ; val_loss: 1.3166770935058594 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 418 ; train_loss: 0.5967127084732056 ; val_loss: 1.3167202472686768 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 419 ; train_loss: 0.5957319140434265 ; val_loss: 1.316805362701416 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 420 ; train_loss: 0.5960788726806641 ; val_loss: 1.3168928623199463 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 421 ; train_loss: 0.5956694483757019 ; val_loss: 1.3169653415679932 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 422 ; train_loss: 0.5963312983512878 ; val_loss: 1.3170186281204224 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 423 ; train_loss: 0.5965415239334106 ; val_loss: 1.3169713020324707 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 424 ; train_loss: 0.5961270928382874 ; val_loss: 1.316765308380127 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 425 ; train_loss: 0.5958887338638306 ; val_loss: 1.3166093826293945 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 426 ; train_loss: 0.5956143736839294 ; val_loss: 1.316443681716919 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 427 ; train_loss: 0.5957381725311279 ; val_loss: 1.3162455558776855 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 428 ; train_loss: 0.5960179567337036 ; val_loss: 1.3161526918411255 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 429 ; train_loss: 0.5959960222244263 ; val_loss: 1.3161073923110962 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 430 ; train_loss: 0.5962559580802917 ; val_loss: 1.3161330223083496 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 431 ; train_loss: 0.596713125705719 ; val_loss: 1.316239356994629 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 432 ; train_loss: 0.5964764952659607 ; val_loss: 1.3162907361984253 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 433 ; train_loss: 0.5951884984970093 ; val_loss: 1.3161543607711792 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 434 ; train_loss: 0.5963813662528992 ; val_loss: 1.3161303997039795 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 435 ; train_loss: 0.595845639705658 ; val_loss: 1.3160810470581055 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 436 ; train_loss: 0.5962703227996826 ; val_loss: 1.31598699092865 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 437 ; train_loss: 0.5961715579032898 ; val_loss: 1.3158882856369019 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 438 ; train_loss: 0.5962358117103577 ; val_loss: 1.3157286643981934 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 439 ; train_loss: 0.5961285829544067 ; val_loss: 1.3156981468200684 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 440 ; train_loss: 0.5958774089813232 ; val_loss: 1.3157213926315308 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 441 ; train_loss: 0.5955702662467957 ; val_loss: 1.3157144784927368 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 442 ; train_loss: 0.595119833946228 ; val_loss: 1.315910816192627 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 443 ; train_loss: 0.5955895781517029 ; val_loss: 1.3161015510559082 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 444 ; train_loss: 0.595808207988739 ; val_loss: 1.316434383392334 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 445 ; train_loss: 0.5958911180496216 ; val_loss: 1.3167506456375122 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 446 ; train_loss: 0.5956513285636902 ; val_loss: 1.3170158863067627 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 447 ; train_loss: 0.5957076549530029 ; val_loss: 1.317173719406128 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 448 ; train_loss: 0.5961206555366516 ; val_loss: 1.317265510559082 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 449 ; train_loss: 0.5964674949645996 ; val_loss: 1.3173595666885376 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 450 ; train_loss: 0.5964266657829285 ; val_loss: 1.317396879196167 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 451 ; train_loss: 0.595585823059082 ; val_loss: 1.3173882961273193 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 452 ; train_loss: 0.5960566401481628 ; val_loss: 1.3172595500946045 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 453 ; train_loss: 0.5956608057022095 ; val_loss: 1.317192554473877 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 454 ; train_loss: 0.5955337285995483 ; val_loss: 1.3171780109405518 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 455 ; train_loss: 0.5960053205490112 ; val_loss: 1.317118763923645 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 456 ; train_loss: 0.5961332321166992 ; val_loss: 1.3171509504318237 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 457 ; train_loss: 0.5961779356002808 ; val_loss: 1.3172261714935303 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 458 ; train_loss: 0.5953137278556824 ; val_loss: 1.3172922134399414 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 459 ; train_loss: 0.5958138704299927 ; val_loss: 1.3173365592956543 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 460 ; train_loss: 0.5953662991523743 ; val_loss: 1.317229151725769 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 461 ; train_loss: 0.5957094430923462 ; val_loss: 1.3170264959335327 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 462 ; train_loss: 0.5956381559371948 ; val_loss: 1.3169074058532715 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 463 ; train_loss: 0.5957352519035339 ; val_loss: 1.316733956336975 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 464 ; train_loss: 0.5962024927139282 ; val_loss: 1.316718339920044 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 465 ; train_loss: 0.5958379507064819 ; val_loss: 1.316745638847351 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 466 ; train_loss: 0.5953377485275269 ; val_loss: 1.3167967796325684 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 467 ; train_loss: 0.5958734750747681 ; val_loss: 1.3167362213134766 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 468 ; train_loss: 0.5953530073165894 ; val_loss: 1.3166288137435913 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 469 ; train_loss: 0.5959790349006653 ; val_loss: 1.3162355422973633 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 470 ; train_loss: 0.5959713459014893 ; val_loss: 1.3156547546386719 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 471 ; train_loss: 0.5957728624343872 ; val_loss: 1.3149690628051758 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 472 ; train_loss: 0.5960258841514587 ; val_loss: 1.3143401145935059 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 473 ; train_loss: 0.5954297184944153 ; val_loss: 1.313882827758789 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 474 ; train_loss: 0.595715343952179 ; val_loss: 1.3134870529174805 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 475 ; train_loss: 0.59592205286026 ; val_loss: 1.3132599592208862 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 476 ; train_loss: 0.5953100323677063 ; val_loss: 1.3130820989608765 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 477 ; train_loss: 0.5953473448753357 ; val_loss: 1.3129957914352417 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 478 ; train_loss: 0.5957112908363342 ; val_loss: 1.312930941581726 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 479 ; train_loss: 0.595422625541687 ; val_loss: 1.3128886222839355 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 480 ; train_loss: 0.5948694944381714 ; val_loss: 1.3129161596298218 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 481 ; train_loss: 0.595470666885376 ; val_loss: 1.312895655632019 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 482 ; train_loss: 0.5955504179000854 ; val_loss: 1.312989592552185 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 483 ; train_loss: 0.5954355597496033 ; val_loss: 1.3130677938461304 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 484 ; train_loss: 0.5957921743392944 ; val_loss: 1.313094139099121 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 485 ; train_loss: 0.5952476263046265 ; val_loss: 1.3131005764007568 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 486 ; train_loss: 0.5955066680908203 ; val_loss: 1.3131213188171387 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 487 ; train_loss: 0.5957852602005005 ; val_loss: 1.313107967376709 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 488 ; train_loss: 0.595431923866272 ; val_loss: 1.3129335641860962 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 489 ; train_loss: 0.5957061052322388 ; val_loss: 1.3127021789550781 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 490 ; train_loss: 0.5953485369682312 ; val_loss: 1.3123410940170288 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 491 ; train_loss: 0.5949746370315552 ; val_loss: 1.3121708631515503 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 492 ; train_loss: 0.5948763489723206 ; val_loss: 1.312018871307373 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 493 ; train_loss: 0.5955050587654114 ; val_loss: 1.3122034072875977 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 494 ; train_loss: 0.5954126119613647 ; val_loss: 1.312415361404419 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 495 ; train_loss: 0.5950444340705872 ; val_loss: 1.3126140832901 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 496 ; train_loss: 0.5949467420578003 ; val_loss: 1.3129042387008667 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 497 ; train_loss: 0.5949506759643555 ; val_loss: 1.3131651878356934 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 498 ; train_loss: 0.5950992107391357 ; val_loss: 1.3135236501693726 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 499 ; train_loss: 0.5950978398323059 ; val_loss: 1.3139029741287231 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 500 ; train_loss: 0.5951662063598633 ; val_loss: 1.3143118619918823 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 501 ; train_loss: 0.5952534079551697 ; val_loss: 1.314650535583496 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 502 ; train_loss: 0.5953569412231445 ; val_loss: 1.3148672580718994 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 503 ; train_loss: 0.5953879952430725 ; val_loss: 1.3150124549865723 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 504 ; train_loss: 0.5951187610626221 ; val_loss: 1.3151941299438477 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 505 ; train_loss: 0.594963014125824 ; val_loss: 1.315354585647583 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 506 ; train_loss: 0.5950539708137512 ; val_loss: 1.3155505657196045 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 507 ; train_loss: 0.5950995087623596 ; val_loss: 1.3157799243927002 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 508 ; train_loss: 0.5953491926193237 ; val_loss: 1.3160130977630615 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 509 ; train_loss: 0.59507155418396 ; val_loss: 1.316250205039978 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 510 ; train_loss: 0.5950988531112671 ; val_loss: 1.3162307739257812 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 511 ; train_loss: 0.5950503349304199 ; val_loss: 1.3161349296569824 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 512 ; train_loss: 0.5954331159591675 ; val_loss: 1.3160996437072754 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 513 ; train_loss: 0.5949064493179321 ; val_loss: 1.3159657716751099 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 514 ; train_loss: 0.5956164598464966 ; val_loss: 1.3160911798477173 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 515 ; train_loss: 0.5951438546180725 ; val_loss: 1.3161635398864746 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 516 ; train_loss: 0.5952227115631104 ; val_loss: 1.316132664680481 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 517 ; train_loss: 0.5954549908638 ; val_loss: 1.315983533859253 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 518 ; train_loss: 0.5953835248947144 ; val_loss: 1.315641164779663 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 519 ; train_loss: 0.5955750346183777 ; val_loss: 1.315352439880371 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 520 ; train_loss: 0.5951762199401855 ; val_loss: 1.3150806427001953 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 521 ; train_loss: 0.5952867269515991 ; val_loss: 1.3148536682128906 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 522 ; train_loss: 0.5953928828239441 ; val_loss: 1.3148866891860962 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 523 ; train_loss: 0.5955016613006592 ; val_loss: 1.31509268283844 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 524 ; train_loss: 0.5949591398239136 ; val_loss: 1.3155138492584229 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 525 ; train_loss: 0.5952615737915039 ; val_loss: 1.3159434795379639 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 526 ; train_loss: 0.5952317714691162 ; val_loss: 1.3163034915924072 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 527 ; train_loss: 0.594936728477478 ; val_loss: 1.316521167755127 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 528 ; train_loss: 0.5954431295394897 ; val_loss: 1.3165249824523926 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 529 ; train_loss: 0.5948531627655029 ; val_loss: 1.3165500164031982 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 530 ; train_loss: 0.5951015949249268 ; val_loss: 1.3166172504425049 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 531 ; train_loss: 0.5952340960502625 ; val_loss: 1.3164032697677612 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 532 ; train_loss: 0.5952573418617249 ; val_loss: 1.3161392211914062 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 533 ; train_loss: 0.5949376821517944 ; val_loss: 1.3157732486724854 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 534 ; train_loss: 0.5950837135314941 ; val_loss: 1.3151609897613525 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 535 ; train_loss: 0.594720184803009 ; val_loss: 1.3145318031311035 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 536 ; train_loss: 0.5950607061386108 ; val_loss: 1.3140602111816406 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 537 ; train_loss: 0.5953282117843628 ; val_loss: 1.3134900331497192 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 538 ; train_loss: 0.5948488712310791 ; val_loss: 1.313407063484192 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 539 ; train_loss: 0.5950179696083069 ; val_loss: 1.313619613647461 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 540 ; train_loss: 0.5948050022125244 ; val_loss: 1.313727617263794 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 541 ; train_loss: 0.5954043865203857 ; val_loss: 1.314139485359192 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 542 ; train_loss: 0.5951329469680786 ; val_loss: 1.3146817684173584 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 543 ; train_loss: 0.5950098633766174 ; val_loss: 1.3150057792663574 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 544 ; train_loss: 0.5950561761856079 ; val_loss: 1.3153828382492065 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 545 ; train_loss: 0.5952407121658325 ; val_loss: 1.3155276775360107 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 546 ; train_loss: 0.5951570272445679 ; val_loss: 1.315514087677002 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 547 ; train_loss: 0.5946288108825684 ; val_loss: 1.3155440092086792 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 548 ; train_loss: 0.594954788684845 ; val_loss: 1.315438985824585 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 549 ; train_loss: 0.5950708985328674 ; val_loss: 1.3154523372650146 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 550 ; train_loss: 0.5949788093566895 ; val_loss: 1.3153566122055054 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 551 ; train_loss: 0.5949908494949341 ; val_loss: 1.3154487609863281 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 552 ; train_loss: 0.5949517488479614 ; val_loss: 1.315557837486267 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 553 ; train_loss: 0.5950561165809631 ; val_loss: 1.3158648014068604 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 554 ; train_loss: 0.5946944952011108 ; val_loss: 1.3161503076553345 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 555 ; train_loss: 0.5949501395225525 ; val_loss: 1.3165955543518066 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 556 ; train_loss: 0.5947844982147217 ; val_loss: 1.3169974088668823 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 557 ; train_loss: 0.5949041843414307 ; val_loss: 1.317498803138733 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 558 ; train_loss: 0.5948732495307922 ; val_loss: 1.3176199197769165 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 559 ; train_loss: 0.5947003364562988 ; val_loss: 1.3177824020385742 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 560 ; train_loss: 0.5948172807693481 ; val_loss: 1.3180086612701416 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 561 ; train_loss: 0.5950273275375366 ; val_loss: 1.3182227611541748 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 562 ; train_loss: 0.5948891639709473 ; val_loss: 1.3184236288070679 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 563 ; train_loss: 0.5949698090553284 ; val_loss: 1.3185850381851196 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 564 ; train_loss: 0.5947270393371582 ; val_loss: 1.31864333152771 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 565 ; train_loss: 0.5948805809020996 ; val_loss: 1.3187533617019653 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 566 ; train_loss: 0.5950496196746826 ; val_loss: 1.3185901641845703 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 567 ; train_loss: 0.5951921343803406 ; val_loss: 1.3182084560394287 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 568 ; train_loss: 0.5945760607719421 ; val_loss: 1.3180553913116455 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 569 ; train_loss: 0.5944329500198364 ; val_loss: 1.3178503513336182 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 570 ; train_loss: 0.5946445465087891 ; val_loss: 1.3175840377807617 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 571 ; train_loss: 0.5946084856987 ; val_loss: 1.3173155784606934 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 572 ; train_loss: 0.5945013165473938 ; val_loss: 1.317427158355713 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 573 ; train_loss: 0.5948229432106018 ; val_loss: 1.317643404006958 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 574 ; train_loss: 0.5946424007415771 ; val_loss: 1.3177683353424072 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 575 ; train_loss: 0.5947285890579224 ; val_loss: 1.3178917169570923 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 576 ; train_loss: 0.5946066379547119 ; val_loss: 1.3179752826690674 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 577 ; train_loss: 0.5947006940841675 ; val_loss: 1.3179525136947632 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 578 ; train_loss: 0.5950655341148376 ; val_loss: 1.3183019161224365 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 579 ; train_loss: 0.5950125455856323 ; val_loss: 1.3188986778259277 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 580 ; train_loss: 0.594773530960083 ; val_loss: 1.3195202350616455 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 581 ; train_loss: 0.5947264432907104 ; val_loss: 1.3204104900360107 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 582 ; train_loss: 0.5946151614189148 ; val_loss: 1.321352243423462 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 583 ; train_loss: 0.59470134973526 ; val_loss: 1.3221862316131592 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 584 ; train_loss: 0.5951283574104309 ; val_loss: 1.322052240371704 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 585 ; train_loss: 0.5947386622428894 ; val_loss: 1.321953535079956 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 586 ; train_loss: 0.5948588848114014 ; val_loss: 1.3215192556381226 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 587 ; train_loss: 0.5945913791656494 ; val_loss: 1.3209933042526245 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 588 ; train_loss: 0.5946308374404907 ; val_loss: 1.3210067749023438 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 589 ; train_loss: 0.5952152013778687 ; val_loss: 1.3211543560028076 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 590 ; train_loss: 0.5947970151901245 ; val_loss: 1.3206093311309814 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 591 ; train_loss: 0.5947461724281311 ; val_loss: 1.3198758363723755 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 592 ; train_loss: 0.5944350957870483 ; val_loss: 1.3186585903167725 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 593 ; train_loss: 0.5948009490966797 ; val_loss: 1.317812442779541 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 594 ; train_loss: 0.594952404499054 ; val_loss: 1.3165093660354614 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 595 ; train_loss: 0.5949616432189941 ; val_loss: 1.316746711730957 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 596 ; train_loss: 0.594777524471283 ; val_loss: 1.3171504735946655 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 597 ; train_loss: 0.594787061214447 ; val_loss: 1.3181123733520508 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 598 ; train_loss: 0.5946918725967407 ; val_loss: 1.3187456130981445 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 599 ; train_loss: 0.594784677028656 ; val_loss: 1.3194466829299927 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 600 ; train_loss: 0.5947739481925964 ; val_loss: 1.3209130764007568 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 601 ; train_loss: 0.594904899597168 ; val_loss: 1.3227300643920898 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 602 ; train_loss: 0.5947327613830566 ; val_loss: 1.3244752883911133 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 603 ; train_loss: 0.5948286652565002 ; val_loss: 1.3259787559509277 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 604 ; train_loss: 0.5948236584663391 ; val_loss: 1.326716661453247 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 605 ; train_loss: 0.5947527885437012 ; val_loss: 1.3261966705322266 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 606 ; train_loss: 0.5948864817619324 ; val_loss: 1.32578706741333 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 607 ; train_loss: 0.5945838093757629 ; val_loss: 1.3246465921401978 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 608 ; train_loss: 0.594867467880249 ; val_loss: 1.323637843132019 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 609 ; train_loss: 0.5948310494422913 ; val_loss: 1.3222131729125977 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 610 ; train_loss: 0.5945261716842651 ; val_loss: 1.3208835124969482 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 611 ; train_loss: 0.5947593450546265 ; val_loss: 1.3193975687026978 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 612 ; train_loss: 0.5944700241088867 ; val_loss: 1.318239450454712 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 613 ; train_loss: 0.5945751667022705 ; val_loss: 1.3172576427459717 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 614 ; train_loss: 0.5946120023727417 ; val_loss: 1.316617727279663 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 615 ; train_loss: 0.5948725342750549 ; val_loss: 1.3164441585540771 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 616 ; train_loss: 0.5948789715766907 ; val_loss: 1.3172683715820312 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 617 ; train_loss: 0.5948781371116638 ; val_loss: 1.3186839818954468 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 618 ; train_loss: 0.594527006149292 ; val_loss: 1.3201767206192017 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 619 ; train_loss: 0.5947652459144592 ; val_loss: 1.3217607736587524 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 620 ; train_loss: 0.5944750905036926 ; val_loss: 1.3232206106185913 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 621 ; train_loss: 0.5947484374046326 ; val_loss: 1.323176383972168 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 622 ; train_loss: 0.5944970846176147 ; val_loss: 1.3223923444747925 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 623 ; train_loss: 0.5945411920547485 ; val_loss: 1.3213458061218262 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 624 ; train_loss: 0.594648003578186 ; val_loss: 1.319211483001709 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 625 ; train_loss: 0.5946296453475952 ; val_loss: 1.3173545598983765 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 626 ; train_loss: 0.5943622589111328 ; val_loss: 1.3159204721450806 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 627 ; train_loss: 0.59462970495224 ; val_loss: 1.3169901371002197 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 628 ; train_loss: 0.5947077870368958 ; val_loss: 1.3176640272140503 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 629 ; train_loss: 0.5944647192955017 ; val_loss: 1.3180923461914062 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 630 ; train_loss: 0.5943971872329712 ; val_loss: 1.3183096647262573 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 631 ; train_loss: 0.5944569706916809 ; val_loss: 1.3192613124847412 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 632 ; train_loss: 0.5946028828620911 ; val_loss: 1.3206324577331543 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 633 ; train_loss: 0.5944432616233826 ; val_loss: 1.3214991092681885 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 634 ; train_loss: 0.5943634510040283 ; val_loss: 1.3219835758209229 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 635 ; train_loss: 0.5951129198074341 ; val_loss: 1.3225675821304321 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 636 ; train_loss: 0.5945775508880615 ; val_loss: 1.323171615600586 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 637 ; train_loss: 0.5946923494338989 ; val_loss: 1.3236994743347168 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 638 ; train_loss: 0.5947225093841553 ; val_loss: 1.324068307876587 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 639 ; train_loss: 0.5948795080184937 ; val_loss: 1.3243236541748047 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 640 ; train_loss: 0.5949211716651917 ; val_loss: 1.3234302997589111 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 641 ; train_loss: 0.5948113203048706 ; val_loss: 1.3215364217758179 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 642 ; train_loss: 0.5942866802215576 ; val_loss: 1.3194249868392944 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 643 ; train_loss: 0.594660758972168 ; val_loss: 1.3166818618774414 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 644 ; train_loss: 0.5943370461463928 ; val_loss: 1.3142116069793701 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 645 ; train_loss: 0.5943661332130432 ; val_loss: 1.3142000436782837 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 646 ; train_loss: 0.5944663286209106 ; val_loss: 1.3155237436294556 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 647 ; train_loss: 0.5948440432548523 ; val_loss: 1.318388819694519 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 648 ; train_loss: 0.5948330760002136 ; val_loss: 1.321495771408081 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 649 ; train_loss: 0.5951927304267883 ; val_loss: 1.3248043060302734 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 650 ; train_loss: 0.5944046378135681 ; val_loss: 1.3262110948562622 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 651 ; train_loss: 0.5946162343025208 ; val_loss: 1.327223539352417 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 652 ; train_loss: 0.5942714214324951 ; val_loss: 1.3278369903564453 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 653 ; train_loss: 0.5944066643714905 ; val_loss: 1.328427791595459 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 654 ; train_loss: 0.5943914651870728 ; val_loss: 1.3289356231689453 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 655 ; train_loss: 0.5950185060501099 ; val_loss: 1.3291407823562622 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 656 ; train_loss: 0.594399631023407 ; val_loss: 1.3272514343261719 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 657 ; train_loss: 0.5944098234176636 ; val_loss: 1.3242919445037842 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 658 ; train_loss: 0.5953418016433716 ; val_loss: 1.3214665651321411 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 659 ; train_loss: 0.5942497253417969 ; val_loss: 1.3183701038360596 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 660 ; train_loss: 0.594272792339325 ; val_loss: 1.3154910802841187 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 661 ; train_loss: 0.5942046642303467 ; val_loss: 1.3130046129226685 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 662 ; train_loss: 0.5944088697433472 ; val_loss: 1.312149167060852 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 663 ; train_loss: 0.5954349040985107 ; val_loss: 1.3110896348953247 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 664 ; train_loss: 0.594965398311615 ; val_loss: 1.309680700302124 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 665 ; train_loss: 0.5946860313415527 ; val_loss: 1.3086602687835693 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 666 ; train_loss: 0.5942471623420715 ; val_loss: 1.3083528280258179 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 667 ; train_loss: 0.594438910484314 ; val_loss: 1.3111143112182617 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 668 ; train_loss: 0.5947623252868652 ; val_loss: 1.3156744241714478 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 669 ; train_loss: 0.5945945978164673 ; val_loss: 1.3207346200942993 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 670 ; train_loss: 0.5947340726852417 ; val_loss: 1.3253334760665894 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 671 ; train_loss: 0.5944233536720276 ; val_loss: 1.3281677961349487 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 672 ; train_loss: 0.5941985845565796 ; val_loss: 1.3306878805160522 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 673 ; train_loss: 0.5943902730941772 ; val_loss: 1.332200050354004 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 674 ; train_loss: 0.5943751931190491 ; val_loss: 1.3327598571777344 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 675 ; train_loss: 0.5948004722595215 ; val_loss: 1.332576036453247 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 676 ; train_loss: 0.5945163369178772 ; val_loss: 1.3324406147003174 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 677 ; train_loss: 0.5944506525993347 ; val_loss: 1.332183837890625 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 678 ; train_loss: 0.5947135090827942 ; val_loss: 1.3294897079467773 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 679 ; train_loss: 0.594479501247406 ; val_loss: 1.3270313739776611 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 680 ; train_loss: 0.5943375825881958 ; val_loss: 1.325313687324524 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 681 ; train_loss: 0.5942790508270264 ; val_loss: 1.3241454362869263 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 682 ; train_loss: 0.5946264266967773 ; val_loss: 1.3230592012405396 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 683 ; train_loss: 0.5945674180984497 ; val_loss: 1.3223198652267456 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 684 ; train_loss: 0.594753623008728 ; val_loss: 1.321797251701355 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 685 ; train_loss: 0.5947977304458618 ; val_loss: 1.3212981224060059 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 686 ; train_loss: 0.5945664048194885 ; val_loss: 1.3206835985183716 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 687 ; train_loss: 0.5946746468544006 ; val_loss: 1.3209476470947266 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 688 ; train_loss: 0.5942293405532837 ; val_loss: 1.3208212852478027 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 689 ; train_loss: 0.5944682955741882 ; val_loss: 1.3209006786346436 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 690 ; train_loss: 0.5947230458259583 ; val_loss: 1.3207377195358276 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 691 ; train_loss: 0.594326376914978 ; val_loss: 1.321545958518982 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 692 ; train_loss: 0.5942497253417969 ; val_loss: 1.3227620124816895 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 693 ; train_loss: 0.5942056179046631 ; val_loss: 1.3237366676330566 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 694 ; train_loss: 0.5942761898040771 ; val_loss: 1.3243801593780518 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 695 ; train_loss: 0.5942673087120056 ; val_loss: 1.3247874975204468 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 696 ; train_loss: 0.5944693088531494 ; val_loss: 1.324978232383728 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 697 ; train_loss: 0.5945312976837158 ; val_loss: 1.3251763582229614 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 698 ; train_loss: 0.5942636728286743 ; val_loss: 1.3253939151763916 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 699 ; train_loss: 0.5945098400115967 ; val_loss: 1.3256540298461914 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 700 ; train_loss: 0.5942721366882324 ; val_loss: 1.324774146080017 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 701 ; train_loss: 0.5947121381759644 ; val_loss: 1.3246407508850098 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 702 ; train_loss: 0.5944334864616394 ; val_loss: 1.3237779140472412 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 703 ; train_loss: 0.5943870544433594 ; val_loss: 1.323306679725647 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 704 ; train_loss: 0.5944712162017822 ; val_loss: 1.322993516921997 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 705 ; train_loss: 0.5941677689552307 ; val_loss: 1.3228240013122559 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 706 ; train_loss: 0.5944398641586304 ; val_loss: 1.3225764036178589 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 707 ; train_loss: 0.5944252014160156 ; val_loss: 1.322668433189392 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 708 ; train_loss: 0.5944794416427612 ; val_loss: 1.3226890563964844 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 709 ; train_loss: 0.5941516757011414 ; val_loss: 1.3227887153625488 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 710 ; train_loss: 0.5943939089775085 ; val_loss: 1.3228740692138672 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 711 ; train_loss: 0.5943635106086731 ; val_loss: 1.3226600885391235 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 712 ; train_loss: 0.5941422581672668 ; val_loss: 1.3225116729736328 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 713 ; train_loss: 0.5943264365196228 ; val_loss: 1.3223146200180054 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 714 ; train_loss: 0.5942260026931763 ; val_loss: 1.322141408920288 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 715 ; train_loss: 0.5942448377609253 ; val_loss: 1.3220579624176025 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 716 ; train_loss: 0.594310998916626 ; val_loss: 1.3220200538635254 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 717 ; train_loss: 0.5941415429115295 ; val_loss: 1.3219763040542603 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 718 ; train_loss: 0.5941055417060852 ; val_loss: 1.3218011856079102 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 719 ; train_loss: 0.5943536758422852 ; val_loss: 1.3216874599456787 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 720 ; train_loss: 0.5943277478218079 ; val_loss: 1.321684718132019 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 721 ; train_loss: 0.5940847992897034 ; val_loss: 1.3217768669128418 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 722 ; train_loss: 0.5941917300224304 ; val_loss: 1.322561264038086 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 723 ; train_loss: 0.5942824482917786 ; val_loss: 1.3232953548431396 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 724 ; train_loss: 0.5941100120544434 ; val_loss: 1.3241729736328125 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 725 ; train_loss: 0.5943577885627747 ; val_loss: 1.3255945444107056 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 726 ; train_loss: 0.5943536758422852 ; val_loss: 1.326745867729187 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 727 ; train_loss: 0.594214677810669 ; val_loss: 1.327565312385559 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 728 ; train_loss: 0.5942360758781433 ; val_loss: 1.32815682888031 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 729 ; train_loss: 0.5943435430526733 ; val_loss: 1.3284064531326294 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 730 ; train_loss: 0.5941713452339172 ; val_loss: 1.3286267518997192 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 731 ; train_loss: 0.5942046046257019 ; val_loss: 1.328021764755249 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 732 ; train_loss: 0.5940269231796265 ; val_loss: 1.3274564743041992 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 733 ; train_loss: 0.5943078398704529 ; val_loss: 1.3267030715942383 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 734 ; train_loss: 0.5943760275840759 ; val_loss: 1.32583487033844 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 735 ; train_loss: 0.594345211982727 ; val_loss: 1.3251466751098633 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 736 ; train_loss: 0.5941877365112305 ; val_loss: 1.3246009349822998 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 737 ; train_loss: 0.5942233800888062 ; val_loss: 1.3241225481033325 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 738 ; train_loss: 0.5942463278770447 ; val_loss: 1.3236967325210571 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 739 ; train_loss: 0.5940821766853333 ; val_loss: 1.323269248008728 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 740 ; train_loss: 0.5943470597267151 ; val_loss: 1.323017954826355 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 741 ; train_loss: 0.5944229364395142 ; val_loss: 1.322981834411621 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 742 ; train_loss: 0.5941990613937378 ; val_loss: 1.322945475578308 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 743 ; train_loss: 0.5941298007965088 ; val_loss: 1.3238661289215088 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 744 ; train_loss: 0.5941314101219177 ; val_loss: 1.324776530265808 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 745 ; train_loss: 0.5941275358200073 ; val_loss: 1.325773000717163 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 746 ; train_loss: 0.5943148136138916 ; val_loss: 1.3267046213150024 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 747 ; train_loss: 0.5943594574928284 ; val_loss: 1.326755404472351 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 748 ; train_loss: 0.5941421389579773 ; val_loss: 1.3265212774276733 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 749 ; train_loss: 0.5942931771278381 ; val_loss: 1.325941562652588 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 750 ; train_loss: 0.5942701101303101 ; val_loss: 1.3253973722457886 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 751 ; train_loss: 0.5940569043159485 ; val_loss: 1.325183629989624 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 752 ; train_loss: 0.5941492319107056 ; val_loss: 1.3255969285964966 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 753 ; train_loss: 0.5943950414657593 ; val_loss: 1.3263115882873535 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 754 ; train_loss: 0.5941911339759827 ; val_loss: 1.327135682106018 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 755 ; train_loss: 0.5942199230194092 ; val_loss: 1.3278703689575195 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 756 ; train_loss: 0.5941881537437439 ; val_loss: 1.3284733295440674 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 757 ; train_loss: 0.5942431688308716 ; val_loss: 1.3286957740783691 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 758 ; train_loss: 0.5942295789718628 ; val_loss: 1.3285949230194092 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 759 ; train_loss: 0.594025731086731 ; val_loss: 1.3284363746643066 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 760 ; train_loss: 0.5941892266273499 ; val_loss: 1.3281900882720947 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 761 ; train_loss: 0.5940642356872559 ; val_loss: 1.3278920650482178 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 762 ; train_loss: 0.5941377878189087 ; val_loss: 1.3276253938674927 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 763 ; train_loss: 0.5942674875259399 ; val_loss: 1.327426552772522 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 764 ; train_loss: 0.5941711664199829 ; val_loss: 1.3274208307266235 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 765 ; train_loss: 0.5940554141998291 ; val_loss: 1.3274035453796387 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 766 ; train_loss: 0.5941178202629089 ; val_loss: 1.327622652053833 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 767 ; train_loss: 0.5940864086151123 ; val_loss: 1.3277696371078491 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 768 ; train_loss: 0.5940567851066589 ; val_loss: 1.3277963399887085 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 769 ; train_loss: 0.5940555334091187 ; val_loss: 1.3279048204421997 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 770 ; train_loss: 0.5941049456596375 ; val_loss: 1.328052282333374 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 771 ; train_loss: 0.5940737128257751 ; val_loss: 1.3280744552612305 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 772 ; train_loss: 0.5942269563674927 ; val_loss: 1.3278706073760986 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 773 ; train_loss: 0.5940474271774292 ; val_loss: 1.327628254890442 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 774 ; train_loss: 0.5941729545593262 ; val_loss: 1.327388882637024 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 775 ; train_loss: 0.5942524075508118 ; val_loss: 1.3270725011825562 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 776 ; train_loss: 0.5942449569702148 ; val_loss: 1.3267500400543213 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 777 ; train_loss: 0.5941509008407593 ; val_loss: 1.326838493347168 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 778 ; train_loss: 0.594233512878418 ; val_loss: 1.327353596687317 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 779 ; train_loss: 0.5941713452339172 ; val_loss: 1.327962040901184 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 780 ; train_loss: 0.5941539406776428 ; val_loss: 1.328607201576233 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 781 ; train_loss: 0.5941047668457031 ; val_loss: 1.3290505409240723 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 782 ; train_loss: 0.5940724611282349 ; val_loss: 1.3293668031692505 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 783 ; train_loss: 0.5942590832710266 ; val_loss: 1.329533576965332 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 784 ; train_loss: 0.5942902565002441 ; val_loss: 1.3295856714248657 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 785 ; train_loss: 0.5942017436027527 ; val_loss: 1.3296689987182617 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 786 ; train_loss: 0.5940642356872559 ; val_loss: 1.3291314840316772 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 787 ; train_loss: 0.594100832939148 ; val_loss: 1.328571081161499 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 788 ; train_loss: 0.5940980911254883 ; val_loss: 1.328170895576477 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 789 ; train_loss: 0.5940952301025391 ; val_loss: 1.3279296159744263 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 790 ; train_loss: 0.594120979309082 ; val_loss: 1.3278993368148804 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 791 ; train_loss: 0.5940908789634705 ; val_loss: 1.3279942274093628 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 792 ; train_loss: 0.5941410660743713 ; val_loss: 1.3280140161514282 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 793 ; train_loss: 0.5941318869590759 ; val_loss: 1.3283694982528687 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 794 ; train_loss: 0.5940600633621216 ; val_loss: 1.3287320137023926 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 795 ; train_loss: 0.5939562916755676 ; val_loss: 1.329155445098877 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 796 ; train_loss: 0.5939850211143494 ; val_loss: 1.3294813632965088 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 797 ; train_loss: 0.5940538048744202 ; val_loss: 1.3297057151794434 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 798 ; train_loss: 0.5942270159721375 ; val_loss: 1.3298163414001465 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 799 ; train_loss: 0.5940473675727844 ; val_loss: 1.3298619985580444 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 800 ; train_loss: 0.5942351818084717 ; val_loss: 1.3290793895721436 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 801 ; train_loss: 0.5939886569976807 ; val_loss: 1.3283681869506836 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 802 ; train_loss: 0.5941747426986694 ; val_loss: 1.32771635055542 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 803 ; train_loss: 0.5942229628562927 ; val_loss: 1.3273959159851074 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 804 ; train_loss: 0.5940657258033752 ; val_loss: 1.3274097442626953 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 805 ; train_loss: 0.5940569043159485 ; val_loss: 1.3274922370910645 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 806 ; train_loss: 0.5939939022064209 ; val_loss: 1.3275059461593628 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 807 ; train_loss: 0.5940462946891785 ; val_loss: 1.3275243043899536 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 808 ; train_loss: 0.5939971804618835 ; val_loss: 1.3278952836990356 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 809 ; train_loss: 0.594158947467804 ; val_loss: 1.328477382659912 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 810 ; train_loss: 0.5939708948135376 ; val_loss: 1.3288947343826294 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 811 ; train_loss: 0.5940842628479004 ; val_loss: 1.3291962146759033 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 812 ; train_loss: 0.5939738750457764 ; val_loss: 1.329338550567627 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 813 ; train_loss: 0.5942209959030151 ; val_loss: 1.3294315338134766 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 814 ; train_loss: 0.5941987633705139 ; val_loss: 1.3289333581924438 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 815 ; train_loss: 0.5939319729804993 ; val_loss: 1.328752040863037 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 816 ; train_loss: 0.5940080285072327 ; val_loss: 1.3286439180374146 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 817 ; train_loss: 0.5940531492233276 ; val_loss: 1.3288497924804688 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 818 ; train_loss: 0.594074010848999 ; val_loss: 1.3288931846618652 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 819 ; train_loss: 0.5939053297042847 ; val_loss: 1.3289328813552856 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 820 ; train_loss: 0.5940737724304199 ; val_loss: 1.3290735483169556 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 821 ; train_loss: 0.5940440893173218 ; val_loss: 1.329291820526123 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 822 ; train_loss: 0.5941421389579773 ; val_loss: 1.3293788433074951 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 823 ; train_loss: 0.5939002633094788 ; val_loss: 1.3294286727905273 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 824 ; train_loss: 0.5940388441085815 ; val_loss: 1.3294662237167358 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 825 ; train_loss: 0.5940638184547424 ; val_loss: 1.3294965028762817 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 826 ; train_loss: 0.5941417813301086 ; val_loss: 1.3295649290084839 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 827 ; train_loss: 0.5941146016120911 ; val_loss: 1.3296968936920166 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 828 ; train_loss: 0.5941029787063599 ; val_loss: 1.3298832178115845 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 829 ; train_loss: 0.594179630279541 ; val_loss: 1.3301836252212524 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 830 ; train_loss: 0.5940039753913879 ; val_loss: 1.3304615020751953 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 831 ; train_loss: 0.5940873622894287 ; val_loss: 1.330747127532959 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 832 ; train_loss: 0.5940474271774292 ; val_loss: 1.3309290409088135 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 833 ; train_loss: 0.5941962599754333 ; val_loss: 1.3310954570770264 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 834 ; train_loss: 0.5939990878105164 ; val_loss: 1.3312078714370728 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 835 ; train_loss: 0.5941891074180603 ; val_loss: 1.3312149047851562 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 836 ; train_loss: 0.5940325260162354 ; val_loss: 1.331255316734314 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 837 ; train_loss: 0.5940729379653931 ; val_loss: 1.3312997817993164 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 838 ; train_loss: 0.5941701531410217 ; val_loss: 1.3311914205551147 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 839 ; train_loss: 0.5941748023033142 ; val_loss: 1.33111572265625 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 840 ; train_loss: 0.5939785242080688 ; val_loss: 1.3311283588409424 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 841 ; train_loss: 0.5940111875534058 ; val_loss: 1.3311536312103271 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 842 ; train_loss: 0.5938948392868042 ; val_loss: 1.3311618566513062 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 843 ; train_loss: 0.5939499139785767 ; val_loss: 1.3311095237731934 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 844 ; train_loss: 0.5940564870834351 ; val_loss: 1.331061601638794 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 845 ; train_loss: 0.5940195918083191 ; val_loss: 1.331205129623413 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 846 ; train_loss: 0.593974232673645 ; val_loss: 1.3313157558441162 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 847 ; train_loss: 0.5939964652061462 ; val_loss: 1.3314282894134521 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 848 ; train_loss: 0.5940176248550415 ; val_loss: 1.3314635753631592 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 849 ; train_loss: 0.5941575169563293 ; val_loss: 1.3312894105911255 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 850 ; train_loss: 0.5939880609512329 ; val_loss: 1.3311121463775635 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 851 ; train_loss: 0.5940395593643188 ; val_loss: 1.3308693170547485 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 852 ; train_loss: 0.5939409136772156 ; val_loss: 1.330590009689331 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 853 ; train_loss: 0.5939089059829712 ; val_loss: 1.33027982711792 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 854 ; train_loss: 0.5939831137657166 ; val_loss: 1.3300220966339111 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 855 ; train_loss: 0.5939730405807495 ; val_loss: 1.329829216003418 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 856 ; train_loss: 0.5940344333648682 ; val_loss: 1.3296358585357666 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 857 ; train_loss: 0.594023585319519 ; val_loss: 1.3297629356384277 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 858 ; train_loss: 0.594033420085907 ; val_loss: 1.3298839330673218 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 859 ; train_loss: 0.5939534902572632 ; val_loss: 1.3300162553787231 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 860 ; train_loss: 0.5941067337989807 ; val_loss: 1.3301304578781128 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 861 ; train_loss: 0.5940797924995422 ; val_loss: 1.3302067518234253 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 862 ; train_loss: 0.5939704775810242 ; val_loss: 1.3302689790725708 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 863 ; train_loss: 0.5940210819244385 ; val_loss: 1.330225944519043 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 864 ; train_loss: 0.5939581990242004 ; val_loss: 1.3302525281906128 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 865 ; train_loss: 0.5940158367156982 ; val_loss: 1.3302509784698486 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 866 ; train_loss: 0.5941197872161865 ; val_loss: 1.33011794090271 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 867 ; train_loss: 0.5940697193145752 ; val_loss: 1.3300117254257202 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 868 ; train_loss: 0.5940288305282593 ; val_loss: 1.3296388387680054 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 869 ; train_loss: 0.5939751267433167 ; val_loss: 1.329304814338684 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 870 ; train_loss: 0.5940618515014648 ; val_loss: 1.3289523124694824 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 871 ; train_loss: 0.5939518213272095 ; val_loss: 1.3285750150680542 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 872 ; train_loss: 0.5939255356788635 ; val_loss: 1.3284982442855835 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 873 ; train_loss: 0.5939458608627319 ; val_loss: 1.3283565044403076 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 874 ; train_loss: 0.5939319729804993 ; val_loss: 1.3281598091125488 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 875 ; train_loss: 0.593981146812439 ; val_loss: 1.3279727697372437 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 876 ; train_loss: 0.594050943851471 ; val_loss: 1.3279352188110352 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 877 ; train_loss: 0.5940405130386353 ; val_loss: 1.3279680013656616 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 878 ; train_loss: 0.5940908789634705 ; val_loss: 1.328577995300293 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 879 ; train_loss: 0.5939590930938721 ; val_loss: 1.3293718099594116 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 880 ; train_loss: 0.5941214561462402 ; val_loss: 1.3299469947814941 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 881 ; train_loss: 0.5941689610481262 ; val_loss: 1.330359935760498 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 882 ; train_loss: 0.5939302444458008 ; val_loss: 1.3307018280029297 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 883 ; train_loss: 0.5939173698425293 ; val_loss: 1.3309111595153809 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 884 ; train_loss: 0.5939830541610718 ; val_loss: 1.3310809135437012 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 885 ; train_loss: 0.59406578540802 ; val_loss: 1.3312681913375854 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 886 ; train_loss: 0.5939432978630066 ; val_loss: 1.331443190574646 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 887 ; train_loss: 0.5940126180648804 ; val_loss: 1.3315273523330688 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 888 ; train_loss: 0.593895673751831 ; val_loss: 1.3315908908843994 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 889 ; train_loss: 0.5939942598342896 ; val_loss: 1.331451654434204 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 890 ; train_loss: 0.5939845442771912 ; val_loss: 1.3311591148376465 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 891 ; train_loss: 0.5938575863838196 ; val_loss: 1.3308049440383911 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 892 ; train_loss: 0.5939297676086426 ; val_loss: 1.3305349349975586 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 893 ; train_loss: 0.5940424799919128 ; val_loss: 1.3303025960922241 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 894 ; train_loss: 0.5939141511917114 ; val_loss: 1.3299883604049683 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 895 ; train_loss: 0.5939131379127502 ; val_loss: 1.3297457695007324 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 896 ; train_loss: 0.5939033031463623 ; val_loss: 1.328713297843933 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 897 ; train_loss: 0.5939483642578125 ; val_loss: 1.3278344869613647 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 898 ; train_loss: 0.5940460562705994 ; val_loss: 1.3273438215255737 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 899 ; train_loss: 0.5940629243850708 ; val_loss: 1.3274965286254883 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 900 ; train_loss: 0.5939406156539917 ; val_loss: 1.3276087045669556 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 901 ; train_loss: 0.593991756439209 ; val_loss: 1.3280339241027832 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 902 ; train_loss: 0.5938673615455627 ; val_loss: 1.3284087181091309 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 903 ; train_loss: 0.5939217805862427 ; val_loss: 1.328685998916626 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 904 ; train_loss: 0.5940134525299072 ; val_loss: 1.3289343118667603 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 905 ; train_loss: 0.5939589738845825 ; val_loss: 1.328657865524292 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 906 ; train_loss: 0.5940696597099304 ; val_loss: 1.3279471397399902 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 907 ; train_loss: 0.5939227938652039 ; val_loss: 1.3273998498916626 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 908 ; train_loss: 0.5939120054244995 ; val_loss: 1.3273471593856812 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 909 ; train_loss: 0.5939847230911255 ; val_loss: 1.3277908563613892 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 910 ; train_loss: 0.5939456224441528 ; val_loss: 1.3284671306610107 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 911 ; train_loss: 0.5939614772796631 ; val_loss: 1.329103708267212 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 912 ; train_loss: 0.5939715504646301 ; val_loss: 1.3294745683670044 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 913 ; train_loss: 0.593878448009491 ; val_loss: 1.3294625282287598 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 914 ; train_loss: 0.5941332578659058 ; val_loss: 1.3294899463653564 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 915 ; train_loss: 0.5940136313438416 ; val_loss: 1.3292592763900757 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 916 ; train_loss: 0.5940101146697998 ; val_loss: 1.3290257453918457 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 917 ; train_loss: 0.5939915180206299 ; val_loss: 1.328874111175537 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 918 ; train_loss: 0.5939303040504456 ; val_loss: 1.3287774324417114 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 919 ; train_loss: 0.5939190983772278 ; val_loss: 1.3294203281402588 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 920 ; train_loss: 0.594036340713501 ; val_loss: 1.3305355310440063 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 921 ; train_loss: 0.5939635634422302 ; val_loss: 1.332540512084961 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 922 ; train_loss: 0.5939241647720337 ; val_loss: 1.334356427192688 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 923 ; train_loss: 0.5939505696296692 ; val_loss: 1.3360974788665771 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 924 ; train_loss: 0.5939300060272217 ; val_loss: 1.3375461101531982 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 925 ; train_loss: 0.5939940214157104 ; val_loss: 1.3374531269073486 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 926 ; train_loss: 0.5940184593200684 ; val_loss: 1.3371765613555908 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 927 ; train_loss: 0.5939217805862427 ; val_loss: 1.335968255996704 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 928 ; train_loss: 0.5938965678215027 ; val_loss: 1.334423542022705 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 929 ; train_loss: 0.593892514705658 ; val_loss: 1.3330689668655396 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 930 ; train_loss: 0.5939120650291443 ; val_loss: 1.3319272994995117 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 931 ; train_loss: 0.5939863324165344 ; val_loss: 1.3317376375198364 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 932 ; train_loss: 0.593967080116272 ; val_loss: 1.3317335844039917 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 933 ; train_loss: 0.5940693020820618 ; val_loss: 1.3324247598648071 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 934 ; train_loss: 0.5939153432846069 ; val_loss: 1.331343412399292 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 935 ; train_loss: 0.593949556350708 ; val_loss: 1.3305420875549316 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 936 ; train_loss: 0.594088613986969 ; val_loss: 1.329804539680481 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 937 ; train_loss: 0.5940066576004028 ; val_loss: 1.328856110572815 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 938 ; train_loss: 0.5940228700637817 ; val_loss: 1.3281868696212769 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 939 ; train_loss: 0.5939676761627197 ; val_loss: 1.3279600143432617 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 940 ; train_loss: 0.5939655900001526 ; val_loss: 1.3280967473983765 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 941 ; train_loss: 0.5939620733261108 ; val_loss: 1.328383445739746 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 942 ; train_loss: 0.5939955711364746 ; val_loss: 1.3286089897155762 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 943 ; train_loss: 0.5938889980316162 ; val_loss: 1.3288602828979492 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 944 ; train_loss: 0.593964695930481 ; val_loss: 1.330117106437683 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 945 ; train_loss: 0.5938878655433655 ; val_loss: 1.3312991857528687 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 946 ; train_loss: 0.5939393639564514 ; val_loss: 1.3313817977905273 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 947 ; train_loss: 0.5939466953277588 ; val_loss: 1.331284523010254 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 948 ; train_loss: 0.5939589738845825 ; val_loss: 1.3313074111938477 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 949 ; train_loss: 0.5939183831214905 ; val_loss: 1.331279993057251 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 950 ; train_loss: 0.5939471125602722 ; val_loss: 1.3312287330627441 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 951 ; train_loss: 0.5939883589744568 ; val_loss: 1.330757737159729 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 952 ; train_loss: 0.5938953757286072 ; val_loss: 1.3299295902252197 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 953 ; train_loss: 0.5938600897789001 ; val_loss: 1.3298726081848145 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 954 ; train_loss: 0.593931257724762 ; val_loss: 1.329857349395752 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 955 ; train_loss: 0.5939435958862305 ; val_loss: 1.3298285007476807 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 956 ; train_loss: 0.5939053297042847 ; val_loss: 1.3296613693237305 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 957 ; train_loss: 0.593957245349884 ; val_loss: 1.3300299644470215 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 958 ; train_loss: 0.594002366065979 ; val_loss: 1.3308303356170654 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 959 ; train_loss: 0.5939885973930359 ; val_loss: 1.3319263458251953 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 960 ; train_loss: 0.5938933491706848 ; val_loss: 1.3322218656539917 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 961 ; train_loss: 0.5938562750816345 ; val_loss: 1.3316131830215454 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 962 ; train_loss: 0.5939206480979919 ; val_loss: 1.3311302661895752 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 963 ; train_loss: 0.5940195322036743 ; val_loss: 1.3301690816879272 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 964 ; train_loss: 0.5940369367599487 ; val_loss: 1.3297500610351562 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 965 ; train_loss: 0.5939887166023254 ; val_loss: 1.3294016122817993 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 966 ; train_loss: 0.5940015316009521 ; val_loss: 1.3288099765777588 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 967 ; train_loss: 0.5939173102378845 ; val_loss: 1.3280718326568604 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 968 ; train_loss: 0.5938940644264221 ; val_loss: 1.3272926807403564 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 969 ; train_loss: 0.5938596725463867 ; val_loss: 1.327986478805542 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 970 ; train_loss: 0.5943716764450073 ; val_loss: 1.329643726348877 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 971 ; train_loss: 0.5938920974731445 ; val_loss: 1.3313803672790527 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 972 ; train_loss: 0.5939422249794006 ; val_loss: 1.332901954650879 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 973 ; train_loss: 0.5938698649406433 ; val_loss: 1.3341419696807861 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 974 ; train_loss: 0.5940937995910645 ; val_loss: 1.3350131511688232 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 975 ; train_loss: 0.5941320061683655 ; val_loss: 1.335653305053711 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 976 ; train_loss: 0.5940282344818115 ; val_loss: 1.3358402252197266 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 977 ; train_loss: 0.5938454866409302 ; val_loss: 1.333990216255188 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 978 ; train_loss: 0.5938707590103149 ; val_loss: 1.330977439880371 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 979 ; train_loss: 0.5940847396850586 ; val_loss: 1.327945351600647 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 980 ; train_loss: 0.5939029455184937 ; val_loss: 1.3251655101776123 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 981 ; train_loss: 0.5938671827316284 ; val_loss: 1.3227529525756836 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 982 ; train_loss: 0.5941827297210693 ; val_loss: 1.3225334882736206 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 983 ; train_loss: 0.5939465165138245 ; val_loss: 1.3220614194869995 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 984 ; train_loss: 0.5939267873764038 ; val_loss: 1.3216570615768433 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 985 ; train_loss: 0.5939083695411682 ; val_loss: 1.321317195892334 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 986 ; train_loss: 0.5939512848854065 ; val_loss: 1.3207775354385376 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 987 ; train_loss: 0.5941025614738464 ; val_loss: 1.3263957500457764 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 988 ; train_loss: 0.593864381313324 ; val_loss: 1.3333334922790527 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 989 ; train_loss: 0.5940646529197693 ; val_loss: 1.3374103307724 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 990 ; train_loss: 0.59391188621521 ; val_loss: 1.3406519889831543 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 991 ; train_loss: 0.5940093398094177 ; val_loss: 1.338356852531433 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 992 ; train_loss: 0.593913197517395 ; val_loss: 1.3360004425048828 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 993 ; train_loss: 0.5939853191375732 ; val_loss: 1.3341835737228394 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 994 ; train_loss: 0.594058096408844 ; val_loss: 1.333027958869934 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 995 ; train_loss: 0.5938903093338013 ; val_loss: 1.3322200775146484 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 996 ; train_loss: 0.5939978957176208 ; val_loss: 1.3339132070541382 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 997 ; train_loss: 0.5938868522644043 ; val_loss: 1.3329944610595703 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 998 ; train_loss: 0.5940033793449402 ; val_loss: 1.3325926065444946 val_bleu_score: 49.58\n",
      "当前学习率->  0.0001\n",
      ">>eopch: 999 ; train_loss: 0.5941683053970337 ; val_loss: 1.3307749032974243 val_bleu_score: 49.58\n"
     ]
    }
   ],
   "source": [
    "#####################  train.py  ##########################\n",
    "###########################################################\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "# 单轮训练\n",
    "def run_epoch(loader, model, loss_fun, optimizer=None):\n",
    "    # 本轮训练总损失\n",
    "    total_loss = 0\n",
    "    # 本轮 总批次\n",
    "    total_batchs = 0\n",
    "    \n",
    "    # print(next(iter(loader)))\n",
    "\n",
    "    # 加载数据并训练\n",
    "    for src_x, src_mask, tgt_x, tgt_mask, tgt_y, batch_tgt_text in loader:\n",
    "        src_x = src_x.to(DEVICE)\n",
    "        src_mask = src_mask.to(DEVICE)\n",
    "        tgt_x = tgt_x.to(DEVICE)\n",
    "        tgt_mask = tgt_mask.to(DEVICE)\n",
    "        tgt_y = tgt_y.to(DEVICE)\n",
    "        \n",
    "        # 模型输出 (batch, words_len, vocab_size) \n",
    "        # (批量个数(预测句子个数), 预测某个句子的单词个数, 预测的单词属于各个分类(vocab_size)的概率个数)\n",
    "        predict = model(src_x, src_mask, tgt_x, tgt_mask)\n",
    "        # print('predict.shape-> ', predict.shape)\n",
    "\n",
    "        # 形状转变，把批量维度去掉 (batch, words_len, vocab_size)  -> (words_len, vocab_size)\n",
    "        # 这样就变成了，预测的单词个数，以及单个单词属于各个分类的概率个数了\n",
    "        predict = predict.reshape(-1, predict.shape[-1])\n",
    "        \n",
    "        # 形状转变，(batch, target_size) -> (target_size) \n",
    "        # batch: 批次\n",
    "        # target_size:目标单词的个数, 里面放的是，目标单词的分类id\n",
    "        # 比如 目标词表有vocab_size个单词, tgt_y放的就是这个单词在词表中所属的id(经过交叉熵函数后，这个位置对应的单词就变成了概率只为1的目标值了)\n",
    "        tgt_y = tgt_y.reshape(-1)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = loss_fun(predict, tgt_y)\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss += loss.item()\n",
    "        total_batchs += 1\n",
    "        \n",
    "        # 反向传播\n",
    "        if optimizer:\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 计算梯度\n",
    "            loss.backward()\n",
    "            # 梯度更新\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 返回平均损失\n",
    "        return total_loss / total_batchs\n",
    "    \n",
    "\n",
    "    \n",
    "# 逐字生成预测值\n",
    "def greedy_decode(model, src_x, src_mask, max_len):\n",
    "    # 如果是多GPU,则原始模型存放在 model.module中\n",
    "    model = model.module if MULTI_GPU else model\n",
    "    \n",
    "    # 录入的句子进行编码 \n",
    "    memory = model.encode(src_x, src_mask)\n",
    "    \n",
    "    # 初始每个批次的目标值   src_x.size(0):是批次\n",
    "    # 形状(batch, 单词个数)\n",
    "    tgt_x = torch.tensor([[SOS_ID]] * src_x.size(0))\n",
    "    \n",
    "    tgt_x = tgt_x.to(DEVICE)\n",
    "    \n",
    "    # 开始生成目标值\n",
    "    for _ in range(max_len):\n",
    "        # 解码其中的批量掩码，这里不需要句子掩码\n",
    "        tgt_mask = get_padding_mask(tgt_x,padding_idx=PAD_ID)\n",
    "        \n",
    "        # 解码 \n",
    "        # 训练时output形状 (batch, 预测过程中句子长度, 词表长度)\n",
    "        # 预测时output形状 (batch, 1, 词表长度)\n",
    "        output = model.decode(tgt_x, tgt_mask, memory, src_mask, training=model.training) \n",
    "       \n",
    "        # 用生成器取最后一个词  output最后形状 (batch, 1, 词表长度)\n",
    "        #  output[:, -1, :].unsqueeze(1)\n",
    "        output = model.generator(output)\n",
    "        # [batch, 1, 20]\n",
    "\n",
    "        # 获取生成词的词id   predict形状(批次, 1)  1表示生成一个词\n",
    "        predict = torch.argmax(output, dim=-1)\n",
    "        \n",
    "        # 所有生成的词拼接上  形状(batch, 单词个数)  tgt_x就是预测的结果\n",
    "        tgt_x = torch.concat([tgt_x, predict], dim=-1)\n",
    "       \n",
    "        # 碰到EOS_ID则结束预测, 如果有一个批次没结束则继续预测？\n",
    "        if torch.all(predict == EOS_ID).item():\n",
    "            break\n",
    "            \n",
    "    \n",
    "    # 中文词表\n",
    "    zh_id2vocab, _ = get_vocab('zh')\n",
    "    # 生成的句子放到如下\n",
    "    batch_tgt_text = []\n",
    "    \n",
    "    # 把预测的id，翻译成对应的词和句子，fangdao batch_tgt_text\n",
    "    for tgt in tgt_x:\n",
    "        # 单个句子\n",
    "        text = []\n",
    "        # tgt中放的是词id\n",
    "        for word_id in tgt:\n",
    "            if word_id == SOS_ID:\n",
    "                continue\n",
    "            elif word_id == EOS_ID:  # 碰到结束标识，改句子后面的解析就不要了\n",
    "                break\n",
    "\n",
    "            # 词id换成词，并放到text中\n",
    "            text.append(zh_id2vocab[word_id])\n",
    "\n",
    "        # 各个批次的句子放到 batch_tgt_text中\n",
    "        batch_tgt_text.append(''.join(text))\n",
    "        \n",
    "    # 返回生成的句子\n",
    "    return batch_tgt_text \n",
    "\n",
    "\n",
    " \n",
    "# 评估翻译效果    \n",
    "def evaluate(loader, model, max_len):\n",
    "    # 目标值，目标句子\n",
    "    tgt_sentence = []\n",
    "    # 翻译的句子\n",
    "    prob_sentence = []\n",
    "\n",
    "    for src_x, src_mask, tgt_x, tgt_mask, tgt_y, tgt_text in loader:\n",
    "        src_x = src_x.to(DEVICE)\n",
    "        src_mask = src_mask.to(DEVICE)\n",
    "        \n",
    "        # 翻译好的句子\n",
    "        batch_prob_text = greedy_decode(model, src_x, src_mask, max_len)\n",
    "\n",
    "        prob_sentence += batch_prob_text\n",
    "        tgt_sentence += tgt_text\n",
    "    \n",
    "    # 返回评估分数\n",
    "    return bleu_score(prob_sentence, [tgt_sentence]),prob_sentence,tgt_sentence\n",
    "\n",
    "# 学习率\n",
    "LR = 0.0001\n",
    "# 训练次数\n",
    "EPOCHS = 1000\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    en_id2vocab,_ = get_vocab('en')\n",
    "    zh_id2vocab,_ = get_vocab('zh')\n",
    "    \n",
    "    # 英文词表大小\n",
    "    SRC_VOCAB_SIZE = len(en_id2vocab)\n",
    "    # 中文词表大小\n",
    "    TGT_VOCAB_SIZE = len(zh_id2vocab)\n",
    "\n",
    "    # 实例化模型\n",
    "    model = make_model(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, D_MODEL, N_HEAD, D_FF, N, DROPOUT)\n",
    "    model = model.to(DEVICE)\n",
    "     # 加载已经训练好的模型\n",
    "    model.load_state_dict(torch.load('./model/best_model.pth', map_location=DEVICE))\n",
    "    \n",
    "    # 如果是多GPU\n",
    "    if MULTI_GPU:\n",
    "        # model = nn.DataParallel(model)\n",
    "        model = BalancedDataParallel(BATCH_SIZE_GPU0, model, dim=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    ## 模型参数量\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print('模型参数量: ',total_params)\n",
    "    \n",
    "    #损失函数和优化器\n",
    "    loss_fun = CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=LABEL_SMOOTHING)\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    # 动态调整学习率\n",
    "    # lr_scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_lambda_fn(step, 1))\n",
    "    \n",
    "    train_dataset = Dataset('train')\n",
    "    train_loader = data.DataLoader(train_dataset, shuffle=True, collate_fn=train_dataset.collate_fn, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    val_dataset = Dataset('val')\n",
    "    val_loader = data.DataLoader(val_dataset, shuffle=True, collate_fn=val_dataset.collate_fn, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    #### 训练  #########\n",
    "    best_bleu_score = 74 # 最好的评估分数\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        ########### 训练模式 ##############\n",
    "        model.train()\n",
    "        train_loss = run_epoch(train_loader, model, loss_fun,optimizer)\n",
    "\n",
    "        # 动态调整学习率\n",
    "        # lr_scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print('当前学习率-> ',current_lr)\n",
    "        \n",
    "        ########### 验证流程 ##############\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(val_loader, model, loss_fun,None)\n",
    "        # 评估\n",
    "        val_bleu_score,prob_sentence,tgt_sentence = evaluate(val_loader,model,MAX_LEN)\n",
    "        \n",
    "        # print('翻译: ',prob_sentence)\n",
    "        # print('目标: ',tgt_sentence)\n",
    "        print('>>eopch:',i, '; train_loss:',train_loss,'; val_loss:',val_loss, 'val_bleu_score:', val_bleu_score)\n",
    "        \n",
    "        # 如果当前评估分数好于 best_bleu_score，则保存模型\n",
    "        if val_bleu_score > best_bleu_score:\n",
    "            torch.save(model.state_dict(), './model/best_model.pth')\n",
    "            best_bleu_score = val_bleu_score\n",
    "            print('翻译: ',prob_sentence)\n",
    "            print('目标: ',tgt_sentence)\n",
    "            print('saveing-> ',train_loss, best_bleu_score)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd5573-c01b-4aa1-906e-d0e3d8adb60e",
   "metadata": {},
   "source": [
    "# 7.模型预测  predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "18a1a333-4ede-4b7a-b589-362e92c5de64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "MULTI_GPU: False\n"
     ]
    }
   ],
   "source": [
    "################################# config.py ################################\n",
    "############################################################################\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "BASE_PATH = '/Users/mashunfeng/My/AI/'\n",
    "\n",
    "# 训练数据文件路径\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, 'news/train.json')\n",
    "# 校验数据文件路径\n",
    "VAL_PATH = os.path.join(BASE_PATH, 'news/dev.json')\n",
    "\n",
    "\n",
    "# 存放中文词表的文件\n",
    "ZH_VOCAB_PATH = os.path.join('/Users/mashunfeng/My/AI/', 'zh.txt')\n",
    "# 存放英文词表的文件\n",
    "EN_VOCAB_PATH = os.path.join('/Users/mashunfeng/My/AI/', 'en.txt')\n",
    "\n",
    "# 特殊字符在词表中的id值\n",
    "PAD_ID = 0  # 屏蔽词id\n",
    "UNK_ID = 1  # 不知道的词id\n",
    "SOS_ID = 2  # 开始标记id\n",
    "EOS_ID = 3  # 结束标记id\n",
    "\n",
    "# 子层编码和解码个数\n",
    "N = 6\n",
    "# 词向量维度\n",
    "D_MODEL = 512\n",
    "# 头数\n",
    "N_HEAD = 8\n",
    "# feedforward 维度\n",
    "D_FF = 2048\n",
    "\n",
    "DROPOUT = 0.1\n",
    "# # 批次\n",
    "BATCH_SIZE = 350\n",
    "BATCH_SIZE_GPU0 = 50\n",
    "\n",
    "# 学习率\n",
    "LR = 1e-5\n",
    "# 训练次数\n",
    "EPOCHS = 10000\n",
    "\n",
    "# 生成句子最大长度 \n",
    "MAX_LEN = 50\n",
    "\n",
    "# 标签平滑\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# 运行设备\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 多GPU\n",
    "MULTI_GPU = False\n",
    "if torch.cuda.device_count() > 1:\n",
    "    MULTI_GPU = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(DEVICE)\n",
    "    print('MULTI_GPU:', MULTI_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "09f7ed06-1851-48f1-859c-8d468b0057a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,  888, 1615,  159,    4,  120,   84,    6,   31,   24,  357,    9,\n",
      "          490,   71,  150,  815,  369,  254,   74,   23, 4016,   14,    7,  254,\n",
      "         2588,  156,   67,   27,    5,    3],\n",
      "        [   2, 1631,   14,   20,    4,   31,    6,    7, 1631, 2509,   19,  174,\n",
      "            5,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "['经过漫长的深知，人们深知，岁月的', '如果说都我们都我们定位所有”都目前']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    en_id2vocab, en_vocab2id  = get_vocab('en')\n",
    "    zh_id2vocab, zh_vocab2id  = get_vocab('zh')\n",
    "    \n",
    "    # 英文词表大小\n",
    "    SRC_VOCAB_SIZE = len(en_id2vocab)\n",
    "    # 中文词表大小\n",
    "    TGT_VOCAB_SIZE = len(zh_id2vocab)\n",
    "\n",
    "    # 实例化模型\n",
    "    model = make_model(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, D_MODEL, N_HEAD, D_FF, N, DROPOUT)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    model = BalancedDataParallel(BATCH_SIZE_GPU0, model, dim=0)\n",
    "\n",
    "    # 加载模型参数\n",
    "    model.load_state_dict(torch.load('/Users/mashunfeng/My/AI/best_model.pth', map_location=DEVICE))\n",
    "    model = model.module\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    src_texts = [\n",
    "        \"Having gone through the long years, people have come to understand more than ever before how important this doctrine is and how precious peace can be.\",\n",
    "        \"Everything is for the people, and everything relies on them.\",\n",
    "    ]\n",
    "\n",
    "    # 获取句子在词表中的id表示\n",
    "    src_tokens = [[en_vocab2id.get(word.lower(), UNK_ID) for word in divided_en(text)] \n",
    "                  for text in src_texts]\n",
    "    # 句子收尾加上开始和结束标识\n",
    "    src_tokens = [torch.LongTensor([SOS_ID]+src+[EOS_ID]) for src in src_tokens]\n",
    "    \n",
    "    # 长度不一致的用0填充\n",
    "    src_x = pad_sequence(src_tokens, batch_first=True, padding_value=PAD_ID)\n",
    "    print(src_x)\n",
    "    # 掩码\n",
    "    src_mask = get_padding_mask(src_x, PAD_ID)\n",
    "    \n",
    "    predict = greedy_decode(model, src_x, src_mask, max_len=10)\n",
    "    print(predict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc33a72-ab24-4403-8d4d-6c749aea2f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
