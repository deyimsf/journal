{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8076fd47-5236-4459-a6c4-e1ea65fd2aad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数量:  90932\n",
      "saveing->  2.170600652694702\n",
      "saveing->  2.1704535484313965\n",
      "saveing->  2.170290470123291\n",
      "saveing->  2.1701200008392334\n",
      "saveing->  2.169950246810913\n",
      "saveing->  2.169790029525757\n",
      "saveing->  2.169635534286499\n",
      "saveing->  2.1694767475128174\n",
      "saveing->  2.169311285018921\n",
      "saveing->  2.169146776199341\n",
      "saveing->  2.16900897026062\n",
      "saveing->  2.1688809394836426\n",
      "saveing->  2.168689727783203\n",
      "saveing->  2.1684422492980957\n",
      "saveing->  2.1681020259857178\n",
      "saveing->  2.167386293411255\n",
      "saveing->  2.164933681488037\n",
      "saveing->  2.14681077003479\n",
      "saveing->  2.089017868041992\n",
      "saveing->  2.0841429233551025\n",
      "saveing->  2.0824124813079834\n",
      "saveing->  2.0818636417388916\n",
      "saveing->  2.081862211227417\n",
      "saveing->  2.081677198410034\n",
      "saveing->  2.081484317779541\n",
      "saveing->  2.0812766551971436\n",
      "saveing->  2.0810554027557373\n",
      "saveing->  2.080846071243286\n",
      "saveing->  2.080658197402954\n",
      "saveing->  2.0804998874664307\n",
      "saveing->  2.080368757247925\n",
      "saveing->  2.0802626609802246\n",
      "saveing->  2.080177068710327\n",
      "saveing->  2.0801050662994385\n",
      "saveing->  2.080043077468872\n",
      "saveing->  2.079986333847046\n",
      "saveing->  2.079932928085327\n",
      "saveing->  2.0798819065093994\n",
      "翻译:  ['我', '我']\n",
      "目标:  ['我喜欢打篮球', '他喜欢读书']\n",
      ">>eopch: 98 ; train_loss: 2.0798332691192627 ; val_loss: 2.797377109527588 \n",
      " val_bleu_score: 0.0\n",
      "saveing->  2.0798332691192627\n",
      "翻译:  ['我', '我']\n",
      "目标:  ['我喜欢打篮球', '他喜欢读书']\n",
      ">>eopch: 99 ; train_loss: 2.0797855854034424 ; val_loss: 2.794420003890991 \n",
      " val_bleu_score: 0.0\n",
      "saveing->  2.0797855854034424\n"
     ]
    }
   ],
   "source": [
    "#################### model.py #####################\n",
    "###################################################\n",
    "import math\n",
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# 嵌入层封装\n",
    "class Embeddings(nn.Module):\n",
    "    # vocab_size: 词表大小\n",
    "    # d_model: 词向量维度\n",
    "    def __init__(self, vocab_size, d_model, padding_idx=0):\n",
    "        super().__init__()\n",
    "        # lookup table Embeding层\n",
    "        self.lut = nn.Embedding(vocab_size, d_model, padding_idx,)\n",
    "        # 词向量维度\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    # 根据词id返回词向量\n",
    "    # x: 入参(句子个数, 句子的id表示)，形状是(句子个数，句子长度)\n",
    "    #    例如: [[1,2,4],[2,4,5]]\n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 2 , '入参形状需要是2维的(句子个数，句子长度)'\n",
    "        \n",
    "        # 返回句子的向量表示,形状(句子个数,句子长度,词向量维度)\n",
    "        # 这里返回的结果，每个元素都乘以了 根号下词向量维度\n",
    "        # 后面对模型词嵌入层参数初始化时，用的方法是xavier,该方法随机初始化参数满足N(0, 1/d_model),乘以 math.sqrt(self.d_model)，可以拉回到 N(0, 1) 分布\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "\n",
    "    \n",
    "# 优化后的位置编码\n",
    "# annotated-transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # max_len: 位置编码最大长度，也是句子长度\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 位置编码为什要加dropout ？？？？？\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 初始化位置编码\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # 位置和除数\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * math.log(10000) / d_model)\n",
    "        \n",
    "        angle = position * div_term\n",
    "        \n",
    "        # 填充pe矩阵值  0::2 从0开始步长是2  1::2 从1开始步长是2\n",
    "        pe[:, 0::2] = torch.sin(angle)\n",
    "        pe[:, 1::2] = torch.cos(angle)\n",
    "        \n",
    "        # pe = torch.linspace(-1,1,max_len).unsqueeze(1)\n",
    "        # pe = pe @ torch.ones(1,max_len)[:,:d_model]\n",
    "\n",
    "        # 同 self.pe = pe  \n",
    "        self.register_buffer('pe', pe) # 表示该值不用梯度更新\n",
    "        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 3 , '入参形状必须是 (batch, sentence_len, d_model)'\n",
    "        \n",
    "        # ## 把某个位置元素设置为0，这个位置是批量入参时的padding  临时功能 #####################\n",
    "        # pe = self.pe[:x.size(1)].masked_fill((x==0), 0)\n",
    "        # return x + pe\n",
    "\n",
    "        #### 正经功能 #####        \n",
    "        # 入参和位置编码相加\n",
    "        # 入参x的形状     (batch, sentence_len, d_model)\n",
    "        # 位置编码形状是   (max_len, d_model)\n",
    "        # 相加的时候位置编码只取 入参的sentens_len(句子长度)长度\n",
    "        # print('PositionalEncoding.forward--> ',x.shape, self.pe.shape)\n",
    "        # print('-->',self.pe[:x.size(1)].shape)\n",
    "        return x + self.pe[:x.size(1)]    \n",
    "    \n",
    "        # 位置编码为什么要加dropout ?????\n",
    "        #return self.dropout(x + self.pe[:x.size(1)])\n",
    "\n",
    "\n",
    "# 封装掩码 \n",
    "# 用来屏蔽批量入参时，句子长短不一致, 批量产生的掩码\n",
    "# 只返回掩码\n",
    "def get_padding_mask(x, padding_idx):\n",
    "    assert x.ndim == 2, '期望维度为2'\n",
    "    \n",
    "    # 入参x形状是 (批次，句子长度), 中间加一个维度，用来和带词向量的入参匹配\n",
    "    # (B,L) -> (B,L,d_model)\n",
    "    return (x == padding_idx).unsqueeze(1)\n",
    "   \n",
    "    \n",
    "\n",
    "# 计算注意力\n",
    "#  q、k、v的形状都是\n",
    "#     为分头 (batch,sen_len,d_model)\n",
    "#     已分头 (batch,n_head,words_len,d_k)\n",
    "def attention(q,k,v, mask=None, dropout=None):\n",
    "    \n",
    "    # 获取q、k、v的最后一个维度,即d_model\n",
    "    d_model = q.size(-1)\n",
    "    # 计算注意力分数 q乘k的转置\n",
    "    scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_model)\n",
    "    # print('q=',q.shape, 'k=',k.shape, 'v=',v.shape, 'mask=',mask.shape, 'scores=',scores.shape)\n",
    "    # q= torch.Size([3, 2, 1, 16]) k= torch.Size([3, 2, 1, 16]) v= torch.Size([3, 2, 1, 16]) mask= torch.Size([3, 1, 1, 2]) scores= torch.Size([3, 2, 1, 1])\n",
    "    \n",
    "    # 按照掩码mask对scores进行屏蔽\n",
    "    if mask is not None:\n",
    "        # 列掩码\n",
    "        # print('scores->',scores.shape, 'mask->',mask.shape)\n",
    "        scores = scores.masked_fill(mask, -1e9)\n",
    "       \n",
    "    # print('--> ',q.shape,k.shape,v.shape, scores.shape)\n",
    "    # torch.Size([3, 2, 1, 16]) torch.Size([3, 2, 1, 16]) torch.Size([3, 2, 1, 16]) torch.Size([3, 2, 1, 2]) torch.Size([3, 2, 1, 2])\n",
    "  \n",
    "\n",
    "    # 用softmax计算出分数的比重\n",
    "    softmax_scores = torch.softmax(scores, dim=-1)\n",
    "   \n",
    "    # dropout\n",
    "    if dropout is not None:\n",
    "        softmax_scores = dropout(softmax_scores)\n",
    "    \n",
    "    # 返回带注意力的数据\n",
    "    context = torch.matmul(softmax_scores, v)\n",
    "    \n",
    "    return context,  softmax_scores\n",
    "\n",
    "\n",
    "\n",
    "# 多头注意力\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout=0):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 头的数量要可以被词向量维度整除\n",
    "        assert d_model % n_head == 0\n",
    "        # 新头的维度\n",
    "        self.d_k = d_model // n_head\n",
    "        # 新头的个数\n",
    "        self.n_head = n_head\n",
    "        \n",
    "        # q k v多头之前先进行一次线性变换(原论文是先分头再线性变换，实验证明两者差不多)\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 多头拼接成原维度后再进行一次线性变换\n",
    "        self.linear = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    # 这里如果是自注意力，那么qkv就是x，入参直接写x就行了，为啥搞了3个入参?????\n",
    "    def forward(self, q, k, v, mask=None):        \n",
    "        assert q.ndim == 3, ' 形状要求(batch,words_len,d_model)'\n",
    "        assert k.ndim == 3, ' 形状要求(batch,words_len,d_model)'\n",
    "        assert v.ndim == 3, ' 形状要求(batch,words_len,d_model)'\n",
    "\n",
    "        \n",
    "        # 残差值，残差和规划应该放到外边，这里就凑活把\n",
    "        residual = q #把q当成入参了，放到做外边可以把带词向量的入参当入参\n",
    "        \n",
    "        # 批次\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # 1.先线性变换\n",
    "        q = self.W_Q(q)\n",
    "        k = self.W_K(k)\n",
    "        v = self.W_V(v)\n",
    "        \n",
    "        # 分头前\n",
    "        # print(q)\n",
    "            \n",
    "        # 2.分头 \n",
    "        # (batch,words_len,d_model) -> (batch, words_len, n_head, d_k)\n",
    "        q = q.view(batch_size, -1, self.n_head, self.d_k)\n",
    "        # (batch, words_len, n_head, d_k) -> (batch, n_head, words_len, d_k) \n",
    "        q = q.transpose(1,2)\n",
    "        \n",
    "        # 分头后\n",
    "        # print(q)\n",
    "\n",
    "            \n",
    "        k = k.view(batch_size, -1, self.n_head, self.d_k)\n",
    "        k = k.transpose(1,2)\n",
    "        \n",
    "        v = v.view(batch_size, -1, self.n_head, self.d_k)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        # 这里需要把掩码做扩维度吗？不可以自动广播吗 ??????\n",
    "        # 因为头是从(batch,words_len,d_model)变成了(batch,n_head,words_len,d_k)，所以掩码在头的维度扩一维\n",
    "        if mask is not None:\n",
    "            # print('-> ',mask.shape, mask.unsqueeze(1).shape)\n",
    "            mask = mask.unsqueeze(1) \n",
    "            \n",
    "        \n",
    "        # 计算注意力\n",
    "        context, softmax_scores = attention(q,k,v, mask=mask, dropout=self.dropout)\n",
    "        # print(context)\n",
    "        \n",
    "        # 掩码的形状和 注意力分数的形状， 注意力分数只关注词的个数\n",
    "        # print('mask.shape-> ',mask.shape, 'softmax_scores.shape->',  softmax_scores.shape)\n",
    "\n",
    "        # 把多头合并成一个头\n",
    "        # (batch, n_head, words_len, d_k) -> (batch, words_len, n_head ,d_k) -> (batch, words_len, n_head * d_k)\n",
    "        context = context.transpose(1,2).reshape(batch_size, -1, self.n_head * self.d_k)\n",
    "        # print(context)\n",
    "\n",
    "        # 然后再过一个线性层\n",
    "        context = self.linear(context)\n",
    "        \n",
    "        # print('residua-> ',residual)\n",
    "        # 残差和层归一化(这个操作最好放到外边)\n",
    "        return self.norm(residual+ context), softmax_scores \n",
    "\n",
    "    \n",
    "\n",
    "# 前馈神经网络\n",
    "class FeedForward(nn.Module):\n",
    "    # d_model: 输入维度  d_ff:输出维度\n",
    "    def __init__(self, d_model, d_ff, dropout=0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        \n",
    "        residual = x\n",
    "\n",
    "        x = self.w_1(x).relu()\n",
    "        x = self.dropout(x)\n",
    "        # print('+'*60)\n",
    "\n",
    "        x = self.w_2(x)\n",
    "        \n",
    "        # 加上残差和归一化\n",
    "        return self.norm(residual + x)\n",
    "\n",
    "    \n",
    "## 编码子层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff, dropout=0):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadedAttention(d_model, n_head, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "\n",
    "        # 1.先计算多头注意力\n",
    "        x,softmax_scores = self.mha(q=x, k=x, v=x, mask=mask)\n",
    "        # print(x.shape)\n",
    "        # 2.在进入前馈神经网络\n",
    "        return self.ff(x)\n",
    "    \n",
    "\n",
    "# 深度拷贝\n",
    "def clones(layer, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
    "\n",
    "## 编码层 好几个EncoderLayer\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        # 有N个EncoderLayer\n",
    "        self.layers = clones(layer,N)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        ### 加个层归一化 ???\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# 获取解码器中的句子掩码，不是批量掩码\n",
    "# words_len 单个句子长度(预测的目标值)\n",
    "def get_subsequent_mask(words_len):\n",
    "    # 形状(batch,words_len,words_len)\n",
    "    mask_shape = (1,words_len,words_len)\n",
    "    \n",
    "    # diagonal=1: 保留主对角线往上1行的数据(不包括主对角线)，不保留的数据都是0 \n",
    "    subsequent_mask = torch.triu(torch.ones(mask_shape), diagonal=1).type(torch.uint8)\n",
    "    \n",
    "    return subsequent_mask == 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 解码器子层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff, dropout=0):\n",
    "        super().__init__()\n",
    "        # 自注意力，多头注意力，qkv同源\n",
    "        self.self_mha = MultiHeadedAttention(d_model=d_model, n_head=n_head, dropout=dropout)\n",
    "        # 交叉注意力，多头注意，q来自解码器，kv来自解码器\n",
    "        self.mha = MultiHeadedAttention(d_model=d_model, n_head=n_head, dropout=dropout)\n",
    "        # 前馈神经网络\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "    \n",
    "    # tgt_mask 是解码层的mask，是叠加了批量掩码和单词掩码的   \n",
    "    # memory 解码器生成的，用来产生kv的中间量\n",
    "    # src_mask 解码器的批量掩码，交叉注意力时，q乘k的转置，需要把k后面的列屏蔽掉，跟解码器是同一个批量掩码\n",
    "    # training: 当前是否处于训练中\n",
    "    def forward(self, x, tgt_mask, memory, src_mask, training=True):\n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        assert tgt_mask.ndim == 3, '形状要求(batch, sentence_len, sentence_len)'\n",
    "        assert memory.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        assert src_mask.ndim == 3 , '形状要求(batch, sentence_len, sentence_len)'\n",
    "            \n",
    "            \n",
    "        ###   掩码形状保持和入参一致  x形状(batch, sentence_len, d_model),x.size(1)表示词个数     ###\n",
    "        if training != True:\n",
    "            tgt_mask = tgt_mask[:, :, :x.size(1)]\n",
    "        #######################################################################################    \n",
    "            \n",
    "        # 1.自多头注意力  \n",
    "        x,softmax_scores = self.self_mha(q=x, k=x, v=x, mask=tgt_mask)\n",
    "        \n",
    "        \n",
    "        ### 如果当前是预测,此时x只需要传递最后一个词就可以了,因为预测词也是一个一个生成的, 但是在循环layer的时候上面的掩码也得变 ###\n",
    "        if training != True:\n",
    "            x = x[:, -1, :].unsqueeze(1)\n",
    "         #######################################################################################\n",
    "        \n",
    "        \n",
    "        # 2.交叉注意力,q有解码器生成， kv有解码器产生，\n",
    "        x,softmax_scores = self.mha(q=x, k=memory, v=memory, mask=src_mask)\n",
    "        \n",
    "        # 做前馈神经网络\n",
    "        return self.feed_forward(x)\n",
    "    \n",
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        # N个DecoderLayer\n",
    "        self.layers = clones(layer,N)\n",
    "    \n",
    "    def forward(self, x, tgt_mask, memory, src_mask, training=True):        \n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        assert tgt_mask.ndim == 3, '形状要求(batch, sentence_len, sentence_len)'\n",
    "        \n",
    "        assert memory.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "        assert src_mask.ndim == 3 , '形状要求(batch, sentence_len, sentence_len)'\n",
    "        \n",
    "        # if training != True:\n",
    "        #     print('--> start--> \\n')\n",
    "        for layer in self.layers:\n",
    "            # if training != True:\n",
    "            #     print('+++> ',x.shape, tgt_mask.shape,  'q:',x, 'mask:',tgt_mask)\n",
    "           \n",
    "            x = layer(x, tgt_mask, memory, src_mask, training)\n",
    "            \n",
    "\n",
    "        # 搞个层归一化？\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    # vocab_size: 目标语言的词表大小\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        # \n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 3 , '形状要求(batch, sentence_len, d_model)'\n",
    "\n",
    "        # x形状 (batch,words_len, d_model) ->(batch, words_len, vocab_size)\n",
    "        x = self.linear(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "    \n",
    "# 模型\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    def forward(self, src_x, src_mask, tgt_x, tgt_mask):\n",
    "        src_embed = self.src_embed(src_x)\n",
    "        tgt_embed = self.tgt_embed(tgt_x)\n",
    "        \n",
    "        memory = self.encoder(src_embed, src_mask)\n",
    "        output = self.decoder(tgt_embed, tgt_mask, memory, src_mask)\n",
    "        \n",
    "        return self.generator(output)\n",
    "\n",
    "    # 编码\n",
    "    def encode(self, src_x, src_mask):\n",
    "        return self.encoder(self.src_embed(src_x), src_mask)\n",
    "\n",
    "    # 解码\n",
    "    def decode(self, tgt_x, tgt_mask, memory, src_mask, training=True):\n",
    "        return self.decoder(self.tgt_embed(tgt_x),tgt_mask, memory, src_mask, training)\n",
    "    \n",
    "\n",
    "# 实例化模型    \n",
    "def make_model(src_vocab_size, tgt_vocab_size, d_model, n_head, d_ff, N, dropout):\n",
    "    \n",
    "    # 编码器\n",
    "    encoderLayer = EncoderLayer(d_model, n_head, d_ff, dropout)\n",
    "    encoder = Encoder(encoderLayer, N)\n",
    "   \n",
    "    # 解码层\n",
    "    decoderLayer = DecoderLayer(d_model, n_head, d_ff, dropout)\n",
    "    decoder = Decoder(decoderLayer, N)\n",
    "    \n",
    "    # 位置\n",
    "    position = PositionalEncoding(d_model=d_model, dropout=DROPOUT)\n",
    "\n",
    "    # 解码嵌入, Sequential可以让Embeddings执行完后再执行position\n",
    "    src_embed = nn.Sequential(Embeddings(src_vocab_size, d_model=d_model), position)\n",
    "    \n",
    "    # 编码嵌入\n",
    "    tgt_embed = nn.Sequential(Embeddings(tgt_vocab_size, d_model=d_model), position)\n",
    "    \n",
    "    # 生成器\n",
    "    generator = Generator(d_model, tgt_vocab_size)\n",
    "   \n",
    "    # 生成模型\n",
    "    model = Transformer(encoder, decoder, src_embed, tgt_embed, generator)\n",
    "    \n",
    "    # 初始化模型参数\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            # 正态分布初始化\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    # 返回模型\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################### config.py #############\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "BASE_PATH = os.path.dirname('.')\n",
    "\n",
    "# 训练数据文件路径\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, './data/inputs/demo/train.json')\n",
    "# 校验数据文件路径\n",
    "VAL_PATH = os.path.join(BASE_PATH, './data/inputs/demo/val.json')\n",
    "\n",
    "\n",
    "# 存放中文词表的文件\n",
    "ZH_VOCAB_PATH = os.path.join(BASE_PATH, './data/vocab/zh.txt')\n",
    "# 存放英文词表的文件\n",
    "EN_VOCAB_PATH = os.path.join(BASE_PATH, './data/vocab/en.txt')\n",
    "\n",
    "# 特殊字符在词表中的id值\n",
    "PAD_ID = 0  # 屏蔽词id\n",
    "UNK_ID = 1  # 不知道的词id\n",
    "SOS_ID = 2  # 开始标记id\n",
    "EOS_ID = 3  # 结束标记id\n",
    "\n",
    "# 子层编码和解码个数\n",
    "N = 3\n",
    "# 词向量维度\n",
    "D_MODEL = 32\n",
    "# 头数\n",
    "N_HEAD = 2\n",
    "# feedforward 维度\n",
    "D_FF = 128\n",
    "\n",
    "DROPOUT = 0\n",
    "# # 批次\n",
    "BATCH_SIZE = 3\n",
    "# 学习率\n",
    "LR = 1e-1\n",
    "# 训练次数\n",
    "EPOCHS = 100\n",
    "\n",
    "# 生成句子最大长度 \n",
    "MAX_LEN = 6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "\n",
    "# 安装 sacrebleu\n",
    "# pip install sacrebleu\n",
    "import sacrebleu\n",
    "\n",
    "# 翻译评估\n",
    "# 预测的句子  hyp =  ['我爱吃苹果。', '这本书非常有趣。', '他是一位优秀的演员。']\n",
    "# 参考句子    refs = [['我喜欢吃苹果。', '我喜欢吃水果。'],\n",
    "#                    ['这本书很有意思。', '这本书很好玩。'],\n",
    "#                    ['他是一个出色的演员。', '他是一名杰出的演员。']]\n",
    "def bleu_score(hyp, refs):\n",
    "    bleu = sacrebleu.corpus_bleu(hyp, refs, tokenize='zh')\n",
    "    # 保留2位小数\n",
    "    return round(bleu.score, 2)\n",
    "\n",
    "\n",
    "# 中文分词\n",
    "def divided_zh(sentence):\n",
    "    return jieba.lcut(sentence)\n",
    "\n",
    "# 英文分词\n",
    "def divided_en(sentence):\n",
    "    # 匹配单词和标点符号\n",
    "    pattern = r'\\w+|[^\\w\\s]'\n",
    "    return re.findall(pattern, sentence)\n",
    "\n",
    "# 获取词表\n",
    "def get_vocab(lang='en'):\n",
    "    if lang == 'en':\n",
    "        file_path = EN_VOCAB_PATH\n",
    "    elif lang == 'zh':\n",
    "        file_path = ZH_VOCAB_PATH\n",
    "    \n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        lines = file.read()\n",
    "        \n",
    "    id2vocab = lines.split('\\n')\n",
    "    vocab2id = {v:k for k,v in enumerate(id2vocab)}\n",
    "    \n",
    "    return id2vocab, vocab2id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_precessor.py\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 生成词表\n",
    "def generate_vocab():\n",
    "    # 中文特殊词  [屏蔽词，不知道的词，开头，结尾]\n",
    "    en_vocab_set = ['<pad>','<unk>','<sos>','<eos>']\n",
    "    # 英文特殊词  [屏蔽词，不知道的词，开头，结尾]\n",
    "    zh_vocab_set = ['<pad>','<unk>','<sos>','<eos>']\n",
    "\n",
    "    en_vocab_list = []\n",
    "    zh_vocab_list = []\n",
    "\n",
    "    # 解析训练数据\n",
    "    with open(TRAIN_PATH, encoding='utf-8') as file:\n",
    "        lines = json.loads(file.read())\n",
    "        \n",
    "        for en_sentence, zh_sentence in lines:\n",
    "            # 收集英文分词后都数据\n",
    "            en_vocab_list += divided_en(en_sentence)\n",
    "            # 收集中文分词后都数据\n",
    "            zh_vocab_list += divided_zh(zh_sentence)\n",
    "    \n",
    "    \n",
    "    # 词表去重\n",
    "    # zh_vocab_list中的词去重排序  [('词1',5),('词2',3)]\n",
    "    zh_vocab_sort_kv = Counter(zh_vocab_list).most_common()\n",
    "    # 去重后的词表  ['词1', '词2', '词3']\n",
    "    zh_vocab_set += [k.lower() for k,v in zh_vocab_sort_kv]\n",
    "    \n",
    "    en_vocab_sort_kv = Counter(en_vocab_list).most_common()\n",
    "    # 去重后的词表  ['词1', '词2', '词3']\n",
    "    en_vocab_set += [k.lower() for k,v in en_vocab_sort_kv]\n",
    "    \n",
    "    print('en_vocab_set count:',len(en_vocab_set))\n",
    "    print('zh_vocab_set count:',len(zh_vocab_set))\n",
    "\n",
    "    \n",
    "    # Python join() 方法用于将序列中的元素以指定的字符连接生成一个新的字符串。\n",
    "    # 生成的词表写到文件中\n",
    "    with open(EN_VOCAB_PATH, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(en_vocab_set))\n",
    "        \n",
    "    with open(ZH_VOCAB_PATH, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(zh_vocab_set))\n",
    "    \n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "################### data_loader.py ####################\n",
    "#######################################################\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, type='train'):\n",
    "        super().__init__()\n",
    "        if type == 'train':\n",
    "            file_path = TRAIN_PATH\n",
    "        elif type == 'val':\n",
    "            file_path = VAL_PATH\n",
    "            \n",
    "        with open(file_path, encoding='utf-8') as file:\n",
    "            self.lines = json.loads(file.read())\n",
    "            \n",
    "            # 词表\n",
    "            _, self.en_vocab2id = get_vocab('en')\n",
    "            _, self.zh_vocab2id = get_vocab('zh')\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "            \n",
    "    \n",
    "    # 取第 index 单条样本\n",
    "    def __getitem__(self, index):\n",
    "        en_text, zh_text = self.lines[index]\n",
    "            \n",
    "        # 取出的句子进行分词，并转成索引表示\n",
    "        source = [self.en_vocab2id.get(word.lower(), UNK_ID) for word in divided_en(en_text)]\n",
    "        target = [self.zh_vocab2id.get(word.lower(), UNK_ID) for word in divided_zh(zh_text)]\n",
    "        \n",
    "        return source, target, zh_text\n",
    "\n",
    "    \n",
    "    # 数据对齐和整理\n",
    "    # batch是 Dataset 中返回的样本数据，有三个 source, target, zh_text\n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        batch_source, batch_target, batch_tgt_text = zip(*batch)\n",
    "    \n",
    "        src_x = pad_sequence(sequences = [torch.LongTensor([SOS_ID]+src+[EOS_ID]) for src in batch_source],\n",
    "                          batch_first = True,\n",
    "                          padding_value = PAD_ID)\n",
    "        \n",
    "        # 批量句子掩码\n",
    "        src_mask = get_padding_mask(src_x, padding_idx=PAD_ID)\n",
    "    \n",
    "       \n",
    "        tgt_full = pad_sequence(sequences = [torch.LongTensor([SOS_ID]+src+[EOS_ID]) for src in batch_target],\n",
    "                          batch_first = True,\n",
    "                          padding_value = PAD_ID)\n",
    "        # 目标输入值 不包括句子的最后一个字符\n",
    "        tgt_x = tgt_full[:, :-1]\n",
    "        # 目标预测值 不包括句子的第一个字符\n",
    "        tgt_y = tgt_full[:, 1:]\n",
    "        \n",
    "        tgt_pad_mask = get_padding_mask(tgt_x, PAD_ID)\n",
    "        tgt_subsqueent_mask = get_subsequent_mask(words_len=tgt_x.size(-1))\n",
    "        tgt_mask = tgt_pad_mask | tgt_subsqueent_mask\n",
    "        \n",
    "        return src_x, src_mask, tgt_x, tgt_mask, tgt_y, batch_tgt_text \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#####################  train.py  ##########################\n",
    "###########################################################\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "# 单轮训练\n",
    "def run_epoch(loader, model, loss_fun, optimizer=None):\n",
    "    # 本轮训练总损失\n",
    "    total_loss = 0\n",
    "    # 本轮 总批次\n",
    "    total_batchs = 0\n",
    "    \n",
    "    # print(next(iter(loader)))\n",
    "\n",
    "    # 加载数据并训练\n",
    "    for src_x, src_mask, tgt_x, tgt_mask, tgt_y, batch_tgt_text in loader:\n",
    "        # 模型输出 (batch, words_len, vocab_size) \n",
    "        # (批量个数(预测句子个数), 预测某个句子的单词个数, 预测的单词属于各个分类(vocab_size)的概率个数)\n",
    "        predict = model(src_x, src_mask, tgt_x, tgt_mask)\n",
    "        # print('predict.shape-> ', predict.shape)\n",
    "\n",
    "        # 形状转变，把批量维度去掉 (batch, words_len, vocab_size)  -> (words_len, vocab_size)\n",
    "        # 这样就变成了，预测的单词个数，以及单个单词属于各个分类的概率个数了\n",
    "        predict = predict.reshape(-1, predict.shape[-1])\n",
    "        \n",
    "        # 形状转变，(batch, target_size) -> (target_size) \n",
    "        # batch: 批次\n",
    "        # target_size:目标单词的个数, 里面放的是，目标单词的分类id\n",
    "        # 比如 目标词表有vocab_size个单词, tgt_y放的就是这个单词在词表中所属的id(经过交叉熵函数后，这个位置对应的单词就变成了概率只为1的目标值了)\n",
    "        tgt_y = tgt_y.reshape(-1)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = loss_fun(predict, tgt_y)\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss += loss.item()\n",
    "        total_batchs += 1\n",
    "        \n",
    "        # 反向传播\n",
    "        if optimizer:\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 计算梯度\n",
    "            loss.backward()\n",
    "            # 梯度更新\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 返回平均损失\n",
    "        return total_loss / total_batchs\n",
    "    \n",
    "\n",
    "    \n",
    "# 逐字生成预测值\n",
    "def greedy_decode(model, src_x, src_mask, max_len):\n",
    "    # 录入的句子进行编码 \n",
    "    memory = model.encode(src_x, src_mask)\n",
    "    \n",
    "    # 初始每个批次的目标值   src_x.size(0):是批次\n",
    "    # 形状(batch, 单词个数)\n",
    "    tgt_x = torch.tensor([[SOS_ID]] * src_x.size(0))\n",
    "    \n",
    "    # 开始生成目标值\n",
    "    for _ in range(max_len):\n",
    "        # 解码其中的批量掩码，这里不需要句子掩码\n",
    "        tgt_mask = get_padding_mask(tgt_x,padding_idx=PAD_ID)\n",
    "        \n",
    "        # 解码 \n",
    "        # 训练时output形状 (batch, 预测过程中句子长度, 词表长度)\n",
    "        # 预测时output形状 (batch, 1, 词表长度)\n",
    "        output = model.decode(tgt_x, tgt_mask, memory, src_mask, training=model.training) \n",
    "\n",
    "        # 用生成器取最后一个词  output最后形状 (batch, 1, 词表长度)\n",
    "        #    output = output[:,-1,:]\n",
    "        output = model.generator(output)\n",
    "        # [batch, 1, 20]\n",
    "\n",
    "        # 获取生成词的词id   predict形状(批次, 1)  1表示生成一个词\n",
    "        predict = torch.argmax(output, dim=-1)\n",
    "        \n",
    "        # 所有生成的词拼接上  形状(batch, 单词个数)  tgt_x就是预测的结果\n",
    "        tgt_x = torch.concat([tgt_x, predict], dim=-1)\n",
    "       \n",
    "        # 碰到EOS_ID则结束预测, 如果有一个批次没结束则继续预测？\n",
    "        if torch.all(predict == EOS_ID).item():\n",
    "            break\n",
    "            \n",
    "    \n",
    "    # 中文词表\n",
    "    zh_id2vocab, _ = get_vocab('zh')\n",
    "    # 生成的句子放到如下\n",
    "    batch_tgt_text = []\n",
    "    \n",
    "    # 把预测的id，翻译成对应的词和句子，fangdao batch_tgt_text\n",
    "    for tgt in tgt_x:\n",
    "        # 单个句子\n",
    "        text = []\n",
    "        # tgt中放的是词id\n",
    "        for word_id in tgt:\n",
    "            if word_id == SOS_ID:\n",
    "                continue\n",
    "            elif word_id == EOS_ID:  # 碰到结束标识，改句子后面的解析就不要了\n",
    "                break\n",
    "\n",
    "            # 词id换成词，并放到text中\n",
    "            text.append(zh_id2vocab[word_id])\n",
    "\n",
    "        # 各个批次的句子放到 batch_tgt_text中\n",
    "        batch_tgt_text.append(''.join(text))\n",
    "        \n",
    "    # 返回生成的句子\n",
    "    return batch_tgt_text \n",
    "\n",
    "\n",
    " \n",
    "# 评估翻译效果    \n",
    "def evaluate(loader, model, max_len):\n",
    "    # 目标值，目标句子\n",
    "    tgt_sentence = []\n",
    "    # 翻译的句子\n",
    "    prob_sentence = []\n",
    "\n",
    "    for src_x, src_mask, tgt_x, tgt_mask, tgt_y, tgt_text in loader:\n",
    "        # 翻译好的句子\n",
    "        batch_prob_text = greedy_decode(model, src_x, src_mask, max_len)\n",
    "\n",
    "        prob_sentence += batch_prob_text\n",
    "        tgt_sentence += tgt_text\n",
    "    \n",
    "    # 返回评估分数\n",
    "    return bleu_score(prob_sentence, [tgt_sentence]),prob_sentence,tgt_sentence\n",
    "    \n",
    "LR = 0.003\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    en_id2vocab,_ = get_vocab('en')\n",
    "    zh_id2vocab,_ = get_vocab('zh')\n",
    "    \n",
    "    # 英文词表大小\n",
    "    SRC_VOCAB_SIZE = len(en_id2vocab)\n",
    "    # 中文词表大小\n",
    "    TGT_VOCAB_SIZE = len(zh_id2vocab)\n",
    "\n",
    "    # 实例化模型\n",
    "    model = make_model(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, D_MODEL, N_HEAD, D_FF, N, DROPOUT)\n",
    "    # 加载已经训练好的模型\n",
    "    # model.load_state_dict(torch.load('./model/best_model.pth'))\n",
    "\n",
    "    \n",
    "    ## 模型参数量\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print('模型参数量: ',total_params)\n",
    "    \n",
    "    #损失函数和优化器\n",
    "    loss_fun = CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    train_dataset = Dataset('train')\n",
    "    train_loader = data.DataLoader(train_dataset, shuffle=False, collate_fn=train_dataset.collate_fn, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    val_dataset = Dataset('val')\n",
    "    val_loader = data.DataLoader(val_dataset, shuffle=False, collate_fn=val_dataset.collate_fn, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    #### 训练  #########\n",
    "    best_bleu_score = 70 # 最好的评估分数\n",
    "    best_train_loss = 2.1706809997558594\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        ########### 训练模式 ##############\n",
    "        model.train()\n",
    "        train_loss = run_epoch(train_loader, model, loss_fun,optimizer)\n",
    "\n",
    "        ########### 验证流程 ##############\n",
    "        model.eval()\n",
    "        val_loss = run_epoch(val_loader, model, loss_fun,None)\n",
    "        # 评估\n",
    "        val_bleu_score,prob_sentence,tgt_sentence = evaluate(val_loader,model,MAX_LEN)\n",
    "        \n",
    "        if i > EPOCHS-3:\n",
    "            print('翻译: ',prob_sentence)\n",
    "            print('目标: ',tgt_sentence)\n",
    "            print('>>eopch:',i, '; train_loss:',train_loss,'; val_loss:',val_loss, '\\n', 'val_bleu_score:', val_bleu_score)\n",
    "        \n",
    "        # 如果当前评估分数好于 best_bleu_score，则保存模型\n",
    "        if train_loss < best_train_loss:\n",
    "            torch.save(model.state_dict(), './model/best_model.pth')\n",
    "            best_train_loss = train_loss\n",
    "            print('saveing-> ',train_loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f720c5-34d4-4bfb-8913-c341b34989e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
