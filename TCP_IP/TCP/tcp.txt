Tcp首部数据格式
 0                                15  16                                31
 -------------------------------------------------------------------------
 |				16位源端口号         	|		16位目的端口号           |
 -------------------------------------------------------------------------
 |									32位序号							 |
 -------------------------------------------------------------------------
 |									32位确认序号				 		 |
 -------------------------------------------------------------------------
 |            |          |U|A|P|R|S|F|					 				 |
 |4位首部长度 | 保留(6位)|R|C|S|S|Y|I|		16位窗口大小		 		 |
 |	     	  |		 	 |G|K|H|T|N|N|					 				 |
 -------------------------------------------------------------------------
 |              16位校验和           |       16位紧急指针                |
 -------------------------------------------------------------------------
 |                               自定义选项                              |
 -------------------------------------------------------------------------
 |                                 数据                                  |
 -------------------------------------------------------------------------

Tcp首部中6个标志比特说明
 URG	紧急指针(urgent pointer)
 ACK	确认应答
 PSH	接收方应该尽快将这个报文段交给应用层
 RST	重建连接
 SYN	请求建立连接
 FIN	请求断开连接

MSS:最大报文段长度，表示TCP传往另一端的最大块数据的长度。
 对于以太网，MSS值最大可到1500 - IP首部(20) - TCP首部(20) = 1460字节
 对于802.3，MSS之最大可到1492 - IP首部(20) - TCP首部(20) = 1452字节

Tcp建立连接  mss:Maximum Segement Size  MTU:Maxmum Transmission Unit(链路层叫法)
                            SYN 157445:157445 <mss 1024>
  SYN_SENT     |--------------------------------------------------->|   SYN_RCVD
               |       SYN 189893:189893 ack 157446 , <mss 1024>    |
 ESTABLISHD    |<---------------------------------------------------|
               |                   ack 189894                       |
               |--------------------------------------------------->|  ESTABLISHED

Tcp终止连接
                           |         FIN          |
 主动关闭   FIN_WAIT_1     |--------------------->|     CLOSE_WAIT 被动关闭
		       			   |	     ACK          |
 	  		FIN_WAIT_2     |<---------------------|          
           				   |	     FIN          |
	  		TIME_WAIT      |<---------------------|      LAST_ACK
           		   		   |	     ACK          |
			   			   |--------------------->|      CLOSED
                           |                      |
			NOTE:对于被动关闭的一端是否发送FIN(CLOSE_WAIT-->LAST_ACK),取决于应用
				 程序是否调用close()方法，如果不调用则被动关闭端保持CLOSE_WAIT,
				 对端保持FIN_WAIT2状态。区别在于主动关闭一端的应用程序已经关闭，
				 剩下的工作已经全权委托操作系统，而被动关闭端的应用程序没有关闭
				 该链接(socket),或者应用程序异常终止了,没有再向对端发送任何数据.
			所以根据以上结论，当系统出现大量CLOSE_WAIT时,肯定是应用程序没有调用
			close()方法。

套接字复位(RST,异常关闭)
 RST报文段不会导致另一端产生任何响应，另一端根本不进行确认。
 收到RST的一方将终止该连接，并通知应用层连接复位。
 java例子:
  ServerSocket ss = new ServerSocket(8989);
  whil(true){
	Socket s = ss.accept();
	s.setSoLinger(true,0);
	s.close();
 } 

半打开连接
 如果一方已经关闭或异常终止连接而另一方还不知道，这种TCP连接称为半打开(Half-Open)
 只要不打算在半打开连接上传输数据，仍处于连接状态的一方就不会检测另一方已经出现异常。

半关闭
  TCP连接的一端在结束它的发送后还能接收来自另一端数据的能力。很少应用程序使用。

SO_REUSEADDR
  该选项可以决定，处在TIME_WAIT状态的连接是否可以被新的连接使用
SO_LINGER
  设置TCP关闭连接时的行为,默认当close()Tcp连接后,系统会将write buffer中的数据
  发送给对端,然后发FIN报文并等待对方确认来完成四次挥手(FIN ACK FIN ACK)。
 
  如果我们将该值设为0,则直接通过发送RST报文来关闭连接,此时buffer中的数据也会直接
  丢弃。这种方式对于主动关闭端来说会跳过TIMEWAIT状态。

  如果为非零则代表一个超时,如果在close()后在指定的时间内能够收到对方的确认,则进入
  正常的四次挥手阶段,否则直接发送RST报文。

TCP_NODELAY
 *可以用来关闭Nagle算法；
 *Nagle算法要求一个TCP连接上最多只能有一个未被确认的未完成的小分组，
  在该分组的确认未到达之前不能发送其他的小分组。同时在等待确认应答
  道来时，Tcp会收集这些小分组，然后打包发送。
  例子：
	telnet www.jd.com 80
  连接上之后我们发这样的数据:
	a
	a
	a
	
  在telnet中敲完回车后，会立即向对端发数据。根据上面的输入可以知道，
  我们发了三次数据，理论上应该产生三个tcp包，实际上只产生了两个。
  原因是第一个包太小，所以在没有收到ack之前不会再发数据，一旦收到ack，
  如果在发第一个包和ack之间又输入了多个小于一个报文段的数据，那么Nagle
  算法是不会发数据的。他会收集这些小分组数据，等收到之前的小分组的ack后
  ，或者收集的数据已经累计到一个报文段的大小，再发数据。	


 *如果服务端从不向客户端发送数据，那么就会经常触发服务器的经受时延的
  确认算法，会有一个200ms内的确认延迟。
SO_RCVBUF
 设置窗口大小
SO_KEEPALIVE
 是否开启保活设置，默认两小时没有数据流通则发送ACK探查包。

TCP滴答计时器(比如200ms一个滴答)   
  TCP软件启动，每隔200ms一个滴答，如下图
    0___1___2___3___4___5___6___7___8___9___10___11___12...504...
  
   设置一个1秒的定时器，需要为该定时器的计数器设为5个滴答
     如果刚好在2这个滴答时候之后设置定时器成功，那么该定时器经历3、4、5、6、7这5个
   滴答之后计数器恢复为零。该定时器历经1秒。
     如果在2~3这两个滴答中间设置计数器，那么该定时器同样经历3、4、5、6、7这5个滴答。
   此时定时器经历900ms。
   
TCP往返时间(RTT)的测定
   在一个连接中，每次发送数据都会先检查定时器是否被使用，如果没有则定时器启动并开始计时，
 待这次发送的数据返回ack后，定时器关闭并记录经历的滴答个数。
    

Tcp慢启动和避免拥塞算法
  该算法可以看"慢启动和避免拥塞算法.jpg/.docx"
  常规避免拥塞算法在发送超时拥塞时，会设置成慢启动(cwnd=1)。
  快速重传：在收到3个及以上的重复ACK后，重传报文不等待定时器溢出。
  快速恢复：快速重传之后不执行慢启动算法，而是执行避免拥塞算法。

滑动窗口
 *窗口左边沿向右边沿靠近称为窗口合拢。发生在数据被发送和确认时。
 *窗口右边沿向右边移动时将允许发送更多的数据，叫窗口张开。发生在另一端接收进程读取
  已确认的数据并释放了TCP缓存时。
 *右边向左边移动时，叫窗口收缩。Host Requirements RFC不建议使用这种方式。

#超时重传机制
  当发送方发现，在timeout时间内收不到已发送包3的ack后，会重传该包3。
  多数实现会直接重传3及其在改时间内已经发出去的4、5等包。

#快速重传机制
  不再以timeout为界限，而以ack个数驱动重传。如果发送方连续3次收到相同的ack,
 就重传。

#Tcp的KeepAlive,有三个选项可设置
 $sysctl -a | grep tcp_keepalive
  
 net.ipv4.tcp_keepalive_time = 7200
   TCP链路在经过多少秒后没有数据报文传输就启动保活探测
 net.ipv4.tcp_keepalive_probes = 9
   当前最多探测的次数 
 net.ipv4.tcp_keepalive_intvl = 75 
   当前发送每个探测报的间隔秒数

 如果系统支持的话可以为每个socket设置这些参数,使用setsockopt设置
 TCP_KEEPIDLE 对应 keepalive_time
 TCP_KEEPCNT 对应 keepalive_probes
 TCP_KEEPINTVL 对应 keepalive_intvl 

 例子:
  Nginx中得listen指令有一个参数可以设置TCP层的keepalive
  so_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt] 
  三个参数,on是开启,off关闭,最后是直接指定keepalive时间	
 
  listen 8080 so_keepalive=7200:75:9

#数据接收过程中涉及的队列
##半链接队列
 该队列用来保存SYN_RECV状态的链接,队列长度由net.ipv4.tcp_max_syn_backlog设置。
 查看默认值:
  $sysctl -a | grep net.ipv4.tcp_max_syn_backlog
   net.ipv4.tcp_max_syn_backlog = 512

##accept队列 
 用来保存ESTABLISHED状态的还未被服务端接收的链接。
 队列长度为min(net.core.somaxconn,backlog),backlog是我们创建
 ServerSocket(int port,int backlog)时指定的参数,最终传递给
 int listen(int sockfd,int backlog)
 查看net.core.somaxconn默认值
  $sysctl -a | grep net.core.somaxconn
   net.core.somaxconn = 128

##SYN flooding
 syn洪水是说,客户端只发送syn包,不回应ack,这样会快速的填满半链接队列,
 从而导致server端无法接收正常的握手请求。

 linux使用SYNcookie来解决这种问题,大致就是将连接信息编码在ISN中返回
 给客户端,server端不再保存这个半链接,利用ACK带回的ISN还原链接信息。
 ?疑问,server端如何识别这个cookie

 这个net.ipv4.tcp_syncookies值是1表示是开启的,默认开启。

##查看网卡是否支持多队列
  $ lspci -vvv
 找Ethernet controller,看是否有MSI-X,Enable+ 并且Count>1,则支持。
 
 看网卡是否开启了多队列:
  $ cat /proc/interrupts
 如果看到eth0-TxRx-0表明已经打开

 最后根据上面看到的中断号,看是否每个队列对应到了不同的CPU
  $ cat /proc/irq/终断号/smp_affinity

##网卡中的RingBuffer队列
 Ring Buffer位于NIC和IP层之间,NIC是网卡最开始存放数据的地方。
 该队列是一个FIFO(先进先出)环形队列。里面只是包含了指向sk_buff
 (socket kernel buffers)的描述符

 查看该队列的设置:
	$ ethtool -g eth0
     Ring parameters for eth0:
     Pre-set maximums:
     RX:		4096
     RX Mini:	0
     RX Jumbo:	0
     TX:		4096
     Current hardware settings:
     RX:		256
     RX Mini:	0
     RX Jumbo:	0
     TX:		256
 可以看到,RX接收队列4096,传输队列256,通过ifconfig查看队列的运行状况:
	$ ifconfig 
    eth0  Link encap:Ethernet  HWaddr 08:00:27:07:9E:3D  
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fe07:9e3d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:49257 errors:0 dropped:0 overruns:0 frame:0
          TX packets:27796 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:12420869 (11.8 MiB)  TX bytes:2558849 (2.4 MiB) 
 看RX这一行
	errors:收包的错误数 
    dropped:数据包已进入RingBuffer,但由于内存不够等系统原因,丢弃的包。
	overruns:数据包没有到RingBuffer就被网卡物理层给丢弃,比如CPU无法
         及时处理中断,不能将RingBuffer中的数据放入到内核中。
    
    如果dropped的数量持续增加,可以用ethtool -G 增大RingBuffer
	 ?好像参数不对 

##数据包接收队列 InputPacket Queue
 当接收数据包的速率大于内核TCP处理包得速率,包会被缓存在TCP层之前
 的队列中,也就是说此时并没有进入到TCP程序。
 
 查看队列长度
 $sysctl -a | grep  net.core.netdev_max_backlog

##recvBuffer队列,TCP的窗口大小
 
#数据包发送涉及的队列
 和数据包接收相反,数据包发送从上往下也经历了三层:用户态空间、
 系统内核空间、最后到网卡驱动。

 应用层数据首先进入内核态的TCP sendbuffer,TCP层将buffer中的数据
 构建成数据包转给IP层。IP层将待发送的数据包放入到队列QDisc，将指向
 数据包的描述符sk_buff放入到RingBuffer输出队列,随后网卡驱动调用DMA
 将数据发送到网络链路上。

##sendBuffer (发送窗口)
  sendBuffer有net.ipv4.tcp_wmem设置,tcp_wmem默认等于net.core.wmem_default。
  如果设置了tcp_wmem,则wmem_default不再起作用,此时sendBuffer在tcp_wmem的
  最大值和最小值之间自动调节。

  如果调用setsockopt()函数设置了socket选项SO_SNDBUF,tcp_wmem将被忽略,SO_SNDBUF
  的最大值有net.core.wmem_max限制。
      
##QDisc队列
  QDisc(queueing discipline)位于IP层和网卡的ringbuffer之间。 
  QDisc的队列长度有txqueuelen设置,可以有ifconfig命令查看并设置。

  使用ifconfig调整txqueuelen的大小:
	$ifconfig eth0 txqueuelen 2000

##RingBuffer
  和接收数据包一样,发送数据包也要经过RingBuffer。
  其中TX是发送队列的长度。

##TCP Segmentation和 Checksum offload
  操作系统可把一些TCP/IP的功能转交给网卡去完成，比如数据包分片和
  校验和，这样可以节省CPU资源，并提升性能。

  比如在以太网中,如果要发送7000byte,那操作系统需要将在TCP层将数据
  拆成 7000/(1500-20-20) = 5 个tcp包。如果这些事交给网卡去做,系统
  就可以直接发送一个7000byte的包,最后有网卡驱动负责拆包并校验。



